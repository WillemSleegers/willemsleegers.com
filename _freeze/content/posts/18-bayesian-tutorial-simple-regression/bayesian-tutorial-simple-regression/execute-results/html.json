{
  "hash": "5c03654a5c775e547e4f69c0e751b751",
  "result": {
    "markdown": "---\ntitle: \"Bayesian tutorial: Single predictor regression\"\ndescription: \"The second of a series of tutorial posts on Bayesian analyses. In this post I focus on using `brms` to run a regression with a single predictor.\"\ndate: 2022-11-24\ncategories:\n  - statistics\n  - tutorial\n  - Bayesian statistics\n  - regression\ncode-fold: true\ncode-tools: true\ntoc: true\ndraft: true\nformat: \n   html:\n     df-print: kable\n---\n\n\nIn my previous [blogpost](bayesian-tutorial-intercept-only.qmd) I showed how to use `brms` and `tidybayes` to run an intercept-only model. Now let's extend that model by adding a predictor. \n\nThe [data](Howell1.csv) is the same as in the previous post. In addition to heights data it also contains weights data. This means we can run a model in which we regress heights onto weights, i.e., a regression with a single predictor. \n\nIf you want to follow along, run the following setup code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(willemverse) # remotes::install_github(\"willemsleegers/willemverse\")\n\ndata <- read_csv(\"Howell1.csv\")\ndata <- filter(data, age >= 18)\n\ntheme_set(theme_minimal())\n\npal_2 <- brew_colors(n = 2, begin = .6)\n```\n:::\n\n\n## Data\n\n\n\nAdding a predictor means the model is different, and that means Now let's add a predictor to our model. Besides h\neights, the data set also contains weights. We can create a model in which we regress heights onto weights. The formula syntax for a model like that in R is `height ~ weight`. We can use this formula again in `get_prior()` to see which priors we need to specify.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(height ~ weight, data = data)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|prior                    |class     |coef   |group |resp |dpar |nlpar |lb |ub |source  |\n|:------------------------|:---------|:------|:-----|:----|:----|:-----|:--|:--|:-------|\n|                         |b         |       |      |     |     |      |   |   |default |\n|                         |b         |weight |      |     |     |      |   |   |default |\n|student_t(3, 154.3, 8.5) |Intercept |       |      |     |     |      |   |   |default |\n|student_t(3, 0, 8.5)     |sigma     |       |      |     |     |      |0  |   |default |\n\n</div>\n:::\n:::\n\n\nThe output is a bit trickier this time. We see the Intercept and sigma priors from our previous model, as well as two extra rows referring to a class called `b`. These two rows actually refer to the same prior, one refers specifically to the weight predictor and one refers to all predictors. If you run a model with many more predictors, you could set one prior that applies to all predictors. In this case though, we only have 1 predictor so it actually doesn't matter, both refer to the same prior.\n\nGiven that this is a bit trickier, and given that I said writing down your model explicitly is better, we should go ahead and do that.\n\n$$heights_i ∼ Normal(\\mu_i, \\sigma)$$\n\n$$\n\\mu_i = \\alpha + \\beta x_i\n$$\n\nWe again specify that the heights are normally distributed, so we still have a $\\mu$ and $\\sigma$, but this time the $\\mu$ is no longer a parameter we will estimate. Instead, it's constructed from other parameters, $\\alpha$, $\\beta$, and an observed variable $x_i$ (the weight observations).\n\nIf you're used to linear regression equations, this notation should not surprise you. $\\alpha$ refers to the intercept and $\\beta$ to the slope.\n\nWe need to set priors on these parameters. The prior for $\\alpha$ can be the same as the prior for $\\mu$ from the previous intercept-only model, if we center the data so the intercept refers to the average height of someone with an average weight, rather than someone with 0 weight (the default, which makes no sense). So let's first mean center the weight observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- mutate(data, weight_mc = weight - mean(weight))\n```\n:::\n\n\nNow we can use the same prior as before, which was a normal distribution with a mean of 170 and a standard deviation of 10 (assuming we did not update this as a result of the previous analysis).\n\nNext is the prior for the slope. This represents the relationship between weights and heights. For every 1 increase in weight, how much do we think that the height will increase or decrease? We could begin with an agnostic prior in which we do not specify the direction and instead just add some uncertainty so the slope can go in either direction. For example, let's put a normal distribution on the slope with a mean of 0 and a standard deviation of 10.\n\nFinally, we have the prior for sigma ($\\sigma$). To remind you, sigma refers to the standard deviation of the errors or the residual standard deviation. Now that we have a predictor that means the sigma can be less than what it was in the intercept-only model because some of the variance in heights might be explained by the weights, thus decreasing the size of the residuals and reducing sigma. So, if we believe in a relationship between heights and weights, we should change our prior for sigma so that it's lower. Given that we used a prior for the slope that is agnostic (there could be a positive, negative, or no relationship), our prior for sigma could be left unchanged because it was broad enough to allow for these possibilities.\n\n### Prior predictive check\n\nWe can again create a prior predictive check to see whether our priors actually make sense. However, instead of plotting the predicted distribution of heights, we're mostly interested in the relationship between weight and height, so we should plot a check of that relationship instead. We could simulate our own data like we did in the previous section or we can just run the Bayesian model and only draw from the prior, which we also did in the previous section.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_height_weight_prior <- brm(\n  height ~ weight_mc,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(normal(170, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\"),\n      prior(normal(0, 10), class = \"b\")\n    ), \n  sample_prior = \"only\",\n  cores = 4,\n  seed = 4,\n  file = \"models/model_height_weight_prior.rds\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCompiling Stan program...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nStart sampling\n```\n:::\n:::\n\n\nBelow we draw intercepts and slopes from the model result and plot 100 of them. To help make sense of the sensibility of the slopes I've added the average weight to the weights so we're back on the normal scale and not the mean centered scale and I've added two dashed lines to indicate the minimum and maximum height we can expect. Note that this time we use the `spread_draws()` function from the `tidybayes` package because we want the data frame to be wide (parameters in separate columns) rather than long (each row being a draw from a parameter), in order to visualize the regression lines.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- spread_draws(\n  model_height_weight_prior, b_Intercept, b_weight_mc, ndraws = 100\n)\n\nweight_mean <- data %>%\n  pull(weight) %>%\n  mean()\n\nggplot(data, aes(x = weight_mc, y = height)) +\n  geom_blank() +\n  geom_abline(\n    data = draws,\n    mapping = aes(intercept = b_Intercept, slope = b_weight_mc),\n    alpha = .25\n  ) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 272, linetype = \"dashed\") +\n  geom_label(x = 0, y = 260, label = \"Tallest person ever\") +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n```\n\n::: {.cell-output-display}\n![A prior predictive check of the relationship between weight and height](bayesian-tutorial-simple-regression_files/figure-html/prior-predictive-check-weight-1.png){width=672}\n:::\n:::\n\n\nThe plot shows a wide range of possible slopes, some of which are definitely unlikely. We should lower our uncertainty by reducing the standard deviation on the prior. In the next model I lower it to 3. Additionally, the negative slopes are all pretty unlikely because we should expect a positive relationship between weight and height (taller people tend to be heavier). We could therefore also change our prior to force it to be positive using the `lb` argument in our prior for `b`. Let's not do this though. Let's assume we have no idea whether the relationship will be positive or negative and instead focus on the standard deviation instead so that we don't obtain relationships we definitely know are unlikely (e.g., if many of the lines allow for many tallest people ever or heights below zero).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_height_weight_prior_2 <- brm(\n  height ~ weight_mc,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(normal(170, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\"),\n      prior(normal(0, 3), class = \"b\")\n    ), \n  sample_prior = \"only\",\n  cores = 4,\n  seed = 4,\n  file = \"models/model_height_weight_prior_2.rds\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCompiling Stan program...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nStart sampling\n```\n:::\n:::\n\n\nLet's inspect the lines again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- spread_draws(\n  model_height_weight_prior_2, b_Intercept, b_weight_mc, ndraws = 100\n)\n\nggplot(data, aes(x = weight_mc, y = height)) +\n  geom_blank() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 272, linetype = \"dashed\") +\n  geom_abline(\n    data = draws,\n    mapping = aes(intercept = b_Intercept, slope = b_weight_mc),\n    alpha = .25\n  ) +\n  geom_label(x = 0, y = 260, label = \"Tallest person ever\") +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n```\n\n::: {.cell-output-display}\n![A prior predictive check of the relationship between weight and height](bayesian-tutorial-simple-regression_files/figure-html/prior-predictive-check-weight-update-1.png){width=672}\n:::\n:::\n\n\nThis looks a lot better, so let's run the model for real now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_height_weight <- brm(\n  data = data, \n  height ~ weight_mc,\n  family = gaussian,\n  prior = c(\n      prior(normal(170, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\"),\n      prior(normal(0, 3), class = \"b\", lb = 0)\n    ), \n  sample_prior = TRUE,\n  cores = 4,\n  seed = 4,\n  file = \"models/model_height_weight.rds\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCompiling Stan program...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nStart sampling\n```\n:::\n\n```{.r .cell-code}\nmodel_height_weight\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ weight_mc \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.27   154.07   155.13 1.00     4271     2758\nweight_mc     0.91      0.04     0.82     0.99 1.00     3166     2527\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.11      0.19     4.75     5.50 1.00     4280     2706\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nWe see that the estimate for the weight predictor is 0.91. Let's plot the entire posterior for the slope estimate and also compare it to the prior we set for it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- model_height_weight %>%\n  gather_draws(prior_b, b_weight_mc) %>%\n  mutate(\n    distribution = if_else(\n      str_detect(.variable, \"prior\"), \"prior\", \"posterior\"\n    )\n  )\n\nggplot(results, aes(x = .value, fill = fct_rev(distribution))) +\n  geom_histogram(binwidth = 0.05, position = \"identity\", alpha = .85) +\n  xlim(0, 5) +\n  labs(x = \"Slope\", y = \"\", fill = \"Distribution\") \n```\n\n::: {.cell-output-display}\n![](bayesian-tutorial-simple-regression_files/figure-html/weight-prior-posterior-1.png){width=672}\n:::\n:::\n\n\nApparently our prior was still very uninformed because the posterior shows we can be confident in a much narrower range of slopes!\n\nLet's also create another plot in which we plot the model results against the observed data. In the graph below we plot the raw data as well as the regression line obtained from our model, together with a 95% CI (obtained via the `median_qi()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslopes_qi <- tibble(\n    weight_mc = seq(from = -20, to = 20, by = 1)\n  ) %>%\n  add_epred_draws(model_height_weight) %>%\n  median_qi()\n\nggplot() +\n  geom_ribbon(\n    mapping = aes(ymin = .lower, ymax = .upper, x = weight_mc),\n    data = slopes_qi,\n    alpha = .25\n  ) +\n  geom_line(\n    mapping = aes(x = weight_mc, y = .epred),\n    data = slopes_qi\n  ) + \n  geom_point(\n    mapping = aes(x = weight_mc, y = height),\n    data = data,\n    alpha = .25\n  ) +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n```\n\n::: {.cell-output-display}\n![](bayesian-tutorial-simple-regression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThis graph is great because it shows us how confident we can be in the regression line. It does omit one source of uncertainty, though. The previous plot only shows the uncertainty about the regression line (the intercept and slope). We can also make a plot with predicted values of individual heights, which also incorporates the uncertainty from the $\\sigma$ parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted_slopes_qi <- tibble(\n    weight_mc = seq(from = -20, to = 20, by = 1)\n  ) %>%\n  add_predicted_draws(model_height_weight) %>%\n  median_qi()\n\nggplot() +\n  geom_ribbon(\n    aes(ymin = .lower, ymax = .upper, x = weight_mc),\n    data = predicted_slopes_qi,\n    alpha = .25\n  ) +\n  geom_line(\n    aes(x = weight_mc, y = .prediction),\n    data = predicted_slopes_qi\n  ) + \n  geom_point(\n    aes(x = weight_mc, y = height),\n    data = data,\n    alpha = .25\n  ) +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n```\n\n::: {.cell-output-display}\n![](bayesian-tutorial-simple-regression_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### Thinking correlations instead\n\nMaybe one reason our prior was so uninformed was because it's harder to think of the right prior for a content-specific topic such as weights and heights of the !Kung San. Maybe we can instead standardize both the heights and weights in order to turn the regression model into a simple correlation analysis. That way we can specify a prior on what we think the correlation should be, which may be easier to do because we then think in terms of whether we think the relationship is small or medium or large, or something along those lines.\n\nSo, let's standardize the heights and weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- mutate(\n  data, \n  height_z = (height - mean(height)) / sd(height),\n  weight_z = (weight - mean(weight)) / sd(weight)\n)\n```\n:::\n\n\nThe formula for our correlation analysis is `height_z ~ weight_z`. Which priors we have to specify remains the same, but what these priors should be changes. For instance, we know that the Intercept has to be 0 now because the heights have been standardized. This means the mean will be 0. In `brms`, we can specify a constant as a prior using `constant()`.\n\nWhat should the prior for $\\sigma$ be? With the variables standardized, $\\sigma$ is limited to range from 0 to 1. If the predictor explains all the variance of the outcome variable, the residuals will be 0, meaning $\\sigma$ will be 0. If the predictor explains no variance, $\\sigma$ is equal to 1 because it will be similar to the standard deviation of the outcome variable, which is 1 because we've standardized it. Interestingly, this also means that the prior for $\\sigma$ is now dependent on the prior for the slope, because the slope is what determines how much variance is explained in the outcome variable. So let's think about the prior for the slope.\n\nThe prior for the slope is a bit easier now. We can specify a normal distribution with a mean of 0 and a standard deviation of 0.5, together with a lower bound of -1 and upper bound of 1. With a standard deviation of 0.5, we cover a large range of possible slopes, but assign more plausibility to smaller correlations and less plausibility to very high correlations (like 1 and -1).\n\nAs for $\\sigma$, let's keep it simple and use a uniform prior that assign equal plausibility to each value between 0 and 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_height_weight_z <- brm(\n  height_z ~ weight_z,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(constant(0), class = \"Intercept\"),\n      prior(uniform(0, 1), class = \"sigma\", ub = 1),\n      prior(normal(0, 0.5), class = \"b\", lb = -1, ub = 1)\n    ), \n  sample_prior = TRUE,\n  cores = 4,\n  seed = 4,\n  file = \"models/model_height_weight_prior_z.rds\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCompiling Stan program...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nStart sampling\n```\n:::\n\n```{.r .cell-code}\nmodel_height_weight_z\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height_z ~ weight_z \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.00    -0.00    -0.00 1.00     2171       NA\nweight_z      0.75      0.03     0.68     0.82 1.00     2171     2359\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.66      0.03     0.61     0.71 1.00     3050     2239\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nTaking a look at the estimates, we see the intercept is indeed 0 (we forced this). The estimate for the slope is 0.75, i.e., the correlation. This means that the estimate for sigma is the square root of 1 minus the variance of the slope estimate (0.75²). In our case, that's .66, which matches the estimate for sigma.",
    "supporting": [
      "bayesian-tutorial-simple-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}