{
  "hash": "f79abc945ab93749a5d0fd68ad51b7f6",
  "result": {
    "markdown": "---\ntitle: \"Bayesian tutorial: Single predictor regression\"\ndescription: \"The second of a series of tutorial posts on Bayesian analyses. In this post I focus on using `brms` to run a regression with a single predictor.\"\ndate: 2023-02-02\ncategories:\n  - statistics\n  - tutorial\n  - Bayesian statistics\n  - regression\ncode-fold: true\ncode-tools: true\ntoc: true\n---\n\n\nIn my previous [blog post](../5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.qmd) I showed how to use `brms` and `tidybayes` to run an intercept-only model. Now let's extend that model by adding a predictor.\n\nThe [data](Howell1.csv) is the same as in the previous post (including the filter that we only focus on people 18 years or older). This data contains weight data as well as height data, so that means we can run a model in which we regress heights onto weights, i.e., a regression with a single predictor.\n\nIf you want to follow along, run the following setup code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(marginaleffects)\nlibrary(tidybayes)\n\ntheme_set(theme_minimal())\ncolors <- c(\"#93CFDB\", \"#1E466E\")\n\noptions(brms.file_refit = \"on_change\")\n\ndata <- read_csv(\"Howell1.csv\")\ndata <- filter(data, age >= 18)\n```\n:::\n\n\n## Adding a single predictor\n\nThe formula syntax for a model in which we regress heights onto weights is `height ~ weight`. We can use this formula in `get_prior()` to see which priors we need to specify.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(height ~ weight, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    prior     class   coef group resp dpar nlpar lb ub\n                   (flat)         b                                   \n                   (flat)         b weight                            \n student_t(3, 154.3, 8.5) Intercept                                   \n     student_t(3, 0, 8.5)     sigma                               0   \n       source\n      default\n (vectorized)\n      default\n      default\n```\n:::\n:::\n\n\nThe output is a bit trickier compared to the intercept-only model output. There's the Intercept and sigma priors again, as well as two extra rows referring to a class called `b`. These two rows actually refer to the same prior, one refers specifically to the weight predictor and one refers to all predictors. If you run a model with many more predictors, you could set one prior that applies to all predictors. In this case though, we only have 1 predictor so it actually doesn't matter, both refer to the same prior.\n\nRecall from the previous post that I said writing down your model explicitly is a better way to understand what you're doing, so let's go ahead and do that.\n\n$$\\displaylines{heights_i âˆ¼ Normal(\\mu_i, \\sigma) \\\\ \\mu_i = \\alpha + \\beta x_i}$$\n\nWe again specify that the heights are normally distributed, so we still have a $\\mu$ and $\\sigma$, but this time the $\\mu$ is no longer a parameter to estimate. Instead, it's constructed from other parameters, $\\alpha$, $\\beta$, and an observed variable $x_i$ (the weight observations).\n\nIf you're used to linear regression equations, this notation should not surprise you. $\\alpha$ refers to the intercept and $\\beta$ to the slope.\n\nWe need to set priors on these parameters. The prior for $\\alpha$ can be the same as the prior for $\\mu$ from the previous intercept-only model if we center the data so the intercept refers to the average height of someone with an average weight, rather than someone with 0 weight (the default, which makes no sense). So let's first mean center the weight observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- mutate(data, weight_mc = weight - mean(weight))\n```\n:::\n\n\nNow we can use the same prior as before, which was a normal distribution with a mean of 160 and a standard deviation of 10 (assuming we did not update this as a result of the previous analysis).\n\nNext is the prior for the slope. This represents the relationship between weights and heights. For every 1 increase in weight, how much do we think that the height will increase or decrease? We could begin with an agnostic prior in which we do not specify the direction and instead just add some uncertainty so the slope can go in either direction. For example, let's put a normal distribution on the slope with a mean of 0 and a standard deviation of 10.\n\nFinally, we have the prior for sigma ($\\sigma$). To remind you, sigma refers to the standard deviation of the errors or the residual standard deviation. Now that we have a predictor that means the sigma can be less than what it was in the intercept-only model because some of the variance in heights might be explained by the weights, decreasing the size of the residuals and therefore sigma. So, if we believe in a relationship between heights and weights, we should change our prior for sigma so that it's lower. Given that we used a prior for the slope that is agnostic (there could be a positive, negative, or no relationship), our prior for sigma could be left unchanged because it was broad enough to allow for these possibilities.\n\n## Prior predictive check\n\nWe can again create a prior predictive check to see whether our priors actually make sense. However, instead of plotting the predicted distribution of heights, we're mostly interested in the relationship between weight and height, so we should plot a check of that relationship instead. We could simulate our own data like I did in the previous post or we can just run the Bayesian model and only draw from the prior, which I also did in the previous post and will do so again here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_height_weight_prior <- brm(\n  height ~ weight_mc,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(normal(160, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\"),\n      prior(normal(0, 10), class = \"b\")\n    ), \n  sample_prior = \"only\",\n  cores = 4,\n  seed = 4,\n  file = \"models/model_height_weight_prior.rds\"\n)\n```\n:::\n\n\nWe can use the `as_draws_df()` function to get draws from the posterior distribution of the intercept and slope parameters. With an intercept and slope we can visualize the relationship we're interested in. Remember, though, that `brms` will give you 4000 draws by default from the posteriors. In other words, you get 4000 intercepts and slopes. That's a bit much to visualize, so let's only draw 100 intercepts and slopes.\n\nTo help make sense of the sensibility of the slopes I've added the average weight to the weights so we're back on the normal scale and not the mean centered scale and I've added two dashed lines to indicate the minimum and maximum height we can expect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- model_height_weight_prior %>%\n  as_draws_df() %>%\n  slice_sample(n = 100)\n\nweight_mean <- data %>%\n  pull(weight) %>%\n  mean()\n\nggplot(data, aes(x = weight_mc, y = height)) +\n  geom_blank() +\n  geom_abline(\n    data = draws,\n    mapping = aes(intercept = b_Intercept, slope = b_weight_mc),\n    alpha = .25\n  ) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 272, linetype = \"dashed\") +\n  geom_label(x = 0, y = 260, label = \"Tallest person ever\") +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n```\n\n::: {.cell-output-display}\n![A prior predictive check of the relationship between weight and height](bayesian-tutorial-simple-regression_files/figure-html/prior-predictive-check-weight-1.png){width=672}\n:::\n:::\n\n\nThe plot shows a wide range of possible slopes, some of which are definitely unlikely because they lead to heights that are smaller than 0 or higher than the tallest person who ever lived. We should lower our uncertainty by reducing the standard deviation on the prior. In the next model I lower it to 3.\n\nAdditionally, the negative slopes are all pretty unlikely because we should expect a positive relationship between weight and height (taller people tend to be heavier). We could therefore also change our prior to force it to be positive using the `lb` argument in our prior for `b` or use a distribution that doesn't allow for any negative values. Let's not do this though. Let's assume we have no idea whether the relationship will be positive or negative and instead focus on the standard deviation instead so that we don't obtain relationships we definitely know are unlikely.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_height_weight_prior_2 <- brm(\n  height ~ weight_mc,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(normal(160, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\"),\n      prior(normal(0, 3), class = \"b\")\n    ), \n  sample_prior = \"only\",\n  cores = 4,\n  seed = 4,\n  file = \"models/model_height_weight_prior_2.rds\"\n)\n```\n:::\n\n\nLet's inspect the lines again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- model_height_weight_prior_2 %>%\n  as_draws_df() %>%\n  slice_sample(n = 100)\n\nggplot(data, aes(x = weight_mc, y = height)) +\n  geom_blank() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 272, linetype = \"dashed\") +\n  geom_abline(\n    data = draws,\n    mapping = aes(intercept = b_Intercept, slope = b_weight_mc),\n    alpha = .25\n  ) +\n  geom_label(x = 0, y = 260, label = \"Tallest person ever\") +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n```\n\n::: {.cell-output-display}\n![A prior predictive check of the relationship between weight and height](bayesian-tutorial-simple-regression_files/figure-html/prior-predictive-check-weight-update-1.png){width=672}\n:::\n:::\n\n\nThis looks a lot better, so let's run the model for real now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_height_weight <- brm(\n  data = data,\n  height ~ weight_mc,\n  family = gaussian,\n  prior = c(\n    prior(normal(160, 10), class = \"Intercept\"),\n    prior(cauchy(5, 5), class = \"sigma\"),\n    prior(normal(0, 3), class = \"b\")\n  ),\n  sample_prior = TRUE,\n  cores = 4,\n  seed = 4,\n  file = \"models/model_height_weight.rds\"\n)\n\nmodel_height_weight\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ weight_mc \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.61      0.27   154.09   155.15 1.00     4529     2939\nweight_mc     0.91      0.04     0.82     0.99 1.00     4267     3282\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.10      0.19     4.74     5.50 1.00     3555     2661\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nWe see that the estimate for the weight predictor is 0.91. For every increase of weight by 1 we can expect their height to increase by this number. We can also be fairly confident in this kind of relationship because the lower and upper bound of the 95% CI ranges from 0.82 to 0.99. These numbers are what we are usually interested in, but let's also plot the the entire posterior for the slope estimate so can see the entire distribution and not just this summary. Let's also add the prior so we can see how much that changed as a result of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- model_height_weight %>%\n  as_draws_df() %>%\n  pivot_longer(\n    cols = c(prior_Intercept, b_Intercept, prior_sigma, sigma),\n    names_to = \"distribution\"\n  ) %>%\n  mutate(\n    parameter = if_else(str_detect(distribution, \"sigma\"), \"sigma\", \"intercept\"),\n    distribution = if_else(\n      str_detect(distribution, \"prior\"), \"prior\", \"posterior\"\n    )\n  )\n\nggplot(results, aes(x = value, fill = fct_rev(distribution))) +\n  geom_histogram(binwidth = 0.05, position = \"identity\", alpha = .85) +\n  xlim(0, 5) +\n  labs(x = \"Slope\", y = \"\", fill = \"Distribution\") +\n  scale_fill_manual(values = colors)\n```\n\n::: {.cell-output-display}\n![](bayesian-tutorial-simple-regression_files/figure-html/weight-prior-posterior-1.png){width=672}\n:::\n:::\n\n\nApparently our prior was still very uninformed because the posterior shows we can be confident in a much narrower range of slopes!\n\nLet's also create another plot in which we plot the slope and its posterior against the observed data. The way to do this is by first creating a data frame containing weights that we want to predict the heights for. The (mean-centered) weights in the data range from -13.92 to 18, so we can roughly use that same range.\n\nThen we use `add_epred_draws()` to predict the expected height for each of the weights we stored in the data frame. This is not a single value. Instead, we get a distribution of possible heights for each weight value. We could plot all of these distributions, for example by creating a shaded region at each weight representing how likely the height is, or we can summarize that distribution of heights for each weight. The `tidybayes` package has the `median_qi()` function to summarize a distribution to a point and interval. By default it uses the median for the point summary and a 5% and 95% quartile range for the interval; the same summary we saw in the output from `brm`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslopes_qi <- tibble(\n  weight_mc = seq(from = -15, to = 20, by = 1)\n) %>%\n  add_epred_draws(model_height_weight) %>%\n  median_qi()\n\nggplot() +\n  geom_ribbon(\n    mapping = aes(ymin = .lower, ymax = .upper, x = weight_mc),\n    data = slopes_qi,\n    alpha = .25\n  ) +\n  geom_line(\n    mapping = aes(x = weight_mc, y = .epred),\n    data = slopes_qi\n  ) +\n  geom_point(\n    mapping = aes(x = weight_mc, y = height),\n    data = data,\n    alpha = .25\n  ) +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n```\n\n::: {.cell-output-display}\n![](bayesian-tutorial-simple-regression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThis graph is great because it shows us how confident we can be in the regression line. It does omit one source of uncertainty, though. The previous plot only shows the uncertainty about the regression line (the intercept and slope). We can also make a plot with predicted values of individual heights, which also incorporates the uncertainty from the $\\sigma$ parameter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredicted_slopes_qi <- tibble(\n  weight_mc = seq(from = -20, to = 20, by = 1)\n) %>%\n  add_predicted_draws(model_height_weight) %>%\n  median_qi()\n\nggplot() +\n  geom_ribbon(\n    aes(ymin = .lower, ymax = .upper, x = weight_mc),\n    data = predicted_slopes_qi,\n    alpha = .25\n  ) +\n  geom_line(\n    aes(x = weight_mc, y = .prediction),\n    data = predicted_slopes_qi\n  ) +\n  geom_point(\n    aes(x = weight_mc, y = height),\n    data = data,\n    alpha = .25\n  ) +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n```\n\n::: {.cell-output-display}\n![](bayesian-tutorial-simple-regression_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nWhile this graph is pretty cool, I haven't ever seen one in a social psychology paper, probably because academic psychologists are mostly interested in the parameters (e.g., means, correlations) rather than predicting individual observations.\n\n## Summary\n\nIn this post I showed how to run a single predictor model in `brms`. The addition of a predictor meant that the previous intercept-only model had to be updated by turning the $\\mu$ parameter into a regression equation. This then required an additional prior for the slope. To help set a prior on the slope, I created a prior predictive check of the slope. Running the model itself was straightforward and I provided several visualizations to help understand the results, including visualizing the posteriors of the slope parameter, the slope across the range of weights, and individual predicted heights.\n\nIn the next post I'll show how to use `brms` to analyze correlations.",
    "supporting": [
      "bayesian-tutorial-simple-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}