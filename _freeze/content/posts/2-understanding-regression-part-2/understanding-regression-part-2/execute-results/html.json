{
  "hash": "264ad4a368d66f17616e259839ace301",
  "result": {
    "markdown": "---\ntitle: Understanding regression (part 2)\ndescription: \"summary of part 2 of understanding regression\"\ndate: 2020-08-01\ncategories:\n  - statistics\n  - tutorial\ncode-fold: true\ntoc: true\ndraft: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(here)\n\n# Load a custom function to calculate the mode\nmode <- function(x) {\n  ux <- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\n# Read in Pokémon data\npokemon <- read_csv(here(\"data\", \"pokemon.csv\"))\n\n# Create a subset with only the first 25 Pokémon\npokemon25 <- filter(pokemon, pokedex <= 25)\npokemon25_50 <- filter(pokemon, pokedex > 25 & pokedex <= 50)\n\n# Create a vector with only the weights\nweights <- pull(pokemon, weight)\n\n# Set seed\nset.seed(42)\n\n# Set options\noptions(\n  knitr.kable.NA = \"-\", \n  digits = 2\n)\n\ntheme_set(theme_minimal())\n```\n:::\n\n\nIn Part 1 of 'Understanding Regression' we figured out where the estimate of an intercept-only regression model came from. It turned out to be the mean of the data. However, this was only the case if we defined our model's error as the sum of squared residuals. If we don't square the residuals, the best fitting value turned out to be the median of the data.\n\nWe also briefly discussed why one could favor squaring the residuals over not squaring them. By squaring the residuals we're punishing models that make larger mistakes. A residual of size 4 gets amplified to being an error of size 16, a residual of size 2 gets doubled to an error of 2 and a residual of 1 is an error of 1. This may simply be a property we like. We may prefer models that make many small errors over models that make large errors. If we do, we could square the residuals and use the mean as our best fitting model. But we can also approach this issue from the other way around. We can think about whether we prefer means over medians. It could be that the mean has certain properties different from that of the median and that may also be better. Part 2 of Understanding Regression is about whether this is the case.\n\n## Population vs. Sample\n\nRemember that our Pokémon weight model was based on the weights of the first 25 Pokémon. This is only a small sample of all Pokémon out there (our data has a total of 893). In the sample we observed, the average weight is 26.14. But what if we had instead focused on the next 25 Pokémon, from Raichu to Diglet? The average weight of these Pokémon is 20.68. That's a difference of 5.46.\n\nLet's actually take a look at the weights of all 893 Pokemon.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pokemon, aes(x = weight)) +\n  geom_histogram(color = \"black\", alpha = .75, binwidth = 25)\n```\n\n::: {.cell-output-display}\n![Weights of 893 Pokémon](understanding-regression-part-2_files/figure-html/fig-pokemon-weights-1.png){#fig-pokemon-weights width=672}\n:::\n:::\n\n\nThis figure shows the weights of *all* the Pokemon that are out there, and thus forms the population of Pokemon weights. Previously we took a *sample* from this population by only looking at 25 Pokemon of this population.\n\nNow imagine that we are not the only ones creating a Pokémon weight model. Instead, we are one of many who are doing so, and we all exchange our model information. We can tell others about our observation that weight = 26.144. In turn, others will give us their sample means. And let's further imagine that we also encounter people who used the median instead. Below I show some code to create these samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples <- crossing(\n    estimator = c(\"mean\", \"median\"),\n    n = c(10, 25, 100),\n    i = 1:10000\n  ) %>%\n  rowwise() %>%\n  mutate(\n    estimate = if_else(\n      condition = estimator == \"mean\", \n      true = mean(sample(weights, n), na.rm = TRUE),\n      false = median(sample(weights, n), na.rm = TRUE)\n    )\n  )\n```\n:::\n\n\nLet's begin by simply looking at the distribution of mean values. The following graph contains 10,000 means, each based on 25 different Pokémon.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeans25 <- filter(samples, estimator == \"mean\" & n == 25)\n\nggplot(means25, aes(x = estimate)) +\n  geom_density(alpha = .5, fill = \"black\") +\n  geom_vline(xintercept = mean(weights), linetype = \"dashed\") +\n  annotate(\"text\", x = 79, y = 0.0195, label = \"population mean\") +\n  labs(x = \"Sample mean\", y = \"Count\")\n```\n\n::: {.cell-output-display}\n![Distribution of 10,000 means based on 25 Pokémon each](understanding-regression-part-2_files/figure-html/fig-pokemon25-mean-weights-1.png){#fig-pokemon25-mean-weights width=672}\n:::\n:::\n\n\nThis distribution of means is also called a sampling distribution of means. In our case, we see that\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimators25 <- filter(samples, n == 25)\n\nggplot(estimators25, aes(x = estimate, fill = estimator)) +\n  geom_density(alpha = .75, adjust = 2) +\n  geom_vline(xintercept = mean(weights), linetype = \"solid\") +\n  geom_vline(xintercept = median(weights), linetype = \"dashed\") +\n  annotate(\n    geom = \"text\", \n    x = mean(weights), \n    y = 0.0465, \n    label = \"population mean\"\n  ) +\n  annotate(\n    geom = \"text\", \n    x = median(weights), \n    y = 0.049, \n    label = \"population median\",\n  ) +\n  labs(x = \"Sample mean\", y = \"Count\") +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75) +\n  coord_cartesian(ylim = c(0, 0.0425), clip = \"off\") +\n  theme(\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n```\n\n::: {.cell-output-display}\n![](understanding-regression-part-2_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "understanding-regression-part-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}