{
  "hash": "f0a53d06d1788f9632262d061b38ee0e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian tutorial: Logistic regression\"\ndescription: \"The xth of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to run a logistic regression.\"\ndate: 2023-02-12\ncategories:\n  - statistics\n  - tutorial\n  - Bayesian statistics\n  - regression\ncode-fold: true\ncode-tools: true\ntoc: true\nformat: \n  html: \n    df-print: kable\ndraft: true\n---\n\n\nThis post is about using brms to run a logistic regression. Logistic regression is different from the models from previous posts because it involves a dichotomous outcome measure. This is different from modeling means, which we have roughly been doing, so there are some new things to keep in mind here.\n\nRun the following code to get started. Note that the data we'll be using is the titanic dataset. It contains several interesting columns such as whether or not the passenger survived, their sex, and their age.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(modelr)\n\ntheme_set(theme_minimal())\nblue_1 <- \"#d1e1ec\"\nblue_2 <- \"#b3cde0\"\nblue_3 <- \"#6497b1\"\nblue_4 <- \"#005b96\"\nblue_5 <- \"#03396c\"\nblue_6 <- \"#011f4b\"\n\noptions(\n  mc.cores = 4,\n  brms.threads = 4,\n  brms.backend = \"cmdstanr\",\n  brms.file_refit = \"on_change\"\n)\n\ndata <- read_csv(\"titanic.csv\")\n```\n:::\n\n\nLet's say that we want to model the proportion of survivors. We can do that by running a logistic regression in which we regress the outcome (`survived`: `0` or `1`) on an intercept and specify that the outcome comes from a binomial family with a logit transformation. We use the `get_prior()` function to figure out which priors we need.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(survived ~ 1, data = data, family = binomial(link = \"logit\"))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|prior                |class     |coef |group |resp |dpar |nlpar |lb |ub |source  |\n|:--------------------|:---------|:----|:-----|:----|:----|:-----|:--|:--|:-------|\n|student_t(3, 0, 2.5) |Intercept |     |      |     |     |      |   |   |default |\n\n</div>\n:::\n:::\n\n\nWe only need a single prior for the intercept. That's easy enough, but the family and link argument make things a little bit more complicated.\n\nBefore we set the priors, let's also write down the formula for this model.\n\n$$\\displaylines{survived_i ∼ Binomial(n, p_i) \\\\ logit(p_i) = \\alpha}$$\n\nNotice that there is an *n* in the formula. This refers to the number of trials while *p* refers to the proportion of successes. In our case, we can simplify the formula and use the Bernoulli family instead of the Binomial family because the way our data is structured *n* is always 1. Each row is a single observation about whether the passenger survived or not. We also could have had an alternative structure in which we simply counted the number of passengers who survived and who didn't, and run a model on this aggregated data. In that case, we would need to use the Binomial family and specify the number of observations (i.e., trials). We don't need to do that, so our formula is:\n\n$$\\displaylines{survived_i ∼ Bernoulli(p_i) \\\\ logit(p_i) = \\alpha}$$\n\nBoth formulas show that we're applying a logit transformation and that we only need to set a single prior.\n\n## Setting the priors\n\nYou might think that because we're modelling a proportion, the prior for the intercept should be a distribution of values constrained to range from 0 to 1. This is not the case because of the logit transformation. The logit link transforms the probability scale onto a linear scale that ranges from minus infinity to infinity, although the bulk of the values are around 0. We can show this in a graph by applying the transformation to values ranging from 0 to 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tibble(\n  x = seq(0, 1, 0.001),\n  logit = logit_scaled(x)\n)\n\nggplot(df, aes(x = x, y = logit)) +\n  geom_point() +\n  labs(x = \"p\", y = \"logit(p)\")\n```\n\n::: {.cell-output-display}\n![Transformation of probability values via the logit link](bayesian-tutorial-logistic-regression_files/figure-html/logit-link-1.png){width=672}\n:::\n:::\n\n\nWhile the graph does not show that it ranges from one infinity to another, it does show we're no longer on the probabilty scale. The numbers on the y-axis do not range from 0 to 1.\n\nOur prior needs to be set on the transformed values, not the probability scale. brms sets a default prior of `student_t(3, 0, 2.5)`. Let's take a look at this prior by running a model and sampling only from this prior (i.e., conducting a prior predictive check).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_prior_default <- brm(\n  survived ~ 1,\n  family = bernoulli(link = \"logit\"),\n  data = data,\n  file = \"models/model-prior-default.rds\",\n  sample_prior = \"only\"\n)\n\nfit_prior_default\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: survived ~ 1 \n   Data: data (Number of observations: 887) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.40      5.72   -10.77     7.76 1.00     1102      711\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\nWe get an estimate of -0.4 and a 95% CI that ranges from -10.77 to 7.76. Let's plot the entire prior distribution, both transformed and untransformed.\n\n\n::: {.cell layout-ncol=\"1\"}\n\n```{.r .cell-code}\ndraws <- fit_prior_default %>%\n  spread_draws(b_Intercept) %>%\n  mutate(b_Intercept_scaled = inv_logit_scaled(b_Intercept))\n\nggplot(draws, aes(x = b_Intercept)) +\n  geom_histogram(binwidth = 1, fill = blue_3)\n\nggplot(draws, aes(x = b_Intercept_scaled)) +\n  geom_histogram(binwidth = 0.01, fill = blue_3)\n```\n\n::: {.cell-output-display}\n![Logit scale](bayesian-tutorial-logistic-regression_files/figure-html/single-proportion-default-prior-plot-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![Probability scale](bayesian-tutorial-logistic-regression_files/figure-html/single-proportion-default-prior-plot-2.png){width=672}\n:::\n\nDefault brms prior on the intercept in a logistic regression\n:::\n\n\nThe top graph shows the prior on the logit scale. The bulk of the values are somewhere between -8 and 8, concentrated around 0. This doesn't seem like a suitable default prior because if we convert these values to the probability scale, it turns out that the extreme values (around 0 and 1) are relatively more likely than other values. I prefer a prior that is either more uniform or perhaps centered around .5, depending on the context. This is likely to be a weak prior though, so it will be easily overrun by data. Nevertheless, let's try out another prior and see if it produces a more uniform distribution on the probability scale.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_prior <- brm(\n  survived ~ 1,\n  family = bernoulli(link = \"logit\"),\n  data = data,\n  prior = prior(student_t(5, 0, 1.5), class = \"Intercept\"),\n  file = \"models/model-prior.rds\",\n  sample_prior = \"only\"\n)\n\ndraws <- fit_prior %>%\n  spread_draws(b_Intercept) %>%\n  mutate(b_Intercept_scaled = inv_logit_scaled(b_Intercept))\n\nggplot(draws, aes(x = b_Intercept_scaled)) +\n  geom_histogram(binwidth = 0.01, fill = blue_3)\n```\n\n::: {.cell-output-display}\n![](bayesian-tutorial-logistic-regression_files/figure-html/single-proportion-prior-1.png){width=672}\n:::\n:::\n\n\nThat looks better, so let's use this prior and estimate the probability of survivors in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- brm(\n  survived ~ 1,\n  family = bernoulli(link = \"logit\"),\n  data = data,\n  prior = prior(student_t(5, 0, 1.5), class = \"Intercept\"),\n  file = \"models/model.rds\",\n)\n\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: survived ~ 1 \n   Data: data (Number of observations: 887) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.47      0.07    -0.60    -0.33 1.00     1199     1788\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\nWe get an estimate of -0.47. Converted to a probability, this is `r round(inv_logit_scales(fixef(fit)[1]), 2). We can do better, of course, and plot a distribution the plausible probability values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- fit %>%\n  spread_draws(b_Intercept) %>%\n  mutate(b_Intercept_scaled = inv_logit_scaled(b_Intercept))\n\nggplot(draws, aes(x = b_Intercept_scaled)) +\n  geom_histogram(binwidth = 0.005, fill = blue_3) +\n  scale_x_continuous(limits = c(0, 1))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Posterior distribution of the probability of](bayesian-tutorial-logistic-regression_files/figure-html/single-proportion-plot-1.png){width=672}\n:::\n:::\n\n\nThe median proportion of survivors is 0.39, with a 95% CI ranging from 0.42 to 0.42.\n\n## Adding a discrete predictor\n\nNext, let's add a discrete predictor, like the passenger's sex. We begin, once again, by checking which priors we need. Note that we'll use the index coding notation (so no intercept).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(survived ~ 0 + sex, family = bernoulli(link = \"logit\"), data = data)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|prior |class |coef      |group |resp |dpar |nlpar |lb |ub |source  |\n|:-----|:-----|:---------|:-----|:----|:----|:-----|:--|:--|:-------|\n|      |b     |          |      |     |     |      |   |   |default |\n|      |b     |sexfemale |      |     |     |      |   |   |default |\n|      |b     |sexmale   |      |     |     |      |   |   |default |\n\n</div>\n:::\n:::\n\n\nWe need two priors, one for each of two sexes. The formula:\n\n$$\\displaylines{survived_i ∼ Bernoulli(p_i) \\\\ logit(p_i) = \\alpha_{sex[i]}}$$\n\nBelow we run the model with the same prior we used before, but specified for both male and female passengers separately.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_sex <- brm(\n  survived ~ 0 + sex,\n  family = bernoulli(link = \"logit\"),\n  data = data,\n  prior = c(\n    prior(student_t(5, 0, 1.5), coef = \"sexfemale\"),\n    prior(student_t(5, 0, 1.5), coef = \"sexmale\")\n  ),\n  file = \"models/model-sex.rds\"\n)\n\nfit_sex\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: survived ~ 0 + sex \n   Data: data (Number of observations: 887) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsexfemale     1.06      0.13     0.80     1.31 1.00     3198     2331\nsexmale      -1.44      0.11    -1.65    -1.23 1.00     3738     2547\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\nA downside with models that use transformations is that the estimates are not that interpretable anymore, so let's just go straight into converting and plotting them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- fit_sex %>%\n  gather_draws(b_sexfemale, b_sexmale) %>%\n  mutate(\n    .variable = case_match(\n      .variable,\n      \"b_sexfemale\" ~ \"female\", \"b_sexmale\" ~ \"male\"\n    ),\n    .value = inv_logit_scaled(.value)\n  )\n\nggplot(draws, aes(x = .value, fill = .variable)) +\n  geom_histogram(binwidth = 0.01) +\n  labs(x = \"proportion of survivors\", y = \"\", fill = \"Sex\") +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_fill_manual(values = c(blue_2, blue_4))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 4 rows containing missing values (`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Posterior distributions of survival proportions by sex](bayesian-tutorial-logistic-regression_files/figure-html/discrete-predictor-plot-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndata_grid(data, sex) |>\n  add_epred_draws(fit_sex) |>\n  ggplot(aes(x = .epred, fill = sex)) +\n  geom_histogram(binwidth = 0.01) +\n  labs(x = \"proportion of survivors\", y = \"\", fill = \"Sex\") +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_fill_manual(values = c(blue_2, blue_4))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 4 rows containing missing values (`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Posterior distributions of survival proportions by sex](bayesian-tutorial-logistic-regression_files/figure-html/discrete-predictor-plot-2.png){width=672}\n:::\n:::\n\n\nLooks like a sizeable difference between the two sexes, which is not at all surprising of course. Women and children were put in lifeboats first, leading to more women surviving than men. We can calculate the size of this difference by simply converting the estimates to proportions and calculating the difference.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_sex %>%\n  spread_draws(b_sexfemale, b_sexmale) %>%\n  mutate(\n    b_sexfemale_scaled = inv_logit_scaled(b_sexfemale),\n    b_sexmale_scaled = inv_logit_scaled(b_sexmale),\n    difference = b_sexfemale_scaled - b_sexmale_scaled\n  ) %>%\n  select(difference) %>%\n  median_hdci()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| difference|    .lower|    .upper| .width|.point |.interval |\n|----------:|---------:|---------:|------:|:------|:---------|\n|  0.5504736| 0.4926149| 0.6076839|   0.95|median |hdci      |\n\n</div>\n:::\n:::\n\n\n## Adding a continuous predictor\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(survived ~ age, family = bernoulli(link = \"logit\"), data = data)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|prior                |class     |coef |group |resp |dpar |nlpar |lb |ub |source  |\n|:--------------------|:---------|:----|:-----|:----|:----|:-----|:--|:--|:-------|\n|                     |b         |     |      |     |     |      |   |   |default |\n|                     |b         |age  |      |     |     |      |   |   |default |\n|student_t(3, 0, 2.5) |Intercept |     |      |     |     |      |   |   |default |\n\n</div>\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit_age <- brm(\n#   survived ~ age,\n#   family = bernoulli(link = \"logit\"),\n#   data = data,\n#   prior = c(\n#     prior(student_t(5, 0, 1.5), class = \"Intercept\"),\n#     prior(student_t(5, 0, 0.5), class = \"b\")\n#   ),\n#   sample_prior = \"only\"\n#   # file = \"models/model-sex.rds\"\n# )\n\n# data %>%\n#   data_grid(age = seq_range(age, 10)) %>%\n#   add_epred_draws(fit_age) %>%\n#   ggplot(aes(x = age, y = .epred)) +\n#   stat_lineribbon() +\n#   scale_y_continuous(limits = c(0, 1)) +\n#   scale_fill_brewer(palette = \"Greys\")\n\n# draws <- fit_sex %>%\n#   gather_draws(b_sexfemale, b_sexmale) %>%\n#   mutate(\n#     .variable = case_match(\n#       .variable,\n#       \"b_sexfemale\" ~ \"female\", \"b_sexmale\" ~ \"male\"\n#     ),\n#     .value = inv_logit_scaled(.value)\n#   )\n\n# ggplot(draws, aes(x = .value, y = .variable)) +\n#   stat_slab() +\n#   stat_dotsinterval(side = \"bottom\", .width = .95) +\n#   labs(x = \"proportion of survivors\", y = \"\") +\n#   scale_x_continuous(limits = c(0, 1))\n```\n:::",
    "supporting": [
      "bayesian-tutorial-logistic-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}