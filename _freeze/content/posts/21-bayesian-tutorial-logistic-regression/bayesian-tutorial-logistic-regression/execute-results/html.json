{
  "hash": "e2f88084a04fe7731e2cdb67dac035e1",
  "result": {
    "markdown": "---\ntitle: \"Bayesian tutorial: Logistic regression\"\ndescription: \"The xth of a series of tutorial posts on Bayesian analyses. In this post I focus on using `brms` to run a logistic regression.\"\ndate: 2023-02-12\ncategories:\n  - statistics\n  - tutorial\n  - Bayesian statistics\n  - regression\ncode-fold: true\ncode-tools: true\ntoc: true\nformat: \n  html: \n    df-print: kable\ndraft: true\n---\n\n\nThis post is about using `brms` to run a logistic regression. Logistic regression is different from the models from previous posts because it involves a dichotomous outcome measure. This is different from modeling means, which we have roughly been doing, so there are some new things to keep in mind here.\n\nRun the following code to get started. Note that the data we'll be using is the titanic dataset. It contains several interesting columns such as whether or not the passenger survived, their sex, and their age.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\n\ndata <- read_csv(\"titanic.csv\")\n\ntheme_set(theme_minimal())\n\ncolors <- c(\"#93CFDB\", \"#1E466E\")\n```\n:::\n\n\nLet's say that we want to model the proportion of survivors. We can do that by running a logistic regression in which we regress the outcome (`survived`: `0` or `1`) on an intercept and specify that the outcome comes from a binomial family with a logit transformation. We use the `get_prior()` function to get a first grasp of which priors we need.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(survived ~ 1, data = data, family = binomial(link = \"logit\"))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|prior                |class     |coef |group |resp |dpar |nlpar |lb |ub |source  |\n|:--------------------|:---------|:----|:-----|:----|:----|:-----|:--|:--|:-------|\n|student_t(3, 0, 2.5) |Intercept |     |      |     |     |      |   |   |default |\n\n</div>\n:::\n:::\n\n\nWe only need a single prior for the intercept. That's easy enough, but the family and link argument are kinda new. \n\n$$\\displaylines{survived_i âˆ¼ Binomial(n, p_i) \\\\ logit(p_i) = \\alpha}$$\n\n## Setting the priors\n\nYou might think that because we're modelling a proportion, the prior for the intercept should be a distribution of values constrained to range from 0 to 1. This is not the case because of the logit transformation. The logit link transforms the probability scale onto a linear scale that ranges from minus infinity to infinity, although the bulk of the values are around 0. We can show this in a graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tibble(\n  x = seq(0, 1, 0.001),\n  logit = logit_scaled(x)\n)\nggplot(df, aes(x = x, y = logit)) +\n  geom_point() +\n  labs(x = \"p\", y = \"logit(p)\")\n```\n\n::: {.cell-output-display}\n![Transformation of probability values via the logit link](bayesian-tutorial-logistic-regression_files/figure-html/logit-link-1.png){width=672}\n:::\n:::\n\n\nOur prior needs to be set on the transformed values, not the original probability scale. brms sets a default prior of `student_t(3, 0, 2.5)`. Let's take a look at what this looks like by running a model and sampling only from this prior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_prior_default <- brm(\n  survived ~ 1,\n  family = binomial(link = \"logit\"),\n  data = data,\n  file = \"model-prior-default.rds\", \n  sample_prior = \"only\"\n)\n```\n:::\n\n\nWe get a warning that using the Bernoulli family might be more efficient because we only have 2 levels and that we need to specify the number of trials when using the Binomial family; let's keep this in mind when we run the code again. For now, let's plot the prior.\n\n\n::: {.cell layout-ncol=\"1\"}\n\n```{.r .cell-code}\ndraws_prior <- as_draws_df(fit_prior_default)\ndraws_prior_inv <- mutate(\n  draws_prior,\n  b_Intercept = inv_logit_scaled(b_Intercept)\n)\n\nggplot(draws_prior, aes(x = b_Intercept, y = \"\")) +\n  stat_halfeye(.width = c(.95)) +\n  stat_pointinterval(\n    geom = \"text\",\n    aes(label = paste0(\n      round(after_stat(x), 2), \"\\n [\",\n      round(after_stat(xmin), 2), \", \",\n      round(after_stat(xmax), 2), \"]\"\n    )),\n    .width = .5,\n    position = position_nudge(y = -0.1),\n    size = 3.5, color = \"gray20\"\n  )\n\nggplot(draws_prior_inv, aes(x = b_Intercept, y = \"\")) +\n  stat_halfeye(.width = c(.95)) +\n  stat_pointinterval(\n    geom = \"text\",\n    aes(label = paste0(\n      round(after_stat(x), 2), \"\\n [\",\n      round(after_stat(xmin), 2), \", \",\n      round(after_stat(xmax), 2), \"]\"\n    )),\n    .width = .5,\n    position = position_nudge(y = -0.1),\n    size = 3.5, color = \"gray20\"\n  )\n```\n\n::: {.cell-output-display}\n![Logit scale](bayesian-tutorial-logistic-regression_files/figure-html/default-prior-plot-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![Probability scale](bayesian-tutorial-logistic-regression_files/figure-html/default-prior-plot-2.png){width=672}\n:::\n\nDefault brms prior on the intercept in a logistic regression\n:::\n\n\nThe top graph shows the prior on the logit scale. The bulk of the values are somewhere between -10 and 10, concentrated around 0. This doesn't seem like a suitable default prior to me because if we convert these values to the probability scale, it turns out that the extreme values (around 0 and 1) are relatively more likely than other values. I would probably prefer a prior that is either more uniform or perhaps centered around .5, depending on the context. This is likely to be a weak prior though, so it will be easily overrun by data. Nevertheless, let's try out another prior and see if it produces a more uniform distribution on the probability scale. Let's also change the `binomial()` family to a `bernoulli()` family.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_prior <- brm(\n  survived ~ 1,\n  family = bernoulli(link = \"logit\"),\n  data = data,\n  prior = prior(student_t(5, 0, 1.5), class = \"Intercept\"),\n  # file = \"model-prior.rds\",\n  sample_prior = \"only\",\n  warmup = 2000,\n  iter = 20000\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCompiling Stan program...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nStart sampling\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL '1bd53010df1aa9c316b58b614576fd0f' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:     1 / 20000 [  0%]  (Warmup)\nChain 1: Iteration:  2000 / 20000 [ 10%]  (Warmup)\nChain 1: Iteration:  2001 / 20000 [ 10%]  (Sampling)\nChain 1: Iteration:  4000 / 20000 [ 20%]  (Sampling)\nChain 1: Iteration:  6000 / 20000 [ 30%]  (Sampling)\nChain 1: Iteration:  8000 / 20000 [ 40%]  (Sampling)\nChain 1: Iteration: 10000 / 20000 [ 50%]  (Sampling)\nChain 1: Iteration: 12000 / 20000 [ 60%]  (Sampling)\nChain 1: Iteration: 14000 / 20000 [ 70%]  (Sampling)\nChain 1: Iteration: 16000 / 20000 [ 80%]  (Sampling)\nChain 1: Iteration: 18000 / 20000 [ 90%]  (Sampling)\nChain 1: Iteration: 20000 / 20000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.007355 seconds (Warm-up)\nChain 1:                0.064394 seconds (Sampling)\nChain 1:                0.071749 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '1bd53010df1aa9c316b58b614576fd0f' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:     1 / 20000 [  0%]  (Warmup)\nChain 2: Iteration:  2000 / 20000 [ 10%]  (Warmup)\nChain 2: Iteration:  2001 / 20000 [ 10%]  (Sampling)\nChain 2: Iteration:  4000 / 20000 [ 20%]  (Sampling)\nChain 2: Iteration:  6000 / 20000 [ 30%]  (Sampling)\nChain 2: Iteration:  8000 / 20000 [ 40%]  (Sampling)\nChain 2: Iteration: 10000 / 20000 [ 50%]  (Sampling)\nChain 2: Iteration: 12000 / 20000 [ 60%]  (Sampling)\nChain 2: Iteration: 14000 / 20000 [ 70%]  (Sampling)\nChain 2: Iteration: 16000 / 20000 [ 80%]  (Sampling)\nChain 2: Iteration: 18000 / 20000 [ 90%]  (Sampling)\nChain 2: Iteration: 20000 / 20000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.007351 seconds (Warm-up)\nChain 2:                0.065498 seconds (Sampling)\nChain 2:                0.072849 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '1bd53010df1aa9c316b58b614576fd0f' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:     1 / 20000 [  0%]  (Warmup)\nChain 3: Iteration:  2000 / 20000 [ 10%]  (Warmup)\nChain 3: Iteration:  2001 / 20000 [ 10%]  (Sampling)\nChain 3: Iteration:  4000 / 20000 [ 20%]  (Sampling)\nChain 3: Iteration:  6000 / 20000 [ 30%]  (Sampling)\nChain 3: Iteration:  8000 / 20000 [ 40%]  (Sampling)\nChain 3: Iteration: 10000 / 20000 [ 50%]  (Sampling)\nChain 3: Iteration: 12000 / 20000 [ 60%]  (Sampling)\nChain 3: Iteration: 14000 / 20000 [ 70%]  (Sampling)\nChain 3: Iteration: 16000 / 20000 [ 80%]  (Sampling)\nChain 3: Iteration: 18000 / 20000 [ 90%]  (Sampling)\nChain 3: Iteration: 20000 / 20000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.007517 seconds (Warm-up)\nChain 3:                0.06504 seconds (Sampling)\nChain 3:                0.072557 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '1bd53010df1aa9c316b58b614576fd0f' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:     1 / 20000 [  0%]  (Warmup)\nChain 4: Iteration:  2000 / 20000 [ 10%]  (Warmup)\nChain 4: Iteration:  2001 / 20000 [ 10%]  (Sampling)\nChain 4: Iteration:  4000 / 20000 [ 20%]  (Sampling)\nChain 4: Iteration:  6000 / 20000 [ 30%]  (Sampling)\nChain 4: Iteration:  8000 / 20000 [ 40%]  (Sampling)\nChain 4: Iteration: 10000 / 20000 [ 50%]  (Sampling)\nChain 4: Iteration: 12000 / 20000 [ 60%]  (Sampling)\nChain 4: Iteration: 14000 / 20000 [ 70%]  (Sampling)\nChain 4: Iteration: 16000 / 20000 [ 80%]  (Sampling)\nChain 4: Iteration: 18000 / 20000 [ 90%]  (Sampling)\nChain 4: Iteration: 20000 / 20000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.007335 seconds (Warm-up)\nChain 4:                0.065061 seconds (Sampling)\nChain 4:                0.072396 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\ndraws_prior <- fit_prior %>%\n  as_draws_df() %>%\n  mutate(\n    b_Intercept = inv_logit_scaled(b_Intercept)\n  )\n\nggplot(draws_prior, aes(x = b_Intercept)) +\n  # stat_halfeye(.width = c(.95))\n  geom_histogram(binwidth = 0.01)\n```\n\n::: {.cell-output-display}\n![](bayesian-tutorial-logistic-regression_files/figure-html/prior-1.png){width=672}\n:::\n:::\n\n\nThat looks better, so let's use this prior and estimate the probability of males in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_intercept <- brm(\n  survived ~ 1,\n  family = bernoulli(link = \"logit\"),\n  data = data,\n  prior = prior(student_t(6, 0, 1.5), class = \"Intercept\"),\n  file = \"model-intercept-only.rds\",\n)\n\nfit_intercept\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: survived ~ 1 \n   Data: data (Number of observations: 887) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.46      0.07    -0.60    -0.34 1.00     1259     1819\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nWe get an estimate of -0.46. Converted to a probability, this is `r round(inv_logit_scales(summary(fit_intercept)$fixed$Estimate), 2). We can do better, of course, and plot a distribution the plausible probability values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws_intercept <- fit_intercept %>%\n  as_draws_df() %>%\n  mutate(b_Intercept = inv_logit_scaled(b_Intercept))\n\nggplot(draws_intercept, aes(x = b_Intercept)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![Posterior distribution of the probability of males](bayesian-tutorial-logistic-regression_files/figure-html/single-probability-1.png){width=672}\n:::\n:::\n\n\n## Adding a discrete predictor\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(survived ~ 0 + sex, family = bernoulli(link = \"logit\"), data = data)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|prior |class |coef      |group |resp |dpar |nlpar |lb |ub |source  |\n|:-----|:-----|:---------|:-----|:----|:----|:-----|:--|:--|:-------|\n|      |b     |          |      |     |     |      |   |   |default |\n|      |b     |sexfemale |      |     |     |      |   |   |default |\n|      |b     |sexmale   |      |     |     |      |   |   |default |\n\n</div>\n:::\n:::\n\n\n$$\\displaylines{males_i âˆ¼ Binomial(n, p_i) \\\\ logit(p_i) = \\alpha + \\beta x_i}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- brm(\n  survived ~ 0 + sex,\n  family = bernoulli(link = \"logit\"),\n  data = data,\n  prior = c(\n    prior(student_t(6, 0, 1.5), coef = \"sexfemale\"),\n    prior(student_t(6, 0, 1.5), coef = \"sexmale\")\n  ),\n  file = \"model-sex.rds\",\n)\n\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: survived ~ 0 + sex \n   Data: data (Number of observations: 887) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsexfemale     1.05      0.13     0.80     1.32 1.00     3128     2452\nsexmale      -1.45      0.11    -1.66    -1.23 1.00     3338     2478\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n```{.r .cell-code}\ndraws <- fit %>%\n  as_draws_df() %>%\n  mutate(\n    b_sexfemale = inv_logit_scaled(b_sexfemale),\n    b_sexmale = inv_logit_scaled(b_sexmale),\n    difference = b_sexfemale - b_sexmale\n  )\n\nlibrary(ggdist)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'ggdist'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n```\n:::\n\n```{.r .cell-code}\ndraws %>%\n  pivot_longer(\n    cols = c(b_sexfemale, b_sexmale),\n    names_to = \"sex\", values_to = \"prob\"\n  ) %>%\n  mutate(\n    sex = case_match(sex, \"b_sexfemale\" ~ \"female\", \"b_sexmale\" ~ \"male\")\n  ) %>%\n  ggplot(aes(x = prob, y = sex)) +\n  stat_halfeye(.width = c(.95)) +\n  stat_pointinterval(\n    geom = \"text\",\n    aes(label = paste0(\n      round(after_stat(x), 2), \"\\n [\",\n      round(after_stat(xmin), 2), \", \",\n      round(after_stat(xmax), 2), \"]\"\n    )),\n    .width = .5,\n    position = position_nudge(y = -0.1),\n    size = 3.5\n  ) +\n  scale_x_continuous(limits = c(0, 1))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n::: {.cell-output-display}\n![](bayesian-tutorial-logistic-regression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(draws, aes(x = difference)) +\n  geom_density(fill = \"steelblue\") +\n  scale_x_continuous(limits = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](bayesian-tutorial-logistic-regression_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n:::",
    "supporting": [
      "bayesian-tutorial-logistic-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}