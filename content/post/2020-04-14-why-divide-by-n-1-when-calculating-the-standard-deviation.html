---
title: Why divide by N - 1 when calculating the standard deviation?
date: '2020-04-14'
slug: why-divide-by-n-1
tags: [statistics]
draft: true
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>The formula for calculating the variance is:</p>
<p><span class="math display">\[\frac{\sum(x_i - \overline{x})^2}{n}\]</span></p>
<p>This makes sense. The variance is a measure of the dispersion around the mean, so we calculate all the deviations from the mean (<span class="math inline">\(x_i - \overline{x}\)</span>) and sum them. We then divide this sum by the number of observations as a scaling factor. If we ignore this number, we could get a very high variance simply by observing a lot of data. So, to fix that problem, we divide by the total number of observations.</p>
<p>However, this is the formula for the <em>population</em> variance. The formula for calculating the variance of a sample is:</p>
<p><span class="math display">\[\frac{\sum(x_i - \overline{x})^2}{n - 1}\]</span></p>
<p>Why do we divide by <em>n</em> - 1 when calculating the variance of a sample?</p>
<p>Letâ€™s see what happens if we calculate the variance by dividing by <em>n</em>, rather than <em>n</em> - 1.</p>
<p><img src="/post/2020-04-14-why-divide-by-n-1-when-calculating-the-standard-deviation_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
