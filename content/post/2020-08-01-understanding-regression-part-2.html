---
title: "Understanding Regression (Part 2)"
date: "8/1/2020"
draft: true
---



<p>However, estimating the model comes with a significant problem. Our model is now completely dependent on the data we sampled. Remember that we focused our attention on the weights of only the first 25 Pokémon. In this sample, the average weight is 26.144. But what if we instead focused on the next 25 Pokémon, from Raichu to Diglet? The average weight of these Pokémon is 20.684. That’s a difference of 5.46. This raises the question: how stable is our mean estimate?</p>
<div id="sampling-distribution-of-the-mean" class="section level2">
<h2>Sampling distribution of the mean</h2>
<p>Imagine that we are not the only ones creating a Pokémon weight model. Instead, we are one of many who are doing so, and we all exchange our model information. We can tell others about our observation that weight = 26.144. In turn, others will give us their sample means. This enables us to plot the distribution of mean values.</p>
<p>In the following graph I plot the distribution of 10.000 means. Each of these means is an average of 25 randomly selected Pokémon.</p>
<p><img src="/post/2020-08-01-understanding-regression-part-2_files/figure-html/unnamed-chunk-1-1.png" width="100%" /></p>
<p>What if we base our mean on more than just 25 Pokémon? Or even fewer Pokémon? Below I plot what the distribution of means looks like at varying sample sizes.</p>
<p><img src="/post/2020-08-01-understanding-regression-part-2_files/figure-html/unnamed-chunk-2-1.png" width="100%" /></p>
<p>Two things jump out. First, as the sample size increases, the distribution of means becomes more narrow. This is fairly easy to understand; more data means we are more likely to capture the range of weight values, giving us a better estimate of the overall mean. With small sample sizes, the mean is more dependent on exactly which weights were drawn from the larger population of weights. Second, we see that as the sample size increases, so does the average weight. There appears to be a bias. This bias is caused by the fact that the weights are not normally distributed. Weights are always a positive number (you can’t weight -6 kg, for instance). Since some Pokémon are very heavy (e.g., Cosmoem and Celesteela both weigh 999.9 kg), the distribution of weights is skewed towards larger weights. The heavy Pokémon have a significant impact on the average weight. Smaller samples are more likely to miss these heavy Pokémon, resulting a lower estimates.</p>
<p>The fact our data is non-normally distributed might actually be a reason to favor an alternative statistical technique to estimate the weight, such as the median. After all, the median is less sensitive to a few large values that skew the distribution. A downside of the median is that it is less efficient than the mean when estimating the center of a collection of values. Intuitively, this can be understood because the median simply ranks values and picks the middle option. It ignores a lot of information about the actual distance between the values, which also makes it so that it is less affected by a few large values. A downside is that this does mean we need more data points relative to methods that take into account more information, such as the mean.</p>
<p>We could fix this issue by log-transforming the weight values. Below I plot the same graph as before, but on log-transformed data. Log-transforming the weights makes large values less influential, and causes the distribution of weights to more closely resemble a normal distribution.</p>
<p><img src="/post/2020-08-01-understanding-regression-part-2_files/figure-html/unnamed-chunk-3-1.png" width="100%" /></p>
<p>Now are means seem to behave appropriately.</p>
<p>Our next challenge is how to describe the uncertainty in the mean distributions. We see that when the average weight is based on a sample of 10 Pokémon, the distribution of means is wider than when the average weight is based on 100 Pokémon. But how much wider?</p>
<p>The width of the sample mean distribution is a function of two things: 1) the sample size, and 2) the distribution of the residuals. We already covered how the sample size is relevant, so let’s focus on the distribution of the residuals.</p>
<p>Let’s visualize the residuals of our <code>weight = mean(weight)</code> model, or, to be more accurate now, our <code>weight_log = mean(weight_log)</code> model.</p>
<p><img src="/post/2020-08-01-understanding-regression-part-2_files/figure-html/unnamed-chunk-4-1.png" width="100%" /></p>
<p>The horizontal line represents the mean of the log-transformed weights. We see that it does not perfectly describe each data point, although we know this is the value that minimizes the sum of the squared residuals. We can now also see that the weights show us just how reliable the mean estimate is. If the Pokémon would have the same weights, the mean would better capture these weights, and the residuals would be smaller. If, and this appears to be the case, Pokémon vary wildly in terms of weight, the mean is often an incorrect description of a Pokémon’s weight, and as a result it is also more likely to vary from sample to sample. The mean is heavily influenced by exactly which Pokémon it has sampled from.</p>
<p>We can describe the variation in residuals using the very definition of the error we have maintained so far: the sum of squared residuals. However, this description ignores an important piece of information, which is the sample size. Large sums of squared residuals could be due to a few very large residuals or to many small residuals. To better capture the accuracy of the model, we should divide the sum of squared residuals by the sample size.</p>
<p>Actually, we should divide the sum of squared residuals by the sample size <em>minus one</em>. Why? Dividing by the sample size would give us a biased estimate of the variation in residuals. We must remember that we are estimating the variation of the residuals with regards to the mean, yet the mean is itself also an estimation. When we calculate the variation in residuals, this is done with respect to the observed mean in the sample, but this mean could also be a different mean, one that might even lie outside of the sample we have observed. As a result, relying on the observed mean produces an underestimation of the variation, because it will always be the mean of the sample, and thus lie somewhere between the minimum and maximum observed weight, while another mean could lie outside of the observed values, which would increase the variation in residuals.</p>
<p>But why divide by the sample size minus one? The underestimation is a function of the variation in sample means. The more variation there is between the different sample means, the more likely it is that any single sample consists of non-representative values. In contrast, if there is little variation in sample means, the underestimation must be smaller. The variation in sample means is actually the variation itself divided by the sample size, which we saw earlier. We saw that there is less variation in sample means when the samples are larger. This means that the amount of underestimation in variance is the variance divided by the sample size, or itself minus itself divided by the sample size. This is the same as saying sample size minus one divided by the sample size. Note that this is the bias we expect. To undo this bias, we then multiply by the sample size divided by the sample size minus one.</p>
<p>We now know how to calculate the variance of the residuals, as well as the variance in the sample means.</p>
<pre><code>## 
##  One Sample t-test
## 
## data:  log(pokemon25$weight)
## t = 11.082, df = 24, p-value = 6.367e-11
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  2.154471 3.140575
## sample estimates:
## mean of x 
##  2.647523</code></pre>
</div>
