---
title: Why divide by N - 1 when calculating the variance of a sample?
date: '2020-08-05'
tags:
  - statistics
slug: why-divide-by-n-1
---



<p>In a recent <a href="https://twitter.com/willemsleegers/status/1290388221394849803?s=20">tweet</a> I asked the question why we use <span class="math inline">\(n - 1\)</span> when calculating the variance of a sample. Many people contributed an answer (thanks!), so make sure to check out the tweet to see all the responses. In this post, I will try to describe my favorite way of looking at the issue, in part based on the responses I received.</p>
<div id="the-formula" class="section level2">
<h2>The Formula</h2>
<p>The formula for calculating the variance is:</p>
<p><span class="math display">\[\frac{\sum(x_i - \overline{x})^2}{n}\]</span></p>
<p>The variance is a measure of the dispersion around the mean, and in that sense this formula makes sense. We calculate all the deviations from the mean (<span class="math inline">\(x_i - \overline{x}\)</span>), square them (for reasons I might go into in a different post) and sum them. We then divide this sum by the number of observations as a scaling factor. If we ignore this number, we could get a very high variance simply by observing a lot of data. So, to fix that problem, we divide by the total number of observations.</p>
<p>However, this is the formula for the <em>population</em> variance. The formula for calculating the variance of a <em>sample</em> is:</p>
<p><span class="math display">\[\frac{\sum(x_i - \overline{x})^2}{n - 1}\]</span></p>
<p>Why do we divide by <em>n</em> - 1?</p>
<p>If you Google this question, you will get a variety of answers. You might find a mathematical proof of why it needs to be <em>n</em> - 1 or something about degrees of freedom. These kinds of answers don’t work for me. I trust them to be correct, but it doesn’t produce any insight. It does not actually help me understand 1) the problem and 2) why the solution is the solution that it is. So, below I am going to try to figure it out in a way that actually makes conceptual and intuitive sense (to me).</p>
</div>
<div id="the-problem" class="section level2">
<h2>The Problem</h2>
<p>The problem with using the population variance formula to calculate the variance of a sample is that it is biased. It is biased in that it produces an <em>underestimation</em> of the true variance. Let’s demonstrate that with some simulated data.</p>
<p>We simulate a population of 1000 data points from a uniform distribution with a range from 1 to 10. Below I show the histogram that represents our population.</p>
<p><img src="/post/2020-08-05-why-divide-by-n-1-when-calculating-the-standard-deviation_files/figure-html/population-1.png" width="100%" /></p>
<p>The mean is 5.38 and the variance is 7.08. Note that these are our population parameters (often denoted as <span class="math inline">\(\mu\)</span> for the population mean and <span class="math inline">\(\sigma^2\)</span> for the population variance). That means that these values are the true values that we want to estimate with samples drawn from our population, so let’s do that.</p>
<p>For instance, we can draw a sample of size 5 from our population. Say we do that and get the following values: 5, 8, 9, 10, 9. We can then calculate the variance in two ways, using division by n and division by n - 1. In the former case, this will result in 2.96 and in the latter case it results in 3.7.</p>
<p>Now let’s do that many many times. Below I show the results of many draws from our population. I simulated drawing samples of size 2 to 10, each 1000 different times. I then plotted for each sample the average biased variance (dividing by n; left) and the average unbiased variance (dividing by n - 1; right).</p>
<p><img src="/post/2020-08-05-why-divide-by-n-1-when-calculating-the-standard-deviation_files/figure-html/bias-1.png" width="100%" /></p>
<p>We see that the biased measure of variance is indeed biased. The average variance is lower than the true variance (indicated by the dashed line) for each sample size. We also see that the unbiased variance is indeed unbiased. On average, the sample variance matches that of the population variance.</p>
<p>The results of using the biased measure of variance reveals several clues for understanding the solution to the bias. We see that the amount of bias is larger when the sample size of the samples is smaller. So the solution should be a function of sample size, such that the required correction will be smaller as the sample size increases. We also see that that the bias at n = 2 is half that of the true variance, 2/3rds at n = 3, 3/4ths at n = 4, and so on. Interesting.</p>
<p>But before we go into the solution, we still need to figure out what exactly causes the bias.</p>
<p>Ideally we would estimate the variance of the sample by subtracting each value from the population mean. However, since we don’t know what the population mean is, we use the next best thing–the sample mean. But this is where the bias comes in. When you use the sample mean, you’re guaranteed that the mean lies somewhere within the range of your data points. In fact, the mean of a sample minimizes the sum of squared deviations from the mean. This means that the sum of deviations from the sample mean is always smaller than the sum of deviations from the population mean. The only exception to that is when the sample mean happens to be the population mean.</p>
<p>Let’s illustrate this with a few graphs. Below are two graphs. In each graph I show 10 data points that represent our population. I also highlight two data points from this population, which represents our sample. In the left graph I show the deviations from the sample mean and in the right graph the deviations from the population mean.</p>
<p><img src="/post/2020-08-05-why-divide-by-n-1-when-calculating-the-standard-deviation_files/figure-html/samples-1.png" width="100%" />
We see that in the left graph the sum of squared deviations are much smaller than in the right graph. The sum of is (8 - 9)² + (10 - 9)² = 2 in the left graph and in the right graph its (8 - 5.9)² + (10 - 5.9)² = 21.22. The sum is smaller when using the sample mean compared to using the population mean.</p>
<p>This is true for any sample you draw from the population (again, except when the sample mean happens to be the same as the population mean). Let’s look at one more draw where the sample mean is closer to the population mean.</p>
<p><img src="/post/2020-08-05-why-divide-by-n-1-when-calculating-the-standard-deviation_files/figure-html/samples2-1.png" width="100%" /></p>
<p>Here the sum in the left graph is (2 - 6)² + (10 - 6)² = 32 and the sum in the right graph is (2 - 5.9)² + (10 - 5.9)² = 32.02. The difference is small now, but using the sample mean still results in a smaller sum compared to using the population mean.</p>
<p>In short, the source of the bias comes from using the sample mean instead of the population mean. The sample mean is always guaranteed to be in the middle of the observed data, thereby reducing the variance, and creating an underestimation.</p>
</div>
<div id="the-solution" class="section level2">
<h2>The Solution</h2>
<p>Now that we know that the bias is caused by the sample mean, we can figure out how to solve the problem.</p>
<p>Looking at the previous graphs, we can see that if the sample mean is far from the population mean, the sample variance is smaller and the bias is large. If the sample mean is close to the population mean, the sample variance is larger and the bias is small. So, the more the sample mean moves around, the greater the bias.</p>
<p>In other words, the variance of a population actually consists of two types of variance: variance of the data points around the sample mean <em>and</em> the variance of the sample mean around the population mean:</p>
<p><span class="math display">\[\sigma^2_{sample} + \sigma^2_{mean} = \sigma^2_{population}\]</span>
Let’s confirm that this is true. For that we need to know how to calculate the variance of the sample mean around the population mean. This is relatively simple; it’s the variance of the population divided by n (<span class="math inline">\(\frac{\sigma^2}n\)</span>). This makes sense because the greater the variance, the more the mean can jump around, but the more data you sample, the closer you get to the population mean.</p>
<p>Now that we can calculate both the variance of the sample and the variance of the sample mean, we can check whether adding them together results in the population variance.</p>
<p>Below I show a graph in which I again sampled from our population, with varying sample sizes. For each sample, I calculated the sample variance (the biased one) and the variance of the mean of that sample (using variance/n). I did this 1000 times per sample size, took the average of each and put them on top of each other. I also added a dashed line to indicate the variance of the population, which is the benchmark we’re trying to reach.</p>
<p><img src="/post/2020-08-05-why-divide-by-n-1-when-calculating-the-standard-deviation_files/figure-html/unnamed-chunk-1-1.png" width="100%" /></p>
<p>Indeed, we see that together the variance of the sample and the variance of the mean of the sample form the population variance.</p>
</div>
<div id="the-math" class="section level2">
<h2>The Math</h2>
<p>Now that we know that the variance of the population consists of the variance of the sample and the variance of the sample mean, we can figure out the correction factor we need to apply to make the biased variance measure unbiased.</p>
<p>We found an interesting pattern in one of the previous figures. We saw that at a sample size of 2, the (biased) sample variance appears to be half that of the (unbiased) population variance. At sample size 3, it’s 2/3rds. At sample size 4, it’s 3/4, and so on.</p>
<p>This means that we can fix the biased variance measure by multiplying it with <span class="math inline">\(n/(n-1)\)</span>. At sample size 2, this would mean we multiply the biased variance by 2 / 1 = 2. For sample size 3, 3 / 2 = 1.5. At sample size 4, 4 / 3 = <span class="math inline">\(1 \frac13\)</span>, resulting in the unbiased variance.</p>
<p>In other words, to unbias the biased variance measure, we multiply it by a correction factor of <span class="math inline">\(n/(n-1)\)</span>. But where does this correction factor come from?</p>
<p>Well, because we now know that the sample variance misses the variance of the sample mean, we can expect that the variance of the sample is biased by an amount equal to the variance of the population minus the variance of the sample mean. In other words:</p>
<p><span class="math display">\[\sigma^2 - \frac{\sigma^2}n\]</span></p>
<p>Rewriting this <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>, produces:</p>
<p><span class="math display">\[\sigma^2\cdot\frac{n - 1}n\]</span>
This means that the variance of a sample will be biased by an amount equal to <span class="math inline">\(\frac{n - 1}n\)</span>. So, to correct that bias, we should multiply the sample variance by the inverse of this bias: <span class="math inline">\(\frac{n}{n-1}\)</span> <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>.</p>
<p>So, an unbiased measure of our sample variance is the biased sample variance times the correction factor:</p>
<p><span class="math display">\[\frac{\sum(x_i - \overline{x})^2}{n}\cdot{\frac n{n-1}}\]</span>
Because the <em>n</em> in the denominator of the left term (the biased variance formula) cancels out the <em>n</em> in the numerator of the right term (the bias correction), the formula can be rewritten as:</p>
<p><span class="math display">\[\frac{\sum(x_i - \overline{x})^2}{n-1}\]</span></p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>We calculate the variance of a sample by summing the squared deviations of each data point from the sample mean and dividing it by <span class="math inline">\(n - 1\)</span>. The <span class="math inline">\(n - 1\)</span> actually comes from a correction factor <span class="math inline">\(\frac n{n-1}\)</span> that is needed to correct for a bias caused by taking the deviations from the sample mean rather than the population mean. Taking the deviations from the sample mean only constitutes the variance around the sample mean, but ignores the variation of the sample mean around the population mean, producing an underestimation equal to the size of the variance of the sample mean <span class="math inline">\(\frac{\sigma^2}{n}\)</span>. The correction factor corrects for this underestimation, producing an unbiased estimate of the population variance.</p>
<p><em>This post was last updated on 2020-08-05.</em></p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Here are the steps to rewrite the formula: <span class="math display">\[\sigma^2 - \frac{\sigma^2}n\]</span> Add an n to the numerator and denominator of the left term: <span class="math display">\[\frac{\sigma^2n}n - \frac{\sigma^2}n\]</span> Combine the terms: <span class="math display">\[\frac{\sigma^2n - \sigma^2}n\]</span> Simplify the numerator: <span class="math display">\[\frac{\sigma^2(n - 1)}n\]</span> Move <span class="math inline">\(\sigma^2\)</span> out of the numerator: <span class="math display">\[\sigma^2\frac{n - 1}n\]</span><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>The inverse of <span class="math inline">\(\frac{n - 1}n\)</span> is <span class="math inline">\(\frac{1}{\frac{n - 1}n}\)</span>. Multiply both the numerator and the denominator by <span class="math inline">\(n\)</span> and you get <span class="math inline">\(\frac{n}{n-1}\)</span><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
