---
title: "Bayesian tutorial: Correlation"
description: "The third of a series of tutorial posts on Bayesian analyses. In this post I focus on using `brms` to model a correlation."
date: 2022-11-24
categories:
  - statistics
  - tutorial
  - Bayesian statistics
  - regression
code-fold: true
code-tools: true
toc: true
draft: true
---

In my previous [blog post](../18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.qmd), I showed how to use `brms` and `tidybayes` to run a simple regression, i.e., a regression with a single predictor. This analysis required us to set three priors: an intercept prior, a sigma prior, and a slope prior. We can simplify this analysis by turning it into a correlational analysis. This will remove the intercept prior and lets us think about the prior for the slope in as a standardized effect size, i.e., the correlation. 

To run a correlational analysis we'll need to standardize the outcome and predictor variable, so in the setup code below I run the setup code as usual and also standardize both variables.

```{r}
#| label: setup
#| message: false
library(tidyverse)
library(brms)
library(tidybayes)

data <- read_csv("Howell1.csv")
data <- filter(data, age >= 18)
data <- mutate(
  data, 
  height_z  = (height - mean(height)) / sd(height),
  weight_z  = (weight - mean(weight)) / sd(weight),
)

theme_set(theme_minimal())

colors <- c("#93CFDB", "#1E466E")
```

The formula for our model is slightly different compared to the formula of the previous single-predictor model and that's because we want to omit the intercept. By standardizing both the outcome and predictor variables, the intercept is guarenteed to be 0. A typical regression model like this will have a slope that goes through the point that represents the means of both outcome and predictor variable. In other words, the value of the outcome when the predictor is 0 (the mean of the predictor) is the mean of the outcome variable (which is 0). I suppose we could still include a prior for the intercept and simply set it to 0 (using `constant(0)`) but we can also simply tell `brms` not to estimate it. The formula syntax then becomes: `height_z ~ 0 + weight_z`.

Let's confirm that this means we only need to set two priors.

```{r}
#| label: get-prior
get_prior(height_z ~ 0 + weight_z, data = data)
```

Indeed, we're left with a prior for $\sigma$ and one for `weight_z`, which we can specify either via class `b` or the specific coefficient for `weight_z`. 

Let's also write down our model more explicitly, which is simply the same model as before but without the intercept ($\alpha$).
$$\displaylines{heights_i ∼ Normal(\mu_i, \sigma) \\ \mu_i = \beta x_i}$$

## Setting the priors

What should the prior for $\sigma$ be? With the variables standardized, $\sigma$ is limited to range from 0 to 1. If the predictor explains all the variance of the outcome variable, the residuals will be 0, meaning $\sigma$ will be 0. If the predictor explains no variance, $\sigma$ is equal to 1 because it will be similar to the standard deviation of the outcome variable, which is 1 because we've standardized it. Interestingly, this also means that the prior for $\sigma$ is now dependent on the prior for the slope, because the slope is what determines how much variance is explained in the outcome variable. So let's think about the prior for the slope.

The prior for the slope is a bit easier now. We can specify a normal distribution with a mean of 0 and a standard deviation of 0.5, together with a lower bound of -1 and upper bound of 1. With a standard deviation of 0.5, we cover a large range of possible slopes, but assign more plausibility to smaller correlations and less plausibility to very high correlations (like 1 and -1).

As for $\sigma$, let's keep it simple and use a uniform prior that assign equal plausibility to each value between 0 and 1.

```{r}
#| label: model-height-weight_z
model_height_weight_z <- brm(
  height_z ~ weight_z,  
  data = data, 
  family = gaussian,
  prior = c(
      prior(constant(0), class = "Intercept"),
      prior(uniform(0, 1), class = "sigma", ub = 1),
      prior(normal(0, 0.5), class = "b", lb = -1, ub = 1)
    ), 
  sample_prior = TRUE,
  cores = 4,
  seed = 4,
  file = "models/model_height_weight_prior_z.rds"
)

model_height_weight_z
```

Taking a look at the estimates, we see the intercept is indeed 0 (we forced this). The estimate for the slope is `r round(fixef(model_height_weight_z)[2, 1], 2)`, i.e., the correlation. This means that the estimate for sigma is the square root of 1 minus the variance of the slope estimate (`r round(fixef(model_height_weight_z)[2, 1], 2)`²). In our case, that's .66, which matches the estimate for sigma.