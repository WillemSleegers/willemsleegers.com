---
title: Understanding regression (part 2)
description: In Part 1 we learned that ordinary least squares regression estimates the mean. But why use the mean instead of the median? This post explores the efficiency advantage of the mean for normally distributed data and explains when this advantage applies.
date: 2020-08-01
updated: 2025-01-09
tags:
  - statistics
  - tutorial
  - regression
code-fold: true
toc: true
draft: true
---

```{r}
#| label: setup
#| message: false
library(tidyverse)
library(here)
library(gt)

# Theme settings
theme_set(theme_minimal(base_size = 16))
primary <- "#16a34a"

# Data
pokemon <- read_csv(here("public", "data", "pokemon.csv"))
pokemon25 <- filter(pokemon, pokedex <= 25)
```

In Part 1 of 'Understanding Regression' we figured out where the estimate of an intercept-only regression model came from. It turned out to be the mean of the data. However, this was only the case if we defined our model's error as the sum of squared residuals. If we don't square the residuals, the best fitting value turned out to be the median of the data, and the mode if we simply counted the number of errors.

We also briefly discussed why one could favor squaring the residuals over not squaring them. By squaring the residuals we're punishing models that make larger mistakes. A residual of size 4 gets amplified to being an error of size 16, a residual of size 2 gets doubled to an error of 4 and a residual of 1 is an error of 1. This may simply be a property we like. We may prefer models that make many small errors over models that make large errors.

But there's another way to think about this. Instead of starting with squared residuals and ending up with the mean, we can start with the question: should we prefer the mean or the median as our estimator? Maybe they have different statistical properties, and one is better suited for certain types of data. Let's explore this question.

## Efficiency: How much do you need?

Let's see what happens when we repeatedly sample from data and calculate both the mean and median each time. I'll draw 10,000 samples at three different sample sizes (10, 25, and 100) from a normal distribution with a mean of 0 and a standard deviation of 1. For each sample, I'll calculate both the mean and the median. This will show us how these estimators behave.

```{r}
#| label: fig-mean-vs-median
#| fig-cap: Comparing mean and median precision
samples <- crossing(
  n = c(10, 25, 100),
  i = 1:10000
) |>
  rowwise() |>
  mutate(
    mean = mean(rnorm(n = n, mean = 0, sd = 1)),
    median = median(rnorm(n = n, mean = 0, sd = 1))
  ) |>
  pivot_longer(cols = c(mean, median), names_to = "statistic")

samples_summary <- samples |>
  group_by(statistic, n) |>
  summarize(
    low = quantile(value, probs = .025),
    mid = quantile(value, probs = .5),
    high = quantile(value, probs = .975),
    max = max(density(value)$y)
  )

ggplot(samples, aes(x = value)) +
  geom_density(fill = primary, alpha = .85) +
  facet_grid(
    statistic ~ n,
    labeller = labeller(n = function(x) paste("n =", x))
  ) +
  geom_text(
    aes(
      x = 0,
      y = max + 0.75,
      label = paste0(
        round(mid, 2),
        "\n[",
        round(low, 2),
        ", ",
        round(high, 2),
        "]"
      )
    ),
    data = samples_summary,
    color = "gray20",
    size = 3
  ) +
  geom_errorbar(
    aes(xmin = low, xmax = high, y = max + 0.25, x = 0),
    data = samples_summary,
    width = .2,
    color = "gray20"
  ) +
  labs(x = "Estimator value", y = "") +
  scale_y_continuous(labels = NULL)
```

Look at that! Both estimators seem to work well---they're both centered around 0 (the true value), and both become more precise as sample size increases. But there's a clear difference in the width of their distributions. The median's intervals (shown by the error bars and text) are consistently wider than the mean's intervals at every sample size.

What does this mean practically? When using the median, we're more uncertain about what the true parameter value is compared to when we use the mean. This difference in precision is what statisticians call efficiency.

To quantify this difference, we can calculate the relative efficiency by comparing the widths of the 95% intervals (from 2.5% to 97.5% percentiles) for the two estimators:

```{r}
#| label: tbl-efficiency
efficiency <- samples_summary |>
  mutate(interval_width = high - low) |>
  select(statistic, n, interval_width) |>
  pivot_wider(names_from = statistic, values_from = interval_width) |>
  mutate(
    width_ratio = median / mean,
    efficiency_loss = (width_ratio - 1) * 100
  )

efficiency |>
  gt() |>
  cols_label(
    n = "Sample size",
    mean = "Mean interval width",
    median = "Median interval width",
    width_ratio = "Width ratio",
    efficiency_loss = "Efficiency loss (%)"
  ) |>
  fmt_number(columns = c(mean, median, width_ratio), decimals = 3) |>
  fmt_number(columns = efficiency_loss, decimals = 1)
```

The table shows that the median's 95% interval is about `r round(mean(efficiency$width_ratio), 2)` times wider than the mean's interval across different sample sizes. This means the median is about `r round(mean(efficiency$efficiency_loss), 0)`% less efficient than the mean. Put another way, we would need roughly `r round(mean(efficiency$width_ratio)^2, 1)` times as much data when using the median to achieve the same level of precision as the mean.

That's a substantial difference! If you're collecting data (which is often expensive and time-consuming), using the mean instead of the median could save you a lot of resources.

But here's the key question: *why* does the mean have this advantage? And perhaps more importantly, *when* does it have this advantage?

## Why the mean wins for normal data

The efficiency advantage we just saw isn't universal---it specifically applies when data follows a normal distribution. Notice that in our simulation, I drew samples from a normal distribution (using `rnorm()`). This wasn't arbitrary.

The normal distribution is special because it's completely characterized by just two parameters: its mean (<math>$\mu$</math>) and its variance (<math>$\sigma^2$</math>). If you know these two numbers, you know everything about the distribution's shape.

When data are normally distributed, the mean isn't just *a* measure of center---it's *the* parameter that defines where the distribution is located. The mean has a special relationship with the normal distribution: it's baked into the very definition of what makes a distribution normal.

The median, while also measuring "center" in some sense, doesn't have this special relationship. When you calculate the median, you only use the rank order of your data---a value of 100 and a value of 1000 both just count as "above the median." You're throwing away information about the actual magnitudes. For normally distributed data, this is wasteful. The mean uses every data point's actual value, extracting all available information from the distribution.

This connects to a deeper principle from information theory. When all you know about your data is its mean and variance, the normal distribution is the distribution that makes the fewest additional assumptions---it's the maximum entropy distribution given those constraints. So if we're willing to assume our data has some mean and some variance, but we don't want to assume anything else, we end up with a normal distribution. And if we're using that distribution to describe our data, the mean is the parameter we're trying to estimate.

Is the mean *the best* estimator you could possibly use? For normally distributed data, yes---this is what the Gauss-Markov theorem tells us. Among all unbiased estimators, the mean (via ordinary least squares) has the smallest variance. I don't have a simple intuitive proof of why this is true, but the efficiency advantage we demonstrated is part of the story: the mean uses all the information in the data most effectively when that data follows a normal distribution.

## When this matters (and when it doesn't)

So should we always use the mean and assume normality? Not quite.

This whole analysis depends on the assumption that our data actually follows a normal distribution. When that assumption holds, the mean is optimal. But when data are heavily skewed, have outliers, or come from very different distributions, the median might actually be better. It's more robust to extreme values.

This is why ordinary least squares regression assumes normally distributed data. Under that assumption, OLS estimates the mean, which is the best choice. The ~57% efficiency advantage we demonstrated means narrower confidence intervals and more statistical power when this assumption is reasonable.

Understanding this helps us see regression not as a black box, but as a principled choice: we're assuming a particular kind of data-generating process (a normal distribution with some mean and variance) and choosing the estimator (the mean) that's optimal for that process. When the assumption breaks down, other approaches might be better---a topic for future posts.

## Conclusion

In Part 1, we learned that minimizing squared residuals gives us the mean. In this post, we've seen *why* that's desirable: when data are normally distributed, the mean is substantially more efficient than alternatives like the median.

This efficiency isn't just a mathematical curiosity---it has practical implications. It means we can get more precise estimates with less data, achieve narrower confidence intervals, and detect effects with greater statistical power. But it comes with a condition: our data need to be normally distributed.

This understanding moves us beyond just knowing *what* regression does (estimates the mean by minimizing squared residuals) to understanding *why* it does it that way (the mean is optimal for normal data given the maximum entropy principle). We now also understand an important limitation: OLS is optimal when the normality assumption holds, but may not be the best choice when data deviates substantially from this pattern.
