---
title: Understanding regression (part 2)
description: "summary of part 2 of understanding regression"
date: 2020-08-01
categories:
  - statistics
  - tutorial
code-fold: true
toc: true
draft: true
---

```{r}
#| label: setup
#| message: false

library(tidyverse)
library(viridis)

# Read in Pokémon data
pokemon <- read_csv("pokemon.csv")

# Create a subset with only the first 25 Pokémon
pokemon25 <- filter(pokemon, pokedex <= 25)
pokemon25_50 <- filter(pokemon, pokedex > 25 & pokedex <= 50)

# Create a vector with only the weights
weights <- pull(pokemon, weight)

# Set seed
set.seed(42)

# Set options
options(
  knitr.kable.NA = "-",
  digits = 2
)

theme_set(theme_minimal())
```

In [Part](../1-understanding-regression-part-1/understanding-regression-part-1.qmd) 1 of 'Understanding Regression' we figured out where the estimate of an intercept-only regression model came from. It turned out to be the mean of the data. However, this was only the case if we defined our model's error as the sum of squared residuals. If we don't square the residuals, the best fitting value turned out to be the median of the data.

We also briefly discussed why one could favor squaring the residuals over not squaring them. By squaring the residuals we're punishing models that make larger mistakes. A residual of size 4 gets amplified to being an error of size 16, a residual of size 2 gets doubled to an error of 4 and a residual of 1 is an error of 1. This may simply be a property we like. We may prefer models that make many small errors over models that make large errors. If we do, we could square the residuals and use the mean as our best fitting model. But we can also approach this issue from the other way around. We can think about whether we prefer means over medians. It could be that the mean has certain properties different from that of the median and that may also be better. Part 2 of Understanding Regression is about whether this is the case.

## Bias, efficiency, and consistency

To determine whether we should prefer the mean over the median, or vice versa, we need to figure out which properties of the estimator we care about. The one's I'm familiar with are unbiasedness, consistency, and effiency. Unbiasedness refers to whether the estimator will correctly estimate the population value. Efficiency refers to how precise the estimator is. The estimator is sampled at a certain sample size and may be more or less precise in estimating the population value. Finally, consistency refers to whether the estimator will become more precise as the sample size increases.

Let's visualize each property by repeatedly calculating the mean of a sample at three different sample sizes. Below I calculate the mean of 10, 25, and 100 samples from a normal distribution with a mean of 0 and a standard deviation of 1. I then plot the distribution of mean values at each sample size (based on 10,000 samples). To help assess the distribution, I also show the 2.5%, 50%, and 97.5% percentile so we can see how variable the mean estimator is and whether it approximates the true parameter (the value 0).

```{r}
#| label: properties-mean
#| fig-cap: Unbiasedness, efficiency, and consistency of the mean
samples_mean <- crossing(
  n = c(10, 25, 100),
  i = 1:10000
) %>%
  rowwise() %>%
  mutate(value = mean(rnorm(n = n, mean = 0, sd = 1)))

samples_mean_summary <- samples_mean %>%
  group_by(n) %>%
  summarize(
    low = quantile(value, probs = .025),
    mid = quantile(value, probs = .5),
    high = quantile(value, probs = .975),
    max = max(density(value)$y)
  )

ggplot(samples_mean, aes(x = value)) +
  geom_density(fill = "steelblue", alpha = .85) +
  facet_grid(~n, labeller = labeller(n = function(x) paste("n =", x))) +
  geom_text(
    aes(
      x = 0,
      y = max + 0.7,
      label = paste0(
        round(mid, 2), "\n[", round(low, 2), ", ", round(high, 2), "]"
      )
    ),
    data = samples_mean_summary,
    color = "gray20", size = 3
  ) +
  geom_errorbar(
    aes(xmin = low, xmax = high, y = max + 0.25, x = 0),
    data = samples_mean_summary, width = .2, color = "gray20"
  ) +
  labs(x = "Estimator value", y = "") +
  scale_y_continuous(labels = NULL)
```

The mean seems to be unbiased because it correctly estimates the value 0 to be the most likely parameter value. The 50% percentile of each distribution is 0. We also see that as the sample size increases, the distribution of means becomes narrower. In other words, the estimate becomes more precise. We can't really judge the efficiency here because we don't have a benchmark to say whether the efficiency is good or bad. We can, however, compare it to using the median.

Below we repeat the code from before but now also with the median of each sample so that we can compare the distribution of means to the distribution of medians.

```{r}
samples <- crossing(
  n = c(10, 25, 100),
  i = 1:10000
) %>%
  rowwise() %>%
  mutate(
    mean = mean(rnorm(n = n, mean = 0, sd = 1)),
    median = median(rnorm(n = n, mean = 0, sd = 1))
  ) %>%
  pivot_longer(cols = c(mean, median), names_to = "statistic")

samples_summary <- samples %>%
  group_by(statistic, n) %>%
  summarize(
    low = quantile(value, probs = .025),
    mid = quantile(value, probs = .5),
    high = quantile(value, probs = .975),
    max = max(density(value)$y)
  )

ggplot(samples, aes(x = value)) +
  geom_density(fill = "steelblue", alpha = .85) +
  facet_grid(
    statistic ~ n,
    labeller = labeller(n = function(x) paste("n =", x))
  ) +
  geom_text(
    aes(
      x = 0,
      y = max + 0.75,
      label = paste0(
        round(mid, 2), "\n[", round(low, 2), ", ", round(high, 2), "]"
      )
    ),
    data = samples_summary,
    color = "gray20", size = 3
  ) +
  geom_errorbar(
    aes(xmin = low, xmax = high, y = max + 0.25, x = 0),
    data = samples_summary, width = .2, color = "gray20"
  ) +
  labs(x = "Estimator value", y = "") +
  scale_y_continuous(labels = NULL)
```

It seems that both the mean and the median are unbiased--both produce distributions centered around 0. They are also both consistent because their precision decreases as the sample size increases. However, the precision seems to differ between the two. The width of the percentile interval (ranging from 2.5% to 97.5%) is larger for the median than it is for the mean, at each sample size. In other words, when using the median, we are more uncertain what the true parameter value is than when we use the mean. Or, in other other words, we need fewer observations when using the mean to reach the same level of precision when using the median. In fact, it seems that the mean, at least in these scenarios, is about 20% to 25% more efficient than the median.

## Outliers

```{r}
samples <- crossing(
  n = c(10, 25, 100),
  i = 1:10000
) %>%
  rowwise() %>%
  mutate(
    mean = mean(
      c(rnorm(n = n, mean = 0, sd = 1)), rep(3, 500)
    ),
    median = median(rnorm(n = n, mean = 0, sd = 1))
  ) %>%
  pivot_longer(cols = c(mean, median), names_to = "statistic")

samples_summary <- samples %>%
  group_by(statistic, n) %>%
  summarize(
    low = quantile(value, probs = .025),
    mid = quantile(value, probs = .5),
    high = quantile(value, probs = .975),
    max = max(density(value)$y)
  )

ggplot(samples, aes(x = value)) +
  geom_density(fill = "steelblue", alpha = .85) +
  facet_grid(
    statistic ~ n,
    labeller = labeller(n = function(x) paste("n =", x))
  ) +
  geom_text(
    aes(
      x = 0,
      y = max + 0.75,
      label = paste0(
        round(mid, 2), "\n[", round(low, 2), ", ", round(high, 2), "]"
      )
    ),
    data = samples_summary,
    color = "gray20", size = 3
  ) +
  geom_errorbar(
    aes(xmin = low, xmax = high, y = max + 0.25, x = 0),
    data = samples_summary, width = .2, color = "gray20"
  ) +
  labs(x = "Estimator value", y = "") +
  scale_y_continuous(labels = NULL)
```

## Population vs. Sample

Remember that our Pokémon weight model was based on the weights of the first 25 Pokémon. This is only a small sample of all Pokémon out there (our data has a total of 893). In the sample we observed, the average weight is `r mean(pull(pokemon25, weight))`. But what if we had instead focused on the next 25 Pokémon, from Raichu to Diglet? The average weight of these Pokémon is `r mean(pull(pokemon25_50, weight))`. That's a difference of `r mean(pull(pokemon25, weight)) - mean(pull(pokemon25_50, weight))`.

Let's actually take a look at the weights of all 893 Pokemon.

```{r}
#| label: fig-pokemon-weights
#| fig-cap: Weights of 893 Pokémon
ggplot(pokemon, aes(x = weight)) +
  geom_histogram(color = "black", alpha = .75, binwidth = 25)
```

This figure shows the weights of *all* the Pokemon that are out there, and thus forms the population of Pokemon weights. Previously we took a *sample* from this population by only looking at 25 Pokemon of this population.

Now imagine that we are not the only ones creating a Pokémon weight model. Instead, we are one of many who are doing so, and we all exchange our model information. We can tell others about our observation that `r paste("weight =", mean(pull(pokemon25, weight)))`. In turn, others will give us their sample means. And let's further imagine that we also encounter people who used the median instead. Below I show some code to create these samples.

```{r}
#| label: samples
samples <- crossing(
  estimator = c("mean", "median"),
  n = c(10, 25, 100),
  i = 1:10000
) %>%
  rowwise() %>%
  mutate(
    estimate = if_else(
      condition = estimator == "mean",
      true = mean(sample(weights, n), na.rm = TRUE),
      false = median(sample(weights, n), na.rm = TRUE)
    )
  )
```

Let's begin by simply looking at the distribution of mean values. The following graph contains 10,000 means, each based on 25 different Pokémon.

```{r}
#| label: fig-pokemon25-mean-weights
#| fig-cap: Distribution of 10,000 means based on 25 Pokémon each
means25 <- filter(samples, estimator == "mean" & n == 25)

ggplot(means25, aes(x = estimate)) +
  geom_density(alpha = .5, fill = "black") +
  geom_vline(xintercept = mean(weights), linetype = "dashed") +
  annotate("text", x = 79, y = 0.0195, label = "population mean") +
  labs(x = "Sample mean", y = "Count")
```

This distribution of means is also called a sampling distribution of means. In our case, we see that

```{r}
estimators25 <- filter(samples, n == 25)

ggplot(estimators25, aes(x = estimate, fill = estimator)) +
  geom_density(alpha = .75, adjust = 2) +
  geom_vline(xintercept = mean(weights), linetype = "solid") +
  geom_vline(xintercept = median(weights), linetype = "dashed") +
  annotate(
    geom = "text",
    x = mean(weights),
    y = 0.0465,
    label = "population mean"
  ) +
  annotate(
    geom = "text",
    x = median(weights),
    y = 0.049,
    label = "population median",
  ) +
  labs(x = "Sample mean", y = "Count") +
  scale_fill_viridis(option = "mako", discrete = TRUE, begin = .25, end = .75) +
  coord_cartesian(ylim = c(0, 0.0425), clip = "off") +
  theme(
    plot.margin = unit(c(2, 1, 1, 1), "lines")
  )
```


### T-distribution

```{r}
df <- 30
population <- tibble(population = rt(10000, df = df, ncp = 1))

ggplot(population, aes(x = population)) +
  geom_density()

df <- crossing(
  n = c(5, 10, 25, 50, 100, 250),
  i = 1:10000
) %>%
  rowwise() %>%
  mutate(
    median = median(rt(n = n, df = df)),
    mean = mean(rt(n = n, df = df))
  ) %>%
  pivot_longer(cols = c(median, mean), names_to = "statistic")

ggplot(df, aes(x = value)) +
  # geom_density(mapping = aes(x = population), data = population, alpha = .4) +
  geom_density(mapping = aes(fill = statistic), alpha = .5) +
  facet_wrap(~n, scales = "free") +
  scale_fill_viridis(option = "mako", discrete = TRUE, begin = .25, end = .75) +
  theme_minimal()
```

```{r}
library(sn)
population <- rsn(1000, 5, 2, 5)

df <- crossing(
  n = c(5, 10, 25, 50, 100, 250),
  i = 1:1000
) %>%
  rowwise() %>%
  mutate(
    median = median(rnorm(n = n, mean = 0, sd = 1)),
    mean = mean(rnorm(n = n, mean = 0, sd = 1))
  ) %>%
  pivot_longer(cols = c(median, mean), names_to = "statistic")

ggplot(df, aes(x = value, fill = statistic)) +
  geom_histogram(binwidth = .05, alpha = .9) +
  facet_wrap(~n, scales = "free") +
  scale_fill_viridis(option = "mako", discrete = TRUE, begin = .25, end = .75) +
  theme_minimal()
```