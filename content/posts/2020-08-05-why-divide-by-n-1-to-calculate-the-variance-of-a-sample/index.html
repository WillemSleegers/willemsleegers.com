---
title: "Why divide by N - 1 to calculate the variance of a sample?"
summary: "In a recent tweet I asked the question why we divide by minus 1 to calculate the variance of a sample. I received many responses, but many of them were exactly of the type I feared: a statistical-jargon response that confuses me more, rather than less. Some of the responses were very useful, though, enabling me to write up this post in which I describe my favorite way of looking at this issue."
date: 2020-08-05
slug: why-divide-by-n-1
tags:
  - statistics
code_folding: true
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>In a recent <a href="https://twitter.com/willemsleegers/status/1290388221394849803?s=20">tweet</a> I asked the question why we use <span class="math inline">\(n - 1\)</span> to calculate the variance of a sample. Many people contributed an answer, but many of them were of the type I feared. The response consists of some statistical jargon that confuses me more, rather than less. Some of the responses were very useful, though, so I recommend checking out the replies to the tweet. Based on some of the responses I received, I will try to describe my favorite way of looking at the issue.</p>
<p>If you want to follow along in R, you can copy the code from each code section; beginning with some setup code.</p>
<pre class="r"><code># Load packages
library(tidyverse)
library(patchwork)

# Create color variables for the plots
green &lt;- &quot;#00B88D&quot;
yellow &lt;- &quot;#f39c12&quot;
off_white &lt;- &quot;#cccccc&quot;

# Create our own variance function that returns the population or sample variance
my_var &lt;- function(x, population = FALSE) {
  if (population) {
    sum((x - mean(x))^2)/length(x)
  } else {
    sum((x - mean(x))^2)/(length(x) - 1)
  }
}</code></pre>
<div id="the-formula" class="section level2">
<h2>The Formula</h2>
<p>The formula for calculating the variance is:</p>
<p><span class="math display">\[\frac{\sum(x_i - \overline{x})^2}{n}\]</span></p>
<p>The variance is a measure of the dispersion around the mean, and in that sense this formula makes sense. We calculate all the deviations from the mean (<span class="math inline">\(x_i - \overline{x}\)</span>), square them (for reasons I might go into in a different post) and sum them. We then divide this sum by the number of observations as a scaling factor. If we ignore this number, we could get a very high variance simply by observing a lot of data. So, to fix that problem, we divide by the total number of observations.</p>
<p>However, this is the formula for the <em>population</em> variance. The formula for calculating the variance of a <em>sample</em> is:</p>
<p><span class="math display">\[\frac{\sum(x_i - \overline{x})^2}{n - 1}\]</span></p>
<p>Why do we divide by <em>n</em> - 1?</p>
<p>If you Google this question, you will get a variety of answers. You might find a mathematical proof of why it needs to be <span class="math inline">\(n - 1\)</span> or something about degrees of freedom. These kinds of answers don’t work for me. I trust them to be correct, but it doesn’t produce any insight. It does not actually help me understand 1) the problem and 2) why the solution is the solution that it is. So, below I am going to try to figure it out in a way that actually makes conceptual and intuitive sense (to me).</p>
</div>
<div id="the-problem" class="section level2">
<h2>The Problem</h2>
<p>The problem with using the population variance formula to calculate the variance of a sample is that it is biased. It is biased in that it produces an <em>underestimation</em> of the true variance. Let’s demonstrate that with some simulated data.</p>
<p>We simulate a population of 1000 data points from a uniform distribution with a range from 1 to 10. Below I show the histogram that represents our population.</p>
<pre class="r"><code># Set the seed for reproducibility
set.seed(1212)

# Create a population consisting of values ranging from 1 to 10
population &lt;- sample(1:10, size = 1000, replace = TRUE)

# Get the mean and population variance
mu &lt;- sum(population)/length(population)
sigma &lt;- my_var(population, population = TRUE)

# Visualize the population
ggplot(tibble(population = population), mapping = aes(x = population)) +
  geom_bar(fill = green) +
  labs(x = &quot;x&quot;, y = &quot;n&quot;) +
  scale_x_continuous(breaks = 1:10)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/population-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The variance is 8.76. Note that this is our population variance (often denoted as <span class="math inline">\(\sigma^2\)</span>). We want to estimate this value using samples drawn from our population, so let’s do that.</p>
<p>To start, we can draw a single sample of size 5. Say we do that and get the following values: 7, 6, 3, 5, 5. We can then calculate the variance in two ways, using division by <span class="math inline">\(n\)</span> and division by <span class="math inline">\(n - 1\)</span>. In the former case, this will result in 1.76 and in the latter case it results in 2.2.</p>
<p>Now let’s do that many many times. Below I show the results of draws from our population. I simulated drawing samples of size 2 to 10, each 1000 different times. I then plotted for each sample size the average biased variance (dividing by <span class="math inline">\(n\)</span>; left) and the average unbiased variance (dividing by <span class="math inline">\(n - 1\)</span>; right).</p>
<pre class="r"><code># Create an empty data frame with the simulation parameters
samples &lt;- crossing(
    n = 2:20,
    i = 1:1000
  )

# Calculate the mean, sample variance, and population variance 
# for each combination of n and i
samples &lt;- samples %&gt;%
  rowwise() %&gt;%
  mutate(
    var_unbiased = my_var(sample(population, n), population = FALSE),
    var_biased = my_var(sample(population, n), population = TRUE)
  )

p_biased &lt;- ggplot(samples, aes(x = n, y = var_biased)) +
  geom_hline(yintercept = sigma, linetype = &quot;dashed&quot;, color = yellow) +
  stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, color = green, size = 3) +
  labs(x = &quot;Sample size (n)&quot;, y = &quot;Variance&quot;) +
  coord_cartesian(ylim = c(0, sigma + 1)) +
  scale_x_continuous(breaks = seq(from = 2, to = 20, by = 2)) +
  ggtitle(&quot;Variance with division by n&quot;)

p_unbiased &lt;- ggplot(samples, aes(x = n, y = var_unbiased)) +
  geom_hline(yintercept = sigma, linetype = &quot;dashed&quot;, color = yellow) +
  stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, color = green, size = 3) +
  labs(x = &quot;Sample size (n)&quot;, y = &quot;Variance&quot;) +
  coord_cartesian(ylim = c(0, sigma + 1)) +
  scale_x_continuous(breaks = seq(from = 2, to = 20, by = 2)) +
  ggtitle(&quot;Variance with division by n - 1&quot;)

p_biased + p_unbiased</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/bias-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We see that the biased measure of variance is indeed biased. The average variance is lower than the true variance (indicated by the dashed line), for each sample size. We also see that the unbiased variance is indeed unbiased. On average, the sample variance matches that of the population variance.</p>
<p>The results of using the biased measure of variance reveals several clues for understanding the solution to the bias. We see that the amount of bias is larger when the sample size of the samples is smaller. So the solution should be a function of sample size, such that the required correction will be smaller as the sample size increases. We also see that that the bias at <span class="math inline">\(n = 2\)</span> is half that of the true variance, <span class="math inline">\(\frac23\)</span> at <span class="math inline">\(n = 3\)</span>, <span class="math inline">\(\frac34\)</span> at <span class="math inline">\(n = 4\)</span>, and so on. Interesting.</p>
<p>But before we go into the solution, we still need to figure out what exactly causes the bias.</p>
<p>Ideally we would estimate the variance of the sample by subtracting each value from the population mean. However, since we don’t know what the population mean is, we use the next best thing—the sample mean. This is where the bias comes in. When you use the sample mean, you’re guaranteed that the mean lies somewhere within the range of your data points. In fact, the mean of a sample minimizes the sum of squared deviations from the mean. This means that the sum of deviations from the sample mean is <em>always</em> smaller than the sum of deviations from the population mean. The only exception to that is when the sample mean happens to be the population mean.</p>
<p>Let’s illustrate this with a few graphs. Below are two graphs. In each graph I show 10 data points that represent our population. I also highlight two data points from this population, which represents our sample. In the left graph I show the deviations from the sample mean and in the right graph the deviations from the population mean.</p>
<pre class="r"><code>x &lt;- c(1, 2, 4, 4, 4, 6, 8, 9, 10, 10)

sample1 &lt;- tibble(
    index = 1:10,
    value = x,
    sample = c(0, 0, 0, 0, 0, 0, 1, 0, 0, 1),
    mean = mean(c(8, 10)),
    mu = mean(x)
  ) %&gt;%
  mutate(
    mean = ifelse(sample == 1, mean, NA),
    mu = ifelse(sample == 1, mu, NA)
  )

p1 &lt;- ggplot(sample1, aes(x = index, y = value, color = factor(sample))) +
  geom_hline(aes(yintercept = mu), linetype = &quot;dashed&quot;, color = off_white) +
  geom_point(size = 2.5) +
  geom_hline(aes(yintercept = mean), linetype = &quot;solid&quot;, color = green,
    size = 1) +
  geom_segment(aes(xend = index, y = mean, yend = value), linetype = &quot;solid&quot;,
    size = 1) +
  coord_flip() +
  scale_color_manual(values = c(off_white, green)) +
  scale_y_continuous(breaks = 1:10) +
  guides(color = FALSE) +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(y = &quot;&quot;) +
  ggtitle(&quot;Deviations from the sample mean&quot;)

p2 &lt;- ggplot(sample1, aes(x = index, y = value, color = factor(sample))) +
  geom_hline(aes(yintercept = mean), linetype = &quot;dashed&quot;, color = off_white) +
  geom_point(size = 2.5) +
  geom_hline(aes(yintercept = mu), linetype = &quot;solid&quot;, color = green,
    size = 1) +
  geom_segment(aes(xend = index, y = mu, yend = value), linetype = &quot;solid&quot;,
    size = 1) +
  coord_flip() +
  scale_color_manual(values = c(off_white, green)) +
  scale_y_continuous(breaks = 1:10) +
  guides(color = FALSE) +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(y = &quot;&quot;) +
  ggtitle(&quot;Deviations from the population mean&quot;)

p1 + p2</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/samples-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We see that in the left graph the sum of squared deviations is much smaller than in the right graph. The sum is <span class="math inline">\((8 - 9)² + (10 - 9)² = 2\)</span> in the left graph and in the right graph it’s <span class="math inline">\((8 - 5.8)² + (10 - 5.8)² = 22.48\)</span>. The sum is smaller when using the sample mean compared to using the population mean.</p>
<p>This is true for any sample you draw from the population (again, except when the sample mean happens to be the same as the population mean). Let’s look at one more draw where the sample mean is closer to the population mean.</p>
<pre class="r"><code>sample2 &lt;- tibble(
    index = 1:10,
    value = x,
    sample = c(0, 1, 0, 0, 0, 0, 0, 0, 1, 0),
    mean = mean(c(2, 10)),
    mu = mean(x)
  ) %&gt;%
  mutate(
    mean = ifelse(sample == 1, mean, NA),
    mu = ifelse(sample == 1, mu, NA)
  )

p1 &lt;- ggplot(sample2, aes(x = index, y = value, color = factor(sample))) +
  geom_hline(aes(yintercept = mu), linetype = &quot;dashed&quot;, color = off_white,
    size = 0.75) +
  geom_point(size = 2.5) +
  geom_hline(aes(yintercept = mean), linetype = &quot;solid&quot;, color = green,
    size = 0.75) +
  geom_segment(aes(xend = index, y = mean, yend = value), linetype = &quot;solid&quot;,
    size = 0.75) +
  coord_flip() +
  scale_color_manual(values = c(off_white, green)) +
  scale_y_continuous(breaks = 1:10) +
  guides(color = FALSE) +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(y = &quot;&quot;) +
  ggtitle(&quot;Deviations from the sample mean&quot;)
  

p2 &lt;- ggplot(sample2, aes(x = index, y = value, color = factor(sample))) +
  geom_hline(aes(yintercept = mean), linetype = &quot;dashed&quot;, color = off_white,
    size = 0.75) +
  geom_point(size = 2.5) +
  geom_hline(aes(yintercept = mu), linetype = &quot;solid&quot;, color = green,
    size = 0.75) +
  geom_segment(aes(xend = index, y = mu, yend = value), linetype = &quot;solid&quot;,
    size = 0.75) +
  coord_flip() +
  scale_color_manual(values = c(off_white, green)) +
  scale_y_continuous(breaks = 1:10) +
  guides(color = FALSE) +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(y = &quot;&quot;) +
  ggtitle(&quot;Deviations from the population mean&quot;)

p1 + p2</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/samples2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Here the sum in the left graph is <span class="math inline">\((2 - 6)² + (10 - 6)² = 32\)</span> and the sum in the right graph is <span class="math inline">\((2 - 5.8)² + (10 - 5.8)² = 32.08\)</span>. The difference is small now, but using the sample mean still results in a smaller sum compared to using the population mean.</p>
<p>In short, the source of the bias comes from using the sample mean instead of the population mean. The sample mean is always guaranteed to be in the middle of the observed data, thereby reducing the variance, and creating an underestimation.</p>
</div>
<div id="the-solution" class="section level2">
<h2>The Solution</h2>
<p>Now that we know that the bias is caused by using the sample mean, we can figure out how to solve the problem.</p>
<p>Looking at the previous graphs, we see that if the sample mean is far from the population mean, the sample variance is smaller and the bias is large. If the sample mean is close to the population mean, the sample variance is larger and the bias is small. So, the more the sample mean moves around the population mean, the greater the bias.</p>
<p>In other words, besides the variance of the data points around the sample mean, there is also the variance of the sample mean around the population mean. We need both variances in order to accurately estimate the population variance.</p>
<p>The population variance is thus the sum of two variances:</p>
<p><span class="math display">\[\sigma^2_{sample} + \sigma^2_{\vphantom{sample}mean} = \sigma^2_{population}\]</span> Let’s confirm that this is true. For that we need to know how to calculate the variance of the sample mean around the population mean. This is relatively simple; it’s the variance of the population divided by n (<span class="math inline">\(\frac{\sigma^2}n\)</span>). This makes sense because the greater the variance in the population, the more the mean can jump around, but the more data you sample, the closer you get to the population mean.</p>
<p>Now that we can calculate both the variance of the sample and the variance of the sample mean, we can check whether adding them together results in the population variance.</p>
<p>Below I show a graph in which I again sampled from our population with varying sample sizes. For each sample, I calculated the sample variance (the biased one) and the variance of the mean of that sample (<span class="math inline">\(\frac{\sigma^2}n\)</span>)<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. I did this 1000 times per sample size, took the average of each and put them on top of each other. I also added a dashed line to indicate the variance of the population, which is the benchmark we’re trying to reach.</p>
<pre class="r"><code># Calculate the variance sources per sample size
variance_sources &lt;- samples %&gt;%
  mutate(var_mean = var_unbiased / n) %&gt;%
  group_by(n) %&gt;%
  summarize(
    var_biased = mean(var_biased),
    var_unbiased = mean(var_unbiased),
    var_mean = mean(var_mean)
  ) %&gt;%
  pivot_longer(cols = c(var_biased, var_mean), names_to = &quot;variance_source&quot;, 
    values_to = &quot;variance&quot;) %&gt;%
  mutate(variance_source = recode(variance_source, &quot;var_biased&quot; = 
      &quot;sample&quot;, &quot;var_mean&quot; = &quot;sample mean&quot;))

ggplot(variance_sources, aes(x = n, fill = variance_source, y = variance)) +
  geom_col() +
  geom_hline(yintercept = sigma, linetype = &quot;dashed&quot;, color = yellow) +
  scale_fill_manual(values = c(green, yellow)) +
  labs(x = &quot;Sample size (n)&quot;, y = &quot;Variance&quot;, fill = &quot;Variance source:&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Indeed, we see that the variance of the sample and the variance of the mean of the sample together form the population variance.</p>
</div>
<div id="the-math" class="section level2">
<h2>The Math</h2>
<p>Now that we know that the variance of the population consists of the variance of the sample and the variance of the sample mean, we can figure out the correction factor we need to apply to make the biased variance measure unbiased.</p>
<p>Previously, we found an interesting pattern in the simulated samples, which is also visible in the previous figure. We saw that at a sample size <span class="math inline">\(n=2\)</span>, the (biased) sample variance appears to be half that of the (unbiased) population variance. At sample size <span class="math inline">\(n=3\)</span>, it’s <span class="math inline">\(\frac23\)</span>. At sample size <span class="math inline">\(n=4\)</span>, it’s <span class="math inline">\(\frac34\)</span>, and so on.</p>
<p>This means that we can fix the biased variance measure by multiplying it with <span class="math inline">\(\frac{n}{(n-1)}\)</span>. At sample size <span class="math inline">\(n = 2\)</span>, this would mean we multiply the biased variance by <span class="math inline">\(\frac21 = 2\)</span>. For sample size <span class="math inline">\(n=3\)</span>, <span class="math inline">\(\frac32 = 1.5\)</span>. At sample size <span class="math inline">\(n=4\)</span>, <span class="math inline">\(\frac43 = 1 \frac13\)</span>, resulting in the unbiased variance.</p>
<p>In other words, to unbias the biased variance measure, we multiply it by a correction factor of <span class="math inline">\(\frac{n}{(n-1)}\)</span>. But where does this correction factor come from?</p>
<p>Well, because the sample variance misses the variance of the sample mean, we can expect that the variance of the sample is biased by an amount equal to the variance of the population minus the variance of the sample mean. In other words:</p>
<p><span class="math display">\[\sigma^2 - \frac{\sigma^2}n\]</span></p>
<p>Rewriting this <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>, produces:</p>
<p><span class="math display">\[\sigma^2\cdot\frac{n - 1}n\]</span> The variance of a sample will be biased by an amount equal to <span class="math inline">\(\frac{n - 1}n\)</span>. To correct that bias we should multiply the sample variance by the inverse of this bias: <span class="math inline">\(\frac{n}{n-1}\)</span> <a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. This is also called <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a>.</p>
<p>So, an unbiased measure of our sample variance is the biased sample variance times the correction factor:</p>
<p><span class="math display">\[\frac{\sum(x_i - \overline{x})^2}{n}\cdot{\frac n{n-1}}\]</span> Because the <em>n</em> in the denominator of the left term (the biased variance formula) cancels out the <em>n</em> in the numerator of the right term (the bias correction), the formula can be rewritten as:</p>
<p><span class="math display">\[\frac{\sum(x_i - \overline{x})^2}{n-1}\]</span></p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>We calculate the variance of a sample by summing the squared deviations of each data point from the sample mean and dividing it by <span class="math inline">\(n - 1\)</span>. The <span class="math inline">\(n - 1\)</span> actually comes from a correction factor <span class="math inline">\(\frac n{n-1}\)</span> that is needed to correct for a bias caused by taking the deviations from the sample mean rather than the population mean. Taking the deviations from the sample mean only constitutes the variance around the sample mean, but ignores the variation of the sample mean around the population mean, producing an underestimation equal to the size of the variance of the sample mean: <span class="math inline">\(\frac{\sigma^2}{n}\)</span>. The correction factor corrects for this underestimation, producing an unbiased estimate of the population variance.</p>
<p><em>This post was last updated on 2021-06-22.</em></p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Here I cheat a little because in order to calculate the variance of the sample mean, I need to use the unbiased variance formula.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Here are the steps to rewrite the formula: <span class="math display">\[\sigma^2 - \frac{\sigma^2}n\]</span> Add an n to the numerator and denominator of the left term: <span class="math display">\[\frac{\sigma^2n}n - \frac{\sigma^2}n\]</span> Combine the terms: <span class="math display">\[\frac{\sigma^2n - \sigma^2}n\]</span> Simplify the numerator: <span class="math display">\[\frac{\sigma^2(n - 1)}n\]</span> Move <span class="math inline">\(\sigma^2\)</span> out of the numerator: <span class="math display">\[\sigma^2\cdot\frac{n - 1}n\]</span><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>The inverse of <span class="math inline">\(\frac{n - 1}n\)</span> is <span class="math inline">\(\frac{1}{\frac{n - 1}n}\)</span>. Multiply both the numerator and the denominator by <span class="math inline">\(n\)</span> and you get <span class="math inline">\(\frac{n}{n-1}\)</span>.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
