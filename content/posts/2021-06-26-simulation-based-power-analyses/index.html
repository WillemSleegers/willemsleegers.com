---
title: Simulation-based power analyses
date: '2021-06-26'
slug: simulation-based-power-analyses
tags:
  - statistics
  - power analysis
output:
  blogdown::html_page:
    toc: true
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#setup">Setup</a></li>
<li><a href="#group---1-dv">1 group - 1 DV</a></li>
<li><a href="#groups---1-dv">2 groups - 1 DV</a>
<ul>
<li><a href="#between-subjects-unequal-variance-and-unequal-sample-sizes">Between-subjects, unequal variance, and unequal sample sizes</a></li>
</ul></li>
</ul>
</div>

<p>Doing power analyses is hard. I know this from experience, both as a researcher and as a reviewer. As a researcher, I find power analyses difficult because performing a good power analysis requires a good understanding of the data. Understanding one’s data is often underestimated, I think. We’re very quick to design a study and start data collection, without often knowing what our data will look like. As a reviewer, I see that power analyses are difficult because of wrong ideas about what a power analysis should look like. The most common misconception I see is that researchers think they should power their study, rather than the set of analyses they will conduct. I also see a lot of power analyses conducted with G*Power, which sometimes looks fine, but oftentimes produces results I know to be wrong (usually involving interaction tests). So what to do?</p>
<p>My favorite way to run power analyses is via simulation. Simulation-based power analyses are more difficult and take longer to setup and run, but they’re more pedagogical. Simulations require you to understand your data because you have to define the exact parameters that define your data set (e.g., means, standard deviations, correlations). It also creates a very intuitive understanding of what power is: Power is simply counting how often you find the results you expect to find.</p>
<p>Still, running simulation-based power analyses might be too difficult for some. So in this blog post I present code to simulate data for a range of different scenarios.</p>
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>Run the following code to get started. The most important package here is <code>MASS</code>. It contains a function called <code>mvrnorm</code> that enables us to simulate data from a multivariate normal distribution. This means we’ll simulate data for scenarios where we have a continuous outcome. I really like this function for simulating data because it has an argument called <code>empirical</code> that you can set to <code>TRUE</code>, which causes your simulated data to have the exact properties you set. This is a great way to check out your simulated data and see if it makes sense.</p>
<p>We will use the <code>tidyverse</code> because we need to prepare the data after simulating it. <code>mvrnorm</code> returns a matrix with each dependent variable as a column. This means we sometimes need to prepare the data so that we can perform the tests we want to run or for visualization purposes.</p>
<p>The <code>tidystats</code> and <code>effectsize</code> packages will be used to inspect the data and check whether it looks plausible.</p>
<p>Finally, we use the <code>broom</code> package to extract p-values from the tests we run. This will be necessary to calculate the power because power is (usually) nothing more than the number of significant p-values divided by the number of studies we simulated. In a future post I might focus on Bayesian analyses, so we won’t be dealing with p-values then, although the logic remains the same.</p>
<p>Besides loading packages, we also set the <code>s</code> variable. The value of this variable will determine how many times we’ll simulate the data. The higher this number, the more accurate our power estimates will be.</p>
<pre class="r active-code"><code># Load packages
library(MASS)
library(tidyverse)
library(tidystats)
library(effectsize)
library(broom)

# Set global parameters
s &lt;- 1000 # Number of loops in the power simulation

# Optional: Create color variables for the plots
green &lt;- &quot;#00B88D&quot;
yellow &lt;- &quot;#f39c12&quot;
off_white &lt;- &quot;#cccccc&quot;</code></pre>
<p>With the setup out of the way, let’s cover our general approach to power analyses:</p>
<ol style="list-style-type: decimal">
<li>Simulate the data with fixed properties</li>
<li>Check the data to see if the data is plausible</li>
<li>Run the tests we want to run on this data</li>
<li>Repeat steps 1 to 3 many times, save the p-values, and calculate power</li>
</ol>
<p>We’ll do this for various scenarios. In each scenario we start by defining the parameters. I’ll focus on providing means, standard deviations, and correlations, because those are usually the parameters we report in the results section, so I’m guessing most researchers will have some intuitions about what these parameters mean and whether the results are plausible.</p>
<p>The <code>mvrnorm</code> function requires that we pass it the sample size, the means, and a variance-covariance matrix. The first two are easy to understand, but the variance-covariane may not be. It’s relatively straightforward to convert means, SDs, and correlations to a variance-covariance matrix, though. Variance is simply the standard deviation squared and the covariance is the product of the standard deviations of the two variables and their correlation. You’ll see in some scenarios below that this is how I construct the variance-covariance matrix.</p>
<p>Note that the result of each power analysis will be the power, and not the sample size needed to obtain a particular power. This is the same as calculating the post-hoc power in G*Power. If you want to figure out what the sample size is for a particular power (e.g., 80%) then you simply change the sample size parameter until you have the power you want.</p>
</div>
<div id="group---1-dv" class="section level2">
<h2>1 group - 1 DV</h2>
<p>The simplest scenario is where we want to simulate a set of normally distributed values for a single group. This requires that we set three parameters: a mean, a standard deviation, and a sample size. We give <code>mvrnorm</code> the sample size (<code>N</code>), the mean (<code>M</code>), and the variance (<code>SD^2</code>).</p>
<pre class="r active-code"><code># Parameters
M &lt;- 0.75
SD &lt;- 5
N &lt;- 90

# Simulate once with empirical = TRUE
responses &lt;- mvrnorm(N, mu = M, Sigma = SD^2, empirical = TRUE)

# Prepare data
colnames(responses) &lt;- &quot;DV&quot;
data &lt;- as_tibble(responses)</code></pre>
<p>The next step is to inspect the data to see whether the parameters are plausible. This can be done by converting the parameters to a standardized effect size and by visualizing the data.</p>
<pre class="r"><code># Calculate a standardized effect size
effect_size &lt;- cohens_d(data$DV)

# Plot the simulated data
ggplot(data, aes(x = DV)) +
  geom_histogram(fill = green, color = background_color) +
  geom_vline(xintercept = M, linetype = &quot;dashed&quot;, color = yellow)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/1-group-inspect-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The histogram roughly shows that we have a mean of 0.75 and a standard deviation of 5. We also calculated the Cohen’s d as a measure of the size of the effect. This data is equal to a Cohen’s d of 0.15.</p>
<p>Next is the analysis we want to power for. In this case, a one-sample t-test can be run to see whether the mean is significantly different from 0. The function for this is <code>t.test</code>.</p>
<pre class="r"><code>t.test(data$DV)</code></pre>
<p>To calculate the power, we repeat the analysis <code>s</code> times. Each time we store the <em>p</em>-value so that later we can calculate the proportion of significant results. Since we don’t need to inspect the data each time, we skip the data preparation step and use the matrix returned by <code>mvrnorm</code> immediately in <code>t.test</code>.</p>
<pre class="r active-code"><code># Power analysis
p_values &lt;- vector(length = s)

for (i in 1:s) {
  # Simulate
  responses &lt;- mvrnorm(N, mu = M, Sigma = SD^2)
  
  # Run test
  test &lt;- t.test(responses[, 1])
  
  # Extract p-value
  p_values[i] &lt;- test$p.value
}

# Calculate power
power &lt;- sum(p_values &lt;= .05) / length(p_values) * 100</code></pre>
<p>With the current parameters, we obtain a power of 28.2. You can adjust the sample size parameter and re-run the code until you know which sample size gives you the desired power.</p>
</div>
<div id="groups---1-dv" class="section level2">
<h2>2 groups - 1 DV</h2>
<p>The next scenario is one in which we have two groups and a single DV. Even in this simple scenario there are already several variations that are important to consider. Do we assume equal variances between groups? Do we assume equal samples sizes? Is the design between or within-subjects?</p>
<div id="between-subjects-unequal-variance-and-unequal-sample-sizes" class="section level3">
<h3>Between-subjects, unequal variance, and unequal sample sizes</h3>
<p>If we want to be able to vary both variances and samples sizes, we can use the code from the previous scenario and simply run it twice, once for each group separately.</p>
<pre class="r active-code"><code># Parameters
M1 &lt;- 5
M2 &lt;- 4
SD1 &lt;- 2
SD2 &lt;- 2
N1 &lt;- 50
N2 &lt;- 40

# Simulate once with empirical = TRUE
group1 &lt;- mvrnorm(N1, mu = M1, Sigma = SD1^2, empirical = TRUE)
group2 &lt;- mvrnorm(N2, mu = M2, Sigma = SD2^2, empirical = TRUE)

# Prepare data
colnames(group1) &lt;- &quot;DV&quot;
colnames(group2) &lt;- &quot;DV&quot;

group1 &lt;- group1 %&gt;%
  as_tibble() %&gt;%
  mutate(condition = &quot;control&quot;)

group2 &lt;- group2 %&gt;%
  as_tibble() %&gt;%
  mutate(condition = &quot;experimental&quot;)

data &lt;- bind_rows(group1, group2)</code></pre>
<p>We can inspect the data by calculating a Cohen’s d and visualizing the results.</p>
<pre class="r"><code># Calculate a standardized effect size
effect_size &lt;- cohens_d(DV ~ condition, data = data)

# Visualize the data
ggplot(data, aes(x = condition, y = DV)) + 
  geom_jitter(width = .2, alpha = .5, color = green) + 
  stat_summary(color = yellow)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The difference between the two groups is equal to a Cohen’s d of 0.5.</p>
<p>An appropriate analysis in this case is a Welch’s two-sample t-test because we have different variances between groups.</p>
<pre class="r"><code>t.test(DV ~ condition, data = data)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  DV by condition
## t = 2, df = 84, p-value = 0.02
## alternative hypothesis: true difference in means between group control and group experimental is not equal to 0
## 95 percent confidence interval:
##  0.16 1.84
## sample estimates:
##      mean in group control mean in group experimental 
##                          5                          4</code></pre>
<p>The power analysis looks as followed:</p>
<pre class="r"><code># Power analysis
p_values &lt;- vector(length = s)

for (i in 1:s) {
  # Simulate
  group1 &lt;- mvrnorm(N1, mu = M1, Sigma = SD1^2)
  group2 &lt;- mvrnorm(N2, mu = M2, Sigma = SD2^2)
  
  # Run test
  test &lt;- t.test(group1[, 1], group2[, 1])
  
  # Extract p-value
  p_values[i] &lt;- test$p.value
}

# Calculate power
power &lt;- sum(p_values &lt;= .05) / length(p_values) * 100</code></pre>
<p>This produces a power of 63.</p>
<p>If we want to assume equal variances and equal sample sizes, we can simulate the data a bit differently. First, we’ll only need 4 parameters. Second, we don’t need to separately simulate the data for each group. We can instead use a single <code>mvrnorm</code> call and provide it with the correct variance-covariance matrix. The crucial bit is to only set the variances and set the covariances to 0. If we do it this way, we do need to prepare the resulting matrix a bit differently. <code>mvnnorm</code> returns a matrix that, when converted to a data frame, results in a wide data frame. That is, the DV of each group is stored in separate columns.</p>
<pre class="r active-code"><code># Parameters
M1 &lt;- 5
M2 &lt;- 4
SD &lt;- 2
N &lt;- 40

# Prepare parameters
mus &lt;- c(M1, M2)
Sigma &lt;- matrix(nrow = 2, ncol = 2, 
    c(
      SD^2, 0,
      0, SD^2
    )
  )

# Simulate once with empirical = TRUE
responses &lt;- mvrnorm(N, mu = mus, Sigma = Sigma, empirical = TRUE)

# Prepare data
colnames(responses) &lt;- c(&quot;group1&quot;, &quot;group2&quot;)
data &lt;- as_tibble(responses)

data_long &lt;- pivot_longer(data, cols = everything(), names_to = &quot;condition&quot;, 
  values_to = &quot;DV&quot;)</code></pre>
</div>
</div>
