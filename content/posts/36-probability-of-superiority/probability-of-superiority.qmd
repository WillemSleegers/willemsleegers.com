---
title: "Probability of superiority"
description: "The probability of superiority is, according to some, a more intuitive effect size measure than other more commonly reported effect sizes. In this blog post I work through some code to calculate the probabily of superiority. "
date: 2024-03-20
categories:
  - statistics
  - effect size
  - probability of superiority
code-tools: true
code-fold: show
draft: true
---

The probability of superiority for a comparison between two groups is  the probability that, when sampling an observation from each of the groups at random, the observation from the second group will be larger than the sample from the first group. It ranges from 0 to 1 (because it's a probability), where 0.5 means there is no difference between the two groups.

An advantage of the probability of superiority, I'll shorten it to p-sup, is that it's more easily understood than something like a Cohen's d. There are still some limitations (e.g., see some discussed here), but I think overall it's an interesting effect size and I should use it more often.

So, in this blog post I'll figure out how to calculate it.

Run the following setup code if you want to follow along.

```{r}
#| label: setup
#| message: false
library(tidyverse)
library(effectsize)
library(RProbSup)
library(brms)
library(tidybayes)

data <- read_csv("data.csv")
data <- drop_na(data)

options(
  mc.cores = 4,
  brms.threads = 4,
  brms.backend = "cmdstanr",
  brms.file_refit = "on_change"
)
```

## Manually calculating p-sup

Let's manually calculate p-sup with some simple data. In the code block below I create two variables containing some data; one per group. I'll also store the length of one of them in a variable because we'll use it later.

```{r}
#| label: manual-data
a <- c(2, 3, 4, 5, 6)
b <- c(1, 2, 3, 4, 5)

n <- length(a)
```

With these two groups of data we want to calculate p-sup. The steps needed consist of making all pairwise comparisons between members of each group, tallying the number of times that the former scores are higher than the latter (and incrementing by 0.5 if they are tied), and dividing by the total number of comparisons that were made (also see [Ruscio, 2008](https://doi.org/10.1037/1082-989X.13.1.19)).

Let's do this step by step and start with making all pairwise comparisons. We can do that using the `outer()` function. We give it two sets of data and tell it which comparison to make. In the code below we have it check whether values in `a` are higher than values in `b`.

```{r}
#| label: manual-outer
outer(a, b, ">")
```

The output is a matrix with `TRUE`s and `FALSE`s. For example, the result in row 1 and column 1 is `TRUE`, indicating that the first element of `a` (2) is larger than the first element of `b` (1). Another example, the result in row 1 column 2 is `FALSE` because the second element of `b` (2) is not larger than the first element of `a` (2), it's a tie.

Next, we need to count the number of times that an element in `a` is larger than an element in `b`. This is easily done by using `sum()` on the matrix. This works because `TRUE` is treated as 1 and `FALSE` as 0.

```{r}
#| label: manual-sum
sum(outer(a, b, ">"))
```

To deal with ties, we count the number of times a tie happens and divide the sum by 2, thereby saying that ties are treated as one value being larger than the other half the time.

Both of the sums are summed together, giving us:

```{r}
#| label: manual-sums
(sum(outer(a, b, ">")) + 0.5 * sum(outer(a, b, "==")))
```

This is actually the same thing as the Wilcoxon (or Mann-Whitney) test statistic.

```{r}
#| label: manual-wilcox
#| warning: false
wilcox.test(a, b)
```

Dividing this number by the total number of comparisons gives us the probability of superiority.

```{r}
#| label: manual-p-sup
(sum(outer(a, b, ">")) + 0.5 * sum(outer(a, b, "=="))) / (n * n)
```

Let's turn this into a function and then compare the result to what we get when we use some ready-made functions from various packages.

```{r}
#| label: manual-comparison
p_sup <- function(a, b) {
  # Count number of times a > b
  sum <- sum(outer(a, b, ">"))

  # Count ties, counting each tie as a half
  ties <- sum(outer(a, b, "==")) * 0.5

  # Sum both and divide by total number of comparisons
  p_sup <- (sum + ties) / (length(a) * length(b))

  return(p_sup)
}

df <- cbind(c(a, b), c(rep("a", 5), rep("b", 5)))

tribble(
  ~package, ~code, ~p_sup,
  "-", "p_sup(a, b)", p_sup(a, b),
  "effectsize",
  "p_superiority(a, b, parametric = FALSE)",
  p_superiority(a, b, parametric = FALSE)$p_superiority,
  "RProbSup", "A(df, 1, 2)", A(df, 1, 2)$A
)
```

## A Frequentist example

```{r}
props <- data |>
  group_by(sex) |>
  count(importance_animal_rights) |>
  mutate(prop = n / sum(n))

ggplot(props, aes(x = importance_animal_rights, y = prop, fill = sex)) +
  geom_col(position = "dodge")

p_superiority(importance_animal_rights ~ sex, data = data, parametric = FALSE)$p_superiority

p_sup(
  pull(filter(data, sex == "Female"), importance_animal_rights),
  pull(filter(data, sex == "Male"), importance_animal_rights)
)
```

## A Bayesian example

```{r}
model <- brm(
  importance_animal_rights ~ sex,
  data = data, file = "model.Rds"
)

draws <- data |>
  add_predicted_draws(model, value = "predicted_outcome")

p_sups <- draws |>
  group_by(.draw) |>
  summarize(
    p_sup = p_superiority(
      predicted_outcome ~ sex,
      parametric = FALSE
    )$p_superiority
  )

median_qi(p_sups, p_sup)

ggplot(p_sups, aes(x = p_sup)) +
  geom_histogram()
```