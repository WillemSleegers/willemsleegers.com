---
title: "Group comparisons all at once"
description: "An example of how to run multiple group comparisons all at once using a multi-level model."
date: 2025-05-02
categories:
  - R
  - statistics
knitr:
  opts_chunk:
    fig.path: "../../../public/figures/45-group-comparisons-all-at-once/"
draft: true
---

On multiple occasions I've had to analyze data and check whether one or more groups differ on any of multiple outcomes. When I first had to do this, I analyzed this mainly by running separate regressions, one for each outcome, to see whether there was a difference. I then realized it's probably possible to test for group differences on multiple outcome all in the same (multi-level) model. In this post I show how to do that and also check whether it's the same as running regressions separately.

Run the following setup code if you want to follow along. Note that it includes a custom `rbinary()` function that generates some binary data for us, which I use in the examples below.

```{r}
#| label: setup
#| message: false
#| code-fold: true
library(tidyverse)
library(lme4)
library(marginaleffects)

# Custom function to generate a vector of 1s and 0s with a specified proportion of 1s
rbinary <- function(p, n, shuffle = TRUE) {
  # Validate inputs
  if (!is.numeric(n) || length(n) != 1 || n <= 0 || n != round(n)) {
    stop("n must be a positive integer")
  }

  if (!is.numeric(p) || length(p) != 1 || p < 0 || p > 1) {
    stop("p must be a numeric value between 0 and 1")
  }

  # Check if p*n is an integer to avoid rounding
  if (abs(p * n - round(p * n)) > .Machine$double.eps^0.5) {
    stop(
      "The product of proportion (p) and sample size (n) must be an integer to avoid rounding"
    )
  }

  # Generate the binary vector
  ones_count <- p * n
  result <- c(rep(1, ones_count), rep(0, n - ones_count))

  # Shuffle the vector only if shuffle is TRUE
  if (shuffle) {
    result <- sample(result)
  }

  return(result)
}
```

## Data

In order to test out whether we can look at group differences for multiple outcomes, we need data. Let's imagine that we're interested in assessing the proportions of multiple outcomes for different groups. In the code below I set some parameters such as the sample size per condition (`n`) and the proportion per group per outcome, and then I generate some data using the custom `rbinary()` function. I format the data frame to be a long format (one outcome value per row) and do some housekeeping such as setting the factor levels of the outcome variable.

```{r}
#| label: data
n <- 100
prop_one_a <- 0.5
prop_one_b <- 0.5
prop_two_a <- 0.25
prop_two_b <- 0.5
prop_three_a <- 0.25
prop_three_b <- 0.75

data <- tibble(
  one_a = rbinary(prop_one_a, n = n),
  one_b = rbinary(prop_one_b, n = n),
  two_a = rbinary(prop_two_a, n = n),
  two_b = rbinary(prop_two_b, n = n),
  three_a = rbinary(prop_three_a, n = n),
  three_b = rbinary(prop_three_b, n = n)
) |>
  pivot_longer(
    cols = everything(),
    names_to = c("outcome", "condition"),
    names_pattern = "(one|two|three)_(a|b)"
  ) |>
  mutate(
    outcome = fct(outcome, levels = c("one", "two", "three")),
    condition = str_to_upper(condition)
    )
```

Let's take a look at what this data frame looks like:

```{r}
#| label: data-head
head(data)
```

And let's also make sure the proportions are what they are supposed to be (e.g., the proportion of condition A should be `r prop_one_a`).

```{r}
#| label: data-props
data |>
  group_by(condition, outcome) |>
  count(value) |>
  mutate(prop = n / sum(n)) |>
  filter(value == 1)
```

That looks correct. One thing we now would like to be able to do is reproduce these proportions using a regression technique (i.e., a logistic regression) and also obtain the difference in proportion between the two groups, by outcome.

## Analyzing a single outcome

Before analyzing the group differences for each outcome all at once, let's first simply look at a difference between conditions for one of the outcomes. This is fairly straightforwardly done using a logistic regression.

```{r}
#| label: glm
fit_glm <- data |>
  filter(outcome == "two") |>
  glm(value ~ condition, data = _, family = binomial())

summary(fit_glm)
```

Let's use the marginaleffects package to look at the difference between condition A and B for this outcome.

```{r}
#| label: glm-comparison
avg_comparisons(
  fit_glm,
  variables = "condition"
)
```

We see a difference of 0.25, which matches our parameter values (`r prop_two_b` - `r prop_two_a` = `r prop_two_b - prop_two_a`). 

## All at once

Now, let's run them all at once using a multilevel logistic regression model. The key is how to specify the formula. 

```{r}
#| label: all-at-once
fit_glmer <- glmer(
  value ~ 0 + outcome + outcome:condition + (1 | outcome:condition),
  data = data,
  family = binomial()
)

summary(fit_glmer)
```

We can again use the marginaleffects package to obtain the differences between the two conditions, this time for each outcome separately:

```{r}
avg_comparisons(
  fit_glmer,
  variables = "condition",
  by = "outcome",
)
```
There we go. Now we performed three logistic regression analyses using a single multilevel model. Note that the results for outcome two are the same as for the simple logistic regression we performed earlier.

Of course, with only three outcomes this might not seem particularly worth it, but you can use this for a larger number of outcomes as well, at which point this becomes quite convenient.