---
title: "Group comparisons all at once"
description: "An example of how to run multiple group comparisons all at once using a single regression model."
date: 2025-05-02
categories:
  - R
  - statistics
knitr:
  opts_chunk:
    fig.path: "../../../public/figures/45-group-comparisons-all-at-once/"
---

On multiple occasions I've had to analyze data and check whether one or more groups differ on any of multiple outcomes. My initial approach to doing this consisted of simply running separate regressions, one for each outcome. I then realized it's probably possible to test for group differences on multiple outcome all in the same model. In this post I show how to do that and also check whether it's the same as running regressions separately.

Run the following setup code if you want to follow along. Note that it includes a custom `rbinary()` function that generates some binary data for us, which I use in the examples below.

```{r}
#| label: setup
#| message: false
#| code-fold: true
library(tidyverse)
library(lme4)
library(marginaleffects)

# Custom function to generate a vector of 1s and 0s with a specified proportion of 1s
rbinary <- function(p, n, shuffle = TRUE) {
  # Validate inputs
  if (!is.numeric(n) || length(n) != 1 || n <= 0 || n != round(n)) {
    stop("n must be a positive integer")
  }

  if (!is.numeric(p) || length(p) != 1 || p < 0 || p > 1) {
    stop("p must be a numeric value between 0 and 1")
  }

  # Check if p*n is an integer to avoid rounding
  if (abs(p * n - round(p * n)) > .Machine$double.eps^0.5) {
    stop(
      "The product of proportion (p) and sample size (n) must be an integer to avoid rounding"
    )
  }

  # Generate the binary vector
  ones_count <- p * n
  result <- c(rep(1, ones_count), rep(0, n - ones_count))

  # Shuffle the vector only if shuffle is TRUE
  if (shuffle) {
    result <- sample(result)
  }

  return(result)
}
```

## Data

Say that we're interested in assessing the proportions of multiple outcomes for different groups. In the code below I set some parameters such as the sample size per condition (`n`) and the proportion per group per outcome. I then generate some data using the custom `rbinary()` function which generates a vector of 1s and 0s with the proportion of 1s matching the proportion given in the function. After that, I format the data frame to be a long format (one outcome value per row) and do some housekeeping such as setting the factor levels of the outcome variable and converting the condition letter to upper case.

```{r}
#| label: data
n <- 100
prop_one_a <- 0.5
prop_one_b <- 0.6
prop_two_a <- 0.25
prop_two_b <- 0.5
prop_three_a <- 0.25
prop_three_b <- 0.75

data <- tibble(
  one_a = rbinary(prop_one_a, n = n),
  one_b = rbinary(prop_one_b, n = n),
  two_a = rbinary(prop_two_a, n = n),
  two_b = rbinary(prop_two_b, n = n),
  three_a = rbinary(prop_three_a, n = n),
  three_b = rbinary(prop_three_b, n = n)
) |>
  pivot_longer(
    cols = everything(),
    names_to = c("outcome", "condition"),
    names_pattern = "(one|two|three)_(a|b)"
  ) |>
  mutate(
    outcome = fct(outcome, levels = c("one", "two", "three")),
    condition = str_to_upper(condition)
    )
```

Let's take a look at what this data frame looks like:

```{r}
#| label: data-head
head(data)
```

And let's also make sure the proportions are what they are supposed to be (e.g., the proportion of condition A in outcome 'one' should be `r prop_one_a`).

```{r}
#| label: data-props
data |>
  group_by(outcome, condition) |>
  count(value) |>
  mutate(prop = n / sum(n)) |>
  filter(value == 1)
```

That looks correct. One thing we now would like to be able to do is obtain the differences in proportions between the two groups, by outcome.

## Analyzing a single outcome

Before analyzing the group differences for each outcome all at once, let's first simply look at a difference between conditions for one of the outcomes. This is fairly straightforwardly done using a logistic regression.

```{r}
#| label: fit-one
fit_one <- data |>
  filter(outcome == "one") |>
  glm(value ~ condition, data = _, family = binomial())

summary(fit_one)
```

We can (and should) use the [marginaleffects](https://marginaleffects.com) package to look at the difference between condition A and B for this outcome, using its `avg_comparisons()` function.

```{r}
#| label: comparison-one
avg_comparisons(
  fit_one,
  variables = "condition"
)
```

We see a difference of 0.1, which matches our parameter values (`r prop_one_b` - `r prop_one_a` = `r prop_one_b - prop_one_a`).

## Multiple outcomes all at once

Now, let's run them all at once. The key is how to specify the formula. Using our two variables (`outcome` and `condition`), we should use the following formula:

```         
value ~ 0 + outcome + outcome:condition
```

With this formula, we:

1.  Supress the global intercept so each `outcome` gets their own intercept, rather than one of the outcomes becoming the reference category
2.  Include `outcome` so they get their own intercepts
3.  Include the interaction term `outcome:condition` so we can estimate separate condition effects for each outcome

The code to run the model looks as follows:

```{r}
#| label: fit-all
fit_all <- glm(
  value ~ 0 + outcome + outcome:condition,
  data = data,
  family = binomial()
)

summary(fit_all)
```

We can again use the marginaleffects package to obtain the differences between the two conditions, this time for each outcome separately by using the `by` argument in `avg_comparisons()`.

```{r}
#| label: comparison-all
avg_comparisons(
  fit_all,
  variables = "condition",
  by = "outcome",
)
```

There we go. Now we performed three logistic regression analyses using a single model. Note that the results for outcome two are the same as for the simple logistic regression we performed earlier.

Of course, with only three outcomes this might not seem particularly worth it, but you can use this for a larger number of outcomes as well, at which point this becomes quite convenient. Do note that you should only do this if the outcomes are similar (e.g., all binary) and that you should not do this with widely different types of outcomes.

## Dependent outcomes

In cases where the outcomes are not independent (e.g., using a within-subjects design), you can extend this approach using a multilevel model. In that case, the formula would look like:

```         
value ~ 0 + outcome + outcome:condition + (1 | outcome:condition)
```

## Summary

It's possible to test group differences for multiple outcomes using a single regression model. This means all results will be stored in a single model object, making it easy to extract predictions and group comparisons all at once.