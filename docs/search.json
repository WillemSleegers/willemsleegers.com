[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dr. Willem Sleegers",
    "section": "",
    "text": "Senior Behavioral Scientist at Rethink Priorities\nResearch Affiliate at Tilburg University\n \n  \n   \n  \n    \n     E-mail\n  \n  \n    \n     Twitter\n  \n  \n    \n     Google Scholar\n  \n  \n    \n     GitHub"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "dr. Willem Sleegers",
    "section": "About",
    "text": "About\nI’m a Senior Behavioral Scientist at Rethink Priorities. I am part of the survey team, which means I conduct research on attitude assessments and attitude change, using surveys and experimental designs. Before joining Rethink Priorities, I was an assistant professor in the Department of Social Psychology at Tilburg University, where I continue to be affiliated. On this website you can find information about some of the projects I’m involved in. I also blog about various topics related to my work.\nLearn more"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "dr. Willem Sleegers",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\nAnimal welfare\n\n\nFarm animals and wild animals suffer in horrible ways in great numbers. At Rethink Priorities, I contribute to various projects aimed at addressing this important problem.\n\n\n\n\n\n\n\n\n\n\nCognitive dissonance\n\n\nIn this project I aim to assess the evidence for cognitive dissonance theory using a large-scaled replication study of a seminal cognitive dissonance study.\n\n\n\n\n\n\n\n\n\n\nstatcheck\n\n\nTogether with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer statistics-related typos.\n\n\n\n\n\n\n\n\n\n\ntidystats\n\n\ntidystats refers to a collection of software solutions to improve how statistics are reported and shared in the field of (social) psychology.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "dr. Willem Sleegers",
    "section": "Blog posts",
    "text": "Blog posts\n\n\n\n\n\nFormatting numbers\n\n\n\n\n\n\nR\n\n\nfunction\n\n\n\nA showcase of the function I use to format numbers.\n\n\n\n\n\nMay 17, 2024\n\n\n\n\n\n\n\nThe metalog distribution\n\n\n\n\n\n\nstatistics\n\n\ndistributions\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nApr 9, 2024\n\n\n\n\n\n\n\nPredicting values of intercept only models\n\n\n\n\n\n\nstatistics\n\n\nprediction\n\n\n\nA post on how to predict values of intercept-only models.\n\n\n\n\n\nMar 18, 2024\n\n\n\n\n\n\nNo matching items\n\n\nMore posts"
  },
  {
    "objectID": "index.html#cv",
    "href": "index.html#cv",
    "title": "dr. Willem Sleegers",
    "section": "CV",
    "text": "CV\nI’m an Senior Behavioral Scientist at Rethink Priorities. I’m a former academic with 10 years of research experience. I have published, and continue to publish, scientific papers in peer-reviewed journals and I also develop research-related software. My skill set consists of various research skills (e.g., experimental design, data analysis, writing) and technical skills (e.g., programming). I’m currently focused on applying my skills to topics of high impact together with my colleagues at Rethink Priorities.\nLearn more"
  },
  {
    "objectID": "content/cv/cv.html",
    "href": "content/cv/cv.html",
    "title": "dr. Willem Sleegers",
    "section": "",
    "text": "Senior Behavioral Scientist at Rethink Priorities\nResearch Affiliate at Tilburg University\n \n  \n   \n  \n     E-mail\n  \n  \n     X\n  \n  \n     Bluesky\n  \n  \n     Google Scholar\n  \n  \n     Github"
  },
  {
    "objectID": "content/cv/cv.html#employment",
    "href": "content/cv/cv.html#employment",
    "title": "dr. Willem Sleegers",
    "section": "Employment",
    "text": "Employment\n\n\n\n\n\n\n\n\n\n2021-current\nSenior Behavioral Scientist at Rethink Priorities\n\n\n2018-2021\nTenure track Assistant Professor in social psychology at Tilburg University\n\n\n2016-2018\nFixed term Assistant Professor in social psychology at Tilburg University\n\n\n2012-2016\nGraduate student on the topic of physiological arousal in meaning maintenance at Tilburg University\n\n\n2011-2012\nStudent assistant during my Research Master: Behavioral Science at Nijmegen University\n\n\n2012-2013\nProgrammer of experimental psychology tasks for FrieslandCampina\n\n\n2007-2010\nMedia analyst for Report International"
  },
  {
    "objectID": "content/cv/cv.html#education",
    "href": "content/cv/cv.html#education",
    "title": "dr. Willem Sleegers",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\n\n\n2012-2016\nGraduate student at Tilburg University supervised by prof. dr. Ilja van Beest and dr. Travis Proulx\n\n\n2012-2016\nStudent member of the Kurt Lewin Institute (KLI)\n\n\n2010-2012\nResearch Master Behavioural Science at Nijmegen University\n\n\n2010-2012\nExpert track in data-analysis\n\n\n2007-2010\nPsychology BSc. at Nijmegen University\n\n\n2007-2010\nHonours Program of Psychology at Nijmegen University"
  },
  {
    "objectID": "content/cv/cv.html#publications",
    "href": "content/cv/cv.html#publications",
    "title": "dr. Willem Sleegers",
    "section": "Publications",
    "text": "Publications\n\nIn preparation\n\n\n\n\n\n\n\n\nSleegers, W. W. A., Vaidis, D. (shared first author) et al. (in prep.). A multi-lab replication of the induced compliance paradigm of cognitive dissonance. Advances in Methods and Practices in Psychological Science. https://osf.io/52wpj\n\n\nJaeger, B., Sleegers, W. W. A., Stern, J., Penke, L., & Jones, A. (in prep.) Testing perceivers’ accuracy and accuracy awareness when forming personality impressions from faces\n\n\n\n\n\n\n\nPreprints\n\n\n\n\n\n\n\n\n\n2023\nSleegers, W., Moss, D., McAuliffe, W., Reinstein, D., & Waldhorn, D. R. (2023). Measuring attitudes towards wild animal welfare: The Wild Animal Welfare Scale. OSF Preprints. https://doi.org/10.31219/osf.io/qfz73\n\n\n\n\n\n\n\nStage-1 accepted manuscripts\n\n\n\n\n\n\n\n\n\n2021\nSleegers, W. W. A., Vaidis, D. (shared first author) et al. (2021). A multi-lab replication of the induced compliance paradigm of cognitive dissonance. Advances in Methods and Practices in Psychological Science. https://osf.io/52wpj\n\n\n\n\n\n\n\nPeer-reviewed journals\n\n\n\n\n\n\n\n\n\n2023\nRen, D., Wesselmann, E. D., Loh, W. W., van Beest, I., van Leeuwen, F., & Sleegers, W. W. A. (2023). Do cues of infectious disease shape people’s affective responses to social exclusion? Emotion, 23(4), 997–1010. https://doi.org/10.1037/emo0001157\n\n\n2023\nvan Leeuwen, F., Jaeger, B., Sleegers, W. W. A., & Petersen, M. B. (2023) Do experimental manipulations of pathogen avoidance motivations influence conformity? Personality & Social Psychology Bulletin. https://doi.org/10.1177/01461672231160655\n\n\n2023\nJaeger, B., & Sleegers, W. W. A. (2023). Racial disparities in the sharing economy: Evidence from more than 100,000 Airbnb hosts across 14 countries. Journal of the Association for Consumer Research, 8(1), 33–46. https://doi.org/10.1086/722700\n\n\n2022\nStavrova, O., Evans, A. M., Sleegers, W. W. A., & van Beest, I. (2022) Examining the accuracy of lay beliefs about the effects of personality on prosocial behavior. Journal of Behavioral Decision Making, 1-19. https://doi.org/10.1002/bdm.2282\n\n\n2022\nBreznau, N., Rinke, E. M., Wuttke, A., Nguyen, H. H. V., Adem, M., Adriaans, J., Alvarez-Benjumea, A., Andersen, H. K., Auer, D., Azevedo, F., Bahnsen, O., Balzer, D., Bauer, G., Bauer, P. C., Baumann, M., Baute, S., Benoit, V., Bernauer, J., Berning, C., Berthold, A., … Żółtak, T. (2022). Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty. Proceedings of the National Academy of Sciences of the United States of America, 119(44), e2203150119. https://doi.org/10.1073/pnas.2203150119\n\n\n2022\nHoogeveen, S., Sarafoglou, A., Aczel, B., Aditya, Y., Alayan, A. J., Allen, P. J., Altay, S., Alzahawi, S., Amir, Y., Anthony, F.-V., Kwame Appiah, O., Atkinson, Q. D., Baimel, A., Balkaya-Ince, M., Balsamo, M., Banker, S., Bartoš, F., Becerra, M., Beffara, B., … Wagenmakers, E.-J. (2022). A many-analysts approach to the relation between religiosity and well-being. Religion, Brain & Behavior, 1–47. https://doi.org/10.1080/2153599X.2022.2070255\n\n\n2021\nPronk, T. M., Bogaers, R. I., Verheijen, M. S., Sleegers, W. W. A. (2021). Pupil size predicts partner choices in online dating. Social Cognition.\n\n\n2021\nEvans, A. M., Kogler, C., & Sleegers, W. W. A. (2021). No effect of synchronicity in online social dilemma experiments: A registered report. Judgment and Decision Making, 16(4), pp. 823 - 843. https://doi.org/10.1017/S1930297500008007\n\n\n2021\nVan Osch, Y., & Sleegers, W. W. A. (2021). Replicating and reversing the group attractiveness effect: Relatively unattractive groups are perceived as less attractive than the average attractiveness of their members. Acta Psychologica, 217, 103331. https://.doi.org/10.1016/j.actpsy.2021.103331\n\n\n2021\nBrandt, M., Sleegers, W. W. A. (2021) Evaluating belief system networks as a theory of political belief system dynamics, 25(2), 159-185. https://doi.org/10.1177/1088868321993751\n\n\n2021\nJones, B. C., DeBruine, L. M., Flake, J. K., et al. (2021) To which world regions does the valence–dominance model of social perception apply? Nature Human Behavior, 5, 159–169. https://doi.org/10.1038/s41562-020-01007-2\n\n\n2020\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2020) Pupillometry and hindsight bias: Physiological arousal predicts compensatory behavior. Social Psychological and Personality Science. https://doi.org/10.1177/1948550620966153\n\n\n2020\nEvans, A., Sleegers, W. W. A., & Mlakar, Ž. (2020). Individual differences in receptivity to scientific bullshit. Judgment and Decision Making, 15(3), 401-412.\n\n\n2020\nJaeger, B., Sleegers, W. W. A., & Evans, A. M. (2020). Automated classification of demographics from face images: A tutorial and validation. Social and Personality Psychology Compass, 14(3). https://doi.org/10.1111/spc3.12520\n\n\n2019\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2019). Confirmation bias and misconceptions: Pupillometric evidence for a confirmation bias in misconceptions feedback. Biological Psychology, 145, 76–83. https://doi.org/10.1016/j.biopsycho.2019.03.018\n\n\n2019\nBender, M., van Osch, Y., Sleegers, W. W. A., & Ye, M. (2019). Social support benefits psychological adjustment of international students: Evidence from a meta-analysis. Journal of Cross-Cultural Psychology, 50(7), 827–847. https://doi.org/10.1177/0022022119861151\n\n\n2019\nVan ’t Veer, A. E., & Sleegers, W. W. A. (2019). Psychology data from an exploration of the effect of anticipatory stress on disgust vs. Non-disgust related moral judgments. Journal of Open Psychology Data, 7(1), 1. https://doi.org/10.5334/jopd.43\n\n\n2018\nJaeger, B., Sleegers, W. W. A., Evans, A. M., Stel, M., & van Beest, I. (2018). The effects of facial attractiveness and trustworthiness in online peer-to-peer markets. Journal of Economic Psychology. https://doi.org/https://doi.org/10.1016/j.joep.2018.11.004\n\n\n2017\nProulx, T., Sleegers, W. W. A., & Tritt, S. (2017). The expectancy bias: Expectancy-violating faces evoke earlier pupillary dilation than neutral or negative faces. Journal of Experimental Social Psychology, 70, 69-79. https://doi.org/10.1016/j.jesp.2016.12.003\n\n\n2017\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2017). The social pain of Cyberball: Decreased pupillary reactivity to exclusion cues. Journal of Experimental Social Psychology, 69, 187–200. https://doi.org/10.1016/j.jesp.2016.08.004\n\n\n2015\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2015). Extremism reduces conflict arousal and increases values affirmation in response to meaning violations. Biological Psychology, 108, 126–131. https://doi.org/10.1016/j.biopsycho.2015.03.012\n\n\n2015\nSleegers, W. W. A., & Proulx, T. (2015). The comfort of approach: Self-soothing effects of behavioral approach in response to meaning violations. Frontiers in Psychology, 5, 1–10. https://doi.org/10.3389/fpsyg.2014.01568\n\n\n\n\n\n\n\nBook chapters\n\n\n\n\n\n\n\n\n\n2019\nvan Beest, I., & Sleegers, W.W.A (2019). Physiostracism: A case for non-invasive measures of arousal in ostracism research. In S. C. Rudert, R. Greifeneder, & K. D. Williams (Eds.), Current directions in ostracism, social exclusion and rejectionresearch. Routledge. https://doi.org/10.4324/9781351255912\n\n\n\n\n\n\n\nDissertation\n\n\n\n\n\n\n\n\n\n2017\nSleegers, W. W. A. (2017). Meaning and pupillometry: The role of physiological arousal in meaning maintenance (Doctoral dissertation). Retrieved from https://pure.uvt.nl/portal/en/publications/meaning-and-pupillometry(20680e63-e785-43d0-a3ae-e97b26de5f05).html\n\n\n\n\n\n\n\nSoftware\n\n\n\n\n\n\n\n\n\n2020\nSleegers, W. W. A. (2020). tidystats: Save output of statistical tests (Version 0.5) [Computer software]. https://doi.org/10.5281/zenodo.4041859\n\n\n2020\nSleegers, W. W. A. (2020). tidystats (Version 1) [Computer software]. https://doi.org/10.5281/zenodo.4434634\n\n\n\n\n\n\n\nWebsites\n\n\n\n\n\n\n\n\nMy personal website where I blog about (some of) my research.https://www.willemsleegers.com\n\n\nThe tidystats website, a support website for my tidystats software.https://www.tidystats.io"
  },
  {
    "objectID": "content/cv/cv.html#presentations",
    "href": "content/cv/cv.html#presentations",
    "title": "dr. Willem Sleegers",
    "section": "Presentations",
    "text": "Presentations\n\nInvited talks\n\n\n\n\n\n\n\n\n\n2021\nSleegers, W. W. A. (2021, March). tidystats. Talk for a research group at the Ministry of Defence.\n\n\n2021\nSleegers, W. W. A. (2021, February). tidystats. Talk for the BSI at Nijmegen University.\n\n\n2021\nSleegers, W. W. A. (2021, February). Cognitive dissonance RRR. Lab meeting at Cardiff University.\n\n\n2018\nSleegers, W. W. A. (2018, December). Pupillometry and psychology. Colloquium presentation for the Laboratoire de Psychologie Sociale department at Paris Descartes University.\n\n\n2018\nSleegers, W. W. A. (2018, October) tidystats. Colloquium presentation for the Methodology and Statistics department at Leiden University.\n\n\n2018\nSleegers, W. W. A. (2018, March) tidystats. Colloquium presentation for the MTO department at Tilburg University.\n\n\n2017\nSleegers, W. W. A. (2017, December). Meaning and pupillometry: The role of physiological arousal in meaning maintenance. Presentation at the ASPO conference as part of receiving the best ASPO dissertation award.\n\n\n2017\nSleegers, W. W. A. (2017, March). Pupillometry and psychological processes. Colloquium presentation at Cardiff University.\n\n\n2015\nSleegers, W. W. A., Proulx, T. & Van Beest, I. (2015, October). Capturing the physiological response to meaning violations: An eye tracker approach. Colloquium presentation at Tilburg University.\n\n\n\n\n\n\n\nConference presentations\n\n\n\n\n\n\n\n\n\n2023\nSleegers, W. W. A. (2023, June). The Wild Animal Welfare scale. Talk at the Animal Advocacy Conference, Canterbury, the UK.\n\n\n2019\nSleegers, W. W. A. (2019, July) tidystats. Lightning talk at the SIPS conference, Rotterdam, the Netherlands.\n\n\n2019\nSleegers, W. W. A. & Jaeger, B. (2019, December) The Social Cost of Correcting Others. Talk at the ASPO conference, Wageningen, the Netherlands.\n\n\n2018\nSleegers, W. W. A. (2018, June) tidystats. Lightning talk at the SIPS conference, Grand Rapids, MI.\n\n\n2017\nSleegers, W. W. A. (2017, August). oTree for social scientists. Presentation at the TIBER conference, Tilburg, the Netherlands.\n\n\n2016\nSleegers, W. W. A., Proulx, T. & Van Beest (2016, December). Evidence of aversive arousal motivating compensatory behavior. Presentation at ASPO conference, Leiden, the Netherlands.\n\n\n2014\nProulx, T. & Sleegers, W. W. A. (2014, May). Meaning Maintenance Model: Towards a unified account of threat-compensation behaviors. Presentation at KLI conference, Zeist, the Netherlands.\n\n\n2014\nSleegers, W. W. A., Proulx, T., & Van Beest (2014, December). Cyberball and eye tracking: Support for the numbing hypothesis of social exclusion. Presentation at ASPO 2014, Groningen, the Netherlands.\n\n\n2014\nSleegers, W. W. A., Proulx, T., & Van Beest (2014, July). Ostracism and eye tracking. Presentation at EASP preconference on threat regulation, Amsterdam, the Netherlands.\n\n\n\n\n\n\n\nSmall meetings\n\n\n\n\n\n\n\n\n\n2019\nSleegers, W. W. A. (2019, November) Addressing Incorrect and Incomplete Statistics Reporting (with tidystats). Talk at the meta-research day at Tilburg University, the Netherlands.\n\n\n\n\n\n\n\nPoster presentations\n\n\n\n\n\n\n\n\n\n2017\nSleegers, W. W. A. (2017, July). Pupillometry and psychology: Pupillometry as an experimental tool for psychologists. Poster session presented at the Ostracism, Social Exclusion, and Rejection conference, Vitznau, Switzerland.\n\n\n2016\nSleegers, W. W. A., Proulx, T., & Van Beest (2016, January). Ostracism and eye tracking: Decreased pupillary reactivity to exclusion cues. Poster session presented at the SPSP conference, San Diego.\n\n\n2015\nSleegers, W. W. A., Proulx, T., & Van Beest (2015, December). Meaning and misconceptions: The effect of error feedback and commitment towards misconceptions on pupil size. Poster session presented at ASPO conference, Amsterdam.\n\n\n2015\nSleegers, W. W. A., Proulx, T. & Van Beest (2015, March). Cyberball and eye tracking: Support for the numbing hypothesis of social exclusion. Poster session presented at ICPS conference, Amsterdam, the Netherlands.\n\n\n2014\nSleegers, W. W. A., Proulx, T, & Van Beest, I. (2014, May). Extremism and the response to meaning threats: Extremism reduces pupillary response to threat and increases affirmation of values. Poster session presented at the KLI conference, Zeist, the Netherlands.\n\n\n\n\n\n\n\nValorization presentations\n\n\n\n\n\n\n\n\n\n2017\nSleegers, W. W. A., & Wagemans, F. M. A. (2017, November). The psychology behind eye tracking. Presentation organized by the Academic Forum of Tilburg University.\n\n\n2017\nWagemans, F. M. A., & Sleegers, W. W. A. (2017, June). Waar walg jij van? Presentation at Mundial festival on attentional processes and disgust sensitivity using eye tracking."
  },
  {
    "objectID": "content/cv/cv.html#journals",
    "href": "content/cv/cv.html#journals",
    "title": "dr. Willem Sleegers",
    "section": "Journals",
    "text": "Journals\nI reviewed for Behavioural Processes, Biological Psychology, British Journal of Psychology, British Journal of Social Psychology, Collabra, European Journal of Social Psychology, Group Processes & Intergroup Relations, International Journal of Psychology, International Review of Social Psychology, Journal of Consumer Behaviour, Journal of Experimental Social Psychology, Journal of Social and Personal Relationships, Personality and Social Psychology Bulletin, PLOS ONE, Self and Identity, Social Cognition, Social Influence, Social Psychology, Current Psychology, Journal of Cognitive Psychology.\nI’m a consulting editor for the Psychology of Human-Animal Intergroup Relations (PHAIR) journal."
  },
  {
    "objectID": "content/cv/cv.html#teaching",
    "href": "content/cv/cv.html#teaching",
    "title": "dr. Willem Sleegers",
    "section": "Teaching",
    "text": "Teaching\n\nCourses\n\n\n\n\n\n\n\n\n\n2017-2021\nSocial Psychology\n\n\n2016-2021\nAttitudes and Advertising\n\n\n2019-2021\nUnderstanding Data with R\n\n\n2015/2017/2019\nResearch Master: Experimental Research and Meta-Analysis\n\n\n2016-2021\nCourse in R software\n\n\n\n\n\n\n\nSeminars\n\n\n\n\n\n2012-2017\nSocial Psychology\n\n\n2015-2016\nIntroduction and History of Psychology\n\n\n2014-2015\nCultural Psychology\n\n\n2013-2015\nAcademic Skills\n\n\n2012-2013\nGroup Skills\n\n\n\n\n\n\n\nIndividual lectures\n\n\n\n\n\n\n\n\n\n2016\nSocial Psychology\n\n\n2014\nIntroduction and History of Psychology on intrapersonal conflict\n\n\n2013\nIntroduction to Social Psychology for prospective students\n\n\n\n\n\n\n\nSupervision\n\n\n\n\n\n2021\nResearch Master in Psychology theses\n\n\n2016-2021\nMaster in Psychology theses\n\n\n2013-2021\nBachelor in Psychology theses\n\n\n2012-2018\nResearch Skills in Psychology\n\n\n\n\n\n\n\nCoordination\n\n\n\n\n\n2014-2021\nSocial Psychology\n\n\n2016-2021\nAttitudes and Advertising\n\n\n2019-2021\nUnderstanding Data with R\n\n\n\n\n\n\n\nOther\n\n\n\n\n\n\n\n\n\n2013-2021\nAn introduction to R; part of the Kurt Lewin Institute course program"
  },
  {
    "objectID": "content/cv/cv.html#technical-skills",
    "href": "content/cv/cv.html#technical-skills",
    "title": "dr. Willem Sleegers",
    "section": "Technical skills",
    "text": "Technical skills\n\nStatistics\n\n\n\n\n\nR: A free software environment for statistical computing and graphics\n\n\nSPSS: A proprietary data analysis program\n\n\n\n\n\n\n\nProgramming\n\n\n\n\n\nQuarto: An open-source scientific and technical publishing system\n\n\nHTML: Markup language for creating web pages and web applications\n\n\nCSS: Markup language for styling web pages and web applications\n\n\nJavaScript: A programming language for creating web applications\n\n\nReact: A JavaScript library for building user interfaces\n\n\nPython: A cross-platform procedural programming language\n\n\n\n\n\n\n\nExperimental design\n\n\n\n\n\n\n\n\nMillisecond’s Inquisit: Stimulus delivery and experimental design software\n\n\noTree: Framework based on Python and Django to create standard and interactive online psychological experiments\n\n\nTobii Studio and Tobii Studio Extensions for E-prime: software to run eye tracker experiments using Tobii eye trackers\n\n\nPsychology Software Tool’s E-Prime: Stimulus delivery and experimental design software\n\n\nAdobe’s Authorware: Stimulus delivery and experimental design software. This has been discontinued, please do not make me use it\n\n\nNeurobehavioural Systems’ Presentation®: A stimulus delivery and experimental control program for neuroscience"
  },
  {
    "objectID": "content/projects.html",
    "href": "content/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Animal welfare\n\n\nFarm animals and wild animals suffer in horrible ways in great numbers. At Rethink Priorities, I contribute to various projects aimed at addressing this important problem.\n\n\n\n\n\n\n\n\n\n\nCognitive dissonance\n\n\nIn this project I aim to assess the evidence for cognitive dissonance theory using a large-scaled replication study of a seminal cognitive dissonance study.\n\n\n\n\n\n\n\n\n\n\nstatcheck\n\n\nTogether with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer statistics-related typos.\n\n\n\n\n\n\n\n\n\n\ntidystats\n\n\ntidystats refers to a collection of software solutions to improve how statistics are reported and shared in the field of (social) psychology.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html",
    "href": "content/projects/tidystats/tidystats.html",
    "title": "tidystats",
    "section": "",
    "text": "tidystats is a project centered around creating software to improve how statistics are reported and shared in the field of (social) psychology."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#why-is-this-important",
    "href": "content/projects/tidystats/tidystats.html#why-is-this-important",
    "title": "tidystats",
    "section": "Why is this important?",
    "text": "Why is this important?\nWith this project, I hope to address two problems in statistics reporting: Incorrect and incomplete statistics reporting.\nStatistics are often reported incorrectly (Nuijten et al., 2016). I think this is because researchers do not have the necessary software tools to reliably take the output of statistics from their data analysis software and enter it into their text editor. Instead, researchers are likely to copy statistics from the output by hand or by copy-pasting the output. Both techniques are error-prone, resulting in many papers containing statistical typos. This is a problem because statistical output is used in meta-analyses to aggregate the statistical evidence for theories, which in turn may affect policy. In some cases, the errors may even be so large that it affects the conclusion drawn from the statistical test.\nThere is also a more fundamental issue. Researchers usually only report the statistics in their manuscript and nowhere else. As a result, researchers face trade-offs between reporting all statistics, writing a legible text, and journal guidelines. Reporting all statistics makes results sections difficult (and boring) to read and it also takes up valuable space. Consequently, researchers are likely to only report the statistics that they deem to be relevant, rather than reporting all the statistics. While this is fine for someone who wants to simply read the paper and get the main takeaway, this is not desirable from a cumulative science perspective. All statistics should be easily available so they can be build on in future research."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#what-am-i-working-on",
    "href": "content/projects/tidystats/tidystats.html#what-am-i-working-on",
    "title": "tidystats",
    "section": "What am I working on?",
    "text": "What am I working on?\nI have designed an R package called tidystats that enables researchers to export the statistics from their analyses into a single file. It works by adding your analyses to a list (in R) and then exporting this list to a JSON file. This file will contain all the statistics from the analyses, in an organized format, ready to be used in other software.\nBy storing the output of statistical tests into a separate file, rather than only in one’s manuscript, the researcher no longer needs to worry about which analyses to report in the space-limited manuscript. They can simply share the file together with the manuscript, on OSF or as supplemental material.\nAn additional benefit is that because JSON files are easy to read for computers, it is (relatively) easy to write software that does cool things with these files.\nAn example of software that can read the JSON file is the tidystats Microsoft Word add-in. This add-in can be installed via the Add-in Store from inside Word. With this add-in, researchers can upload the JSON file to reveal a user-friendly list of their analyses. Clicking on an analysis reveals the associated statistics and clicking on a statistic inserts it into the document. This add-in also comes with several time-saving features, such as inserting multiple statistics at once (immediately in APA style) and automatic updating of statistics.\nRecently, I’ve also obtained a grant to work on tidystats. Thanks to this grant, the functionality of tidystats will be expanded, both in terms of adding support for more analyses and by expanding to other platforms, such as Python and Google Docs.\nBesides working on the software itself, I also spent some time on making it accessible. I have given talks introducing tidystats and I’ve created a website to help people become familiar with tidystats."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#links",
    "href": "content/projects/tidystats/tidystats.html#links",
    "title": "tidystats",
    "section": "Links",
    "text": "Links\n\nThe tidystats website\nR package on CRAN\nR package GitHub repository\nThe tidystats Word add-in in AppSource (the Office add-in store)\nWord add-in GitHub repository\nA blog post describing an example of using tidystats"
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html",
    "href": "content/projects/animal-welfare/animal-welfare.html",
    "title": "Animal welfare",
    "section": "",
    "text": "Now that I work at Rethink Priorities I get to devote a significant chunk of my time on projects related to animal welfare. I’ve only recently joined RP, though, so there is not much yet to show, but below I explain why I want to focus on this more and some steps I’ve taken so far."
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html#why-is-this-important",
    "href": "content/projects/animal-welfare/animal-welfare.html#why-is-this-important",
    "title": "Animal welfare",
    "section": "Why is this important?",
    "text": "Why is this important?\n\n“It may come one day to be recognized, that the number of legs, the villosity of the skin, or the termination of the os sacrum, are reasons equally insufficient for abandoning a sensitive being to the same fate. What else is it that should trace the insuperable line? Is it the faculty of reason, or perhaps, the faculty for discourse?…the question is not, Can they reason? nor, Can they talk? but, Can they suffer?\n\nThis (partial) quote by Jeremy Bentham gets to the heart of the matter. Whether something deserves moral concern is predominantly (if not only) a function of whether that something is capable of suffering. Cheating on a partner is bad not because cheating is inherently bad, but because it likely causes great suffering on the cheated-on partner. If your partner doesn’t care about being cheated on (i.e., being in an open relationship), cheating is no longer a bad thing. This shows morality is about the consequences of one’s actions and whether those consequences cause suffering.\nMany animals can suffer. It is unclear and impossible with our current knowledge about consciousness to assess which animals are capable of suffering, but we know enough to confidently say that some animals can suffer. Large mammals such as cows, sheep, goats, and horses can undoubtedly suffer. Smaller creatures such as chicken and turkeys are similarly unlucky and likely also capable. It is less clear when it comes to fish, but I would put my money them being able to suffer rather than being experience-less creatures.\nThe examples of animals I used above are the kinds of animals we farm. These are the kinds of animals we treat in ways that cause them to suffer, with great intensity and in great numbers. Chicken, for example, live in crowded spaces that cause in-fighting, the spread of diseases, and deaths due to, for instance, pile ups. They are artificially selected to grow at unhealthily fast rates, causing physical abnormalities. They are prevented from displaying their instinctive behaviors, such as establishing pecking orders, dust bathing, building nests, and spreading their wings. Sometimes farmers address these problems, although not always in the animal’s best interest. Injuries due to in-fighting is reduced by cutting or burning off the beaks, thus preventing them from harming each other. Other farm animals face similar situations.\nWhat makes it worse is the scale of factory farming. In the Netherlands alone, over 600 million land animals were killed in 2019. And that’s just in the Netherlands, a pretty tiny country. In the U.S., 9.76 billion land animals were killed in 2020. These numbers are so big they almost lose their meaning. The reality is, however, that factory farming causing suffering in billions and billions of individual animals, every year.\nPeople might retort that killing animals for food is simply the natural order of things. This argument is easy to refute: The natural order also sucks. We should not look at nature to determine what is good or bad (this is called the naturalistic fallacy). In nature, all kinds of suffering takes place. Animals (including humans) die due to various causes including disease, disasters, predation, starvation, and so on. These things are normal in nature. As humans, we have done our best to remove all these natural threats from our lives because that reduces our suffering. If we want to be natural, we should invite all these threats back into our lives. Of course, that’s not what we want to do, because we don’t want to suffer.\nI think we should extent that courtesy also to wild animals. We have succeeded in making our lives a lot better, while ignoring the same problems in other species. If we care about the lives of conscious creatures (such as our fellow humans, our pets, our zoo animals), we should also care about the lives of wild animals.\nIn short, farm animals suffer in horrible ways in great numbers, and the same happens in nature (although perhaps less efficiently than in factory farms). Given that suffering is the main reason to care about something, this logically means that we should figure out ways to alleviate their suffering. I hope to contribute to this."
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html#what-am-i-working-on",
    "href": "content/projects/animal-welfare/animal-welfare.html#what-am-i-working-on",
    "title": "Animal welfare",
    "section": "What am I working on?",
    "text": "What am I working on?\nAt Rethink Priorities I’m working on the development of a scale to assess people’s attitudes toward wild animal suffering. We aim to publish the results of this project in an academic journal with the goal for many other academics to begin studying the topic of wild animal suffering.\nI have also joined the following groups:\n\nSociety for the Psychology of Human-Animal Intergroup Relations (PHAIR)\nResearch to End Consumption of Animal Products (RECAP)\n\nBy joining these groups I hope to learn more about current research directions and to join existing projects. Eventually I also hope to use these platforms to share my own work.\nI’ve joined a project by Mercy for Animals on a multi-country survey to develop insights on people’s knowledge, attitudes, behavioral intentions and behaviors regarding farmed animal issues and key advocacy activities.\nI’ve started my own project that is a meta-analysis on meat intervention studies. There have been several meta-analyses on this topic (see here for a recent one), but I think I can contribute in some unique ways by creating a ‘live meta-analysis’ that can continuously be updated with new studies."
  },
  {
    "objectID": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html",
    "href": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html",
    "title": "Bayesian tutorial: Single predictor regression",
    "section": "",
    "text": "In my previous blog post I showed how to use brms and tidybayes to run an intercept-only model. Now let’s extend that model by adding a predictor.\nThe data is the same as in the previous post (including the filter that we only focus on people 18 years or older). This data contains weight data as well as height data, so that means we can run a model in which we regress heights onto weights, i.e., a regression with a single predictor.\nIf you want to follow along, run the following setup code.\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(modelr)\n\ntheme_set(theme_minimal())\ncolors &lt;- c(\"#d1e1ec\", \"#b3cde0\", \"#6497b1\", \"#005b96\", \"#03396c\", \"#011f4b\")\n\noptions(\n  mc.cores = 4,\n  brms.threads = 4,\n  brms.backend = \"cmdstanr\",\n  brms.file_refit = \"on_change\"\n)\n\ndata &lt;- read_csv(\"Howell1.csv\")\ndata &lt;- filter(data, age &gt;= 18)"
  },
  {
    "objectID": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#adding-a-single-predictor",
    "href": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#adding-a-single-predictor",
    "title": "Bayesian tutorial: Single predictor regression",
    "section": "Adding a single predictor",
    "text": "Adding a single predictor\nThe formula syntax for a model in which we regress heights onto weights is height ~ weight. We can use this formula in get_prior() to see which priors we need to specify.\n\n\nCode\nget_prior(height ~ weight, data = data)\n\n\n                    prior     class   coef group resp dpar nlpar lb ub\n                   (flat)         b                                   \n                   (flat)         b weight                            \n student_t(3, 154.3, 8.5) Intercept                                   \n     student_t(3, 0, 8.5)     sigma                               0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nThe output is a bit trickier compared to the intercept-only model output. There’s the Intercept and sigma priors again, as well as two extra rows referring to a class called b. These two rows actually refer to the same prior, one refers specifically to the weight predictor and one refers to all predictors. If you run a model with many more predictors, you could set one prior that applies to all predictors. In this case though, we only have 1 predictor so it actually doesn’t matter, both refer to the same prior.\nRecall from the previous post that I said writing down your model explicitly is a better way to understand what you’re doing, so let’s go ahead and do that.\n\\[\\displaylines{heights_i ∼ Normal(\\mu_i, \\sigma) \\\\ \\mu_i = \\alpha + \\beta x_i}\\]\nWe again specify that the heights are normally distributed, so we still have a \\(\\mu\\) and \\(\\sigma\\), but this time the \\(\\mu\\) is no longer a parameter to estimate. Instead, it’s constructed from other parameters, \\(\\alpha\\), \\(\\beta\\), and an observed variable \\(x_i\\) (the weight observations).\nIf you’re used to linear regression equations, this notation should not surprise you. \\(\\alpha\\) refers to the intercept and \\(\\beta\\) to the slope.\nWe need to set priors on these parameters. The prior for \\(\\alpha\\) can be the same as the prior for \\(\\mu\\) from the previous intercept-only model if we center the data so the intercept refers to the average height of someone with an average weight, rather than someone with 0 weight (the default, which makes no sense). So let’s first mean center the weight observations.\n\n\nCode\ndata &lt;- mutate(data, weight_mc = weight - mean(weight))\n\n\nNow we can use the same prior as before, which was a normal distribution with a mean of 160 and a standard deviation of 10 (assuming we did not update this as a result of the previous analysis).\nNext is the prior for the slope. This represents the relationship between weights and heights. For every 1 increase in weight, how much do we think that the height will increase or decrease? We could begin with an agnostic prior in which we do not specify the direction and instead just add some uncertainty so the slope can go in either direction. For example, let’s put a normal distribution on the slope with a mean of 0 and a standard deviation of 10.\nFinally, we have the prior for sigma (\\(\\sigma\\)). To remind you, sigma refers to the standard deviation of the errors or the residual standard deviation. Now that we have a predictor that means the sigma can be less than what it was in the intercept-only model because some of the variance in heights might be explained by the weights, decreasing the size of the residuals and therefore sigma. So, if we believe in a relationship between heights and weights, we should change our prior for sigma so that it’s lower. Given that we used a prior for the slope that is agnostic (there could be a positive, negative, or no relationship), our prior for sigma could be left unchanged because it was broad enough to allow for these possibilities."
  },
  {
    "objectID": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#prior-predictive-check",
    "href": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#prior-predictive-check",
    "title": "Bayesian tutorial: Single predictor regression",
    "section": "Prior predictive check",
    "text": "Prior predictive check\nWe can again create a prior predictive check to see whether our priors actually make sense. However, instead of plotting the predicted distribution of heights, we’re mostly interested in the relationship between weight and height, so we should plot a check of that relationship instead. We could simulate our own data like I did in the previous post or we can just run the Bayesian model and only draw from the prior, which I also did in the previous post and will do so again here.\n\n\nCode\nmodel_height_weight_prior &lt;- brm(\n  height ~ weight_mc,\n  data = data,\n  family = gaussian,\n  prior = c(\n    prior(normal(160, 10), class = \"Intercept\"),\n    prior(cauchy(5, 5), class = \"sigma\"),\n    prior(normal(0, 10), class = \"b\")\n  ),\n  sample_prior = \"only\",\n  seed = 4,\n  silent = 2,\n  file = \"models/model_height_weight_prior.rds\"\n)\n\n\nWe can use the spread_draws() function to get draws from the posterior distribution of the intercept and slope parameters. With an intercept and slope we can visualize the relationship we’re interested in. Remember, though, that brms will give you 4000 draws by default from the posteriors. In other words, you get 4000 intercepts and slopes. That’s a bit much to visualize, so let’s only draw 100 intercepts and slopes.\nTo help make sense of the sensibility of the slopes I’ve added the average weight to the weights so we’re back on the normal scale and not the mean centered scale and I’ve added two dashed lines to indicate a very obvious minimum and a possible maximum height.\n\n\nCode\ndraws &lt;- spread_draws(\n  model_height_weight_prior,\n  b_Intercept, b_weight_mc,\n  ndraws = 100\n)\n\nweight_mean &lt;- data %&gt;%\n  pull(weight) %&gt;%\n  mean()\n\nggplot(data, aes(x = weight_mc, y = height)) +\n  geom_blank() +\n  geom_abline(\n    data = draws,\n    mapping = aes(intercept = b_Intercept, slope = b_weight_mc),\n    alpha = .25\n  ) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 272, linetype = \"dashed\") +\n  geom_label(x = 0, y = 260, label = \"Tallest person ever\") +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n\n\n\n\n\nA prior predictive check of the relationship between weight and height\n\n\n\n\nThe plot shows a wide range of possible slopes, some of which are definitely unlikely because they lead to heights that are smaller than 0 or higher than the tallest person who ever lived. We should lower our uncertainty by reducing the standard deviation on the prior. In the next model I lower it to 3.\nAdditionally, the negative slopes are actually also pretty unlikely because we should expect a positive relationship between weight and height (taller people tend to be heavier). We could therefore also change our prior to force it to be positive using the lb argument in our prior for b or use a distribution that doesn’t allow for any negative values. Let’s not do this though. Let’s assume we have no idea whether the relationship will be positive or negative and instead focus on the standard deviation instead so that we don’t obtain relationships we definitely know are unlikely.\n\n\nCode\nmodel_height_weight_prior_2 &lt;- brm(\n  height ~ weight_mc,\n  data = data,\n  family = gaussian,\n  prior = c(\n    prior(normal(160, 10), class = \"Intercept\"),\n    prior(cauchy(5, 5), class = \"sigma\"),\n    prior(normal(0, 3), class = \"b\")\n  ),\n  sample_prior = \"only\",\n  seed = 4,\n  silent = 2,\n  file = \"models/model_height_weight_prior_2.rds\"\n)\n\n\nLet’s inspect the lines again.\n\n\nCode\ndraws &lt;- spread_draws(\n  model_height_weight_prior_2, b_Intercept, b_weight_mc,\n  ndraws = 100\n)\n\nggplot(data, aes(x = weight_mc, y = height)) +\n  geom_blank() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 272, linetype = \"dashed\") +\n  geom_abline(\n    data = draws,\n    mapping = aes(intercept = b_Intercept, slope = b_weight_mc),\n    alpha = .25\n  ) +\n  geom_label(x = 0, y = 260, label = \"Tallest person ever\") +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n\n\n\n\n\nA prior predictive check of the relationship between weight and height\n\n\n\n\nThis looks a lot better, so let’s run the model for real now. sample_prior is now set to TRUE so we obtain samples of both prior and posterior distributions.\n\n\nCode\nmodel_height_weight &lt;- brm(\n  data = data,\n  height ~ weight_mc,\n  family = gaussian,\n  prior = c(\n    prior(normal(160, 10), class = \"Intercept\"),\n    prior(cauchy(5, 5), class = \"sigma\"),\n    prior(normal(0, 3), class = \"b\")\n  ),\n  sample_prior = TRUE,\n  seed = 4,\n  silent = 2,\n  file = \"models/model_height_weight.rds\"\n)\n\nmodel_height_weight\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ weight_mc \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.27   154.05   155.14 1.00     3891     2537\nweight_mc     0.90      0.04     0.82     0.99 1.00     3628     2881\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.10      0.20     4.72     5.53 1.00     4396     2544\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe estimate for the weight predictor is 0.9. For every increase of weight by 1 we can expect height to increase by this number. We can also be fairly confident in this kind of relationship because the lower and upper bound of the 95% CI ranges from 0.82 to 0.99.\nThese numbers are what we are usually interested in, but let’s also plot the the entire posterior for the slope estimate so can see the entire distribution and not just the summary results. Let’s also add the prior so we can see how much that changed as a result of the data. This time we use gather_draws() to create a long data frame, instead of a wide data frame that you get with spread_draws().\n\n\nCode\ndraws &lt;- model_height_weight |&gt;\n  gather_draws(prior_b, b_weight_mc) |&gt;\n  mutate(\n    distribution = if_else(\n      str_detect(.variable, \"prior\"), \"prior\", \"posterior\"\n    )\n  )\n\nggplot(draws, aes(x = .value, fill = fct_rev(distribution))) +\n  geom_histogram(binwidth = 0.05, position = \"identity\") +\n  labs(x = \"Slope\", y = \"\", fill = \"Distribution\") +\n  scale_fill_manual(values = c(colors[2], colors[5])) +\n  coord_cartesian(xlim = c(-5, 5))\n\n\n\n\n\n\n\n\n\nApparently our prior was still very uninformed because the posterior shows we can be confident in a much narrower range of slopes!\nLet’s also create another plot in which we plot the slope and its posterior against the observed data. The way to do this is by first creating a data frame containing weights that we want to predict the heights for. Below I use the data_grid() function from the modelr package to create this data frame. Specifically, I use it to create a data frame with a weight_mc column that has all unique (rounded) values of the mean-centered weight column.\nThen we use add_epred_draws() to predict the expected height for each mean centered weight. This is not a single value per weight. Instead, we get a distribution of possible heights for each weight value. We could plot all of these distributions, for example by creating a shaded region at each weight representing how likely the height is, or we can summarize that distribution of heights for each weight. The tidybayes package has the median_qi() function to summarize a distribution to a point and interval. By default it uses the median for the point summary and a 5% and 95% quartile range for the interval; the same summary we saw in the output from brm.\nFinally, I add a new column called weight with weight values back on the normal scale (not mean centered).\n\n\nCode\nslopes_qi &lt;- data_grid(data, weight_mc = round(weight_mc)) |&gt;\n  add_epred_draws(model_height_weight) |&gt;\n  median_qi() |&gt;\n  mutate(weight = weight_mc + weight_mean)\n\nggplot() +\n  geom_ribbon(\n    mapping = aes(ymin = .lower, ymax = .upper, x = weight),\n    data = slopes_qi,\n    alpha = .25\n  ) +\n  geom_line(\n    mapping = aes(x = weight, y = .epred),\n    data = slopes_qi\n  ) +\n  geom_point(\n    mapping = aes(x = weight, y = height),\n    data = data,\n    alpha = .25\n  ) +\n  labs(x = \"Weight\", y = \"Height\")\n\n\n\n\n\n\n\n\n\nThis graph is great because it shows us how confident we can be in the regression line. It does omit one source of uncertainty, though. The previous plot only shows the uncertainty about the regression line (the intercept and slope). We can also make a plot with predicted values of individual heights, which also incorporates the uncertainty from the \\(\\sigma\\) parameter. To get these values, we use add_predicted_draws().\n\n\nCode\npredicted_slopes_qi &lt;- data_grid(data, weight_mc = round(weight_mc)) |&gt;\n  add_predicted_draws(model_height_weight) |&gt;\n  median_qi() |&gt;\n  mutate(weight = weight_mc + weight_mean)\n\nggplot() +\n  geom_ribbon(\n    aes(ymin = .lower, ymax = .upper, x = weight),\n    data = predicted_slopes_qi,\n    alpha = .25\n  ) +\n  geom_line(\n    aes(x = weight, y = .prediction),\n    data = predicted_slopes_qi\n  ) +\n  geom_point(\n    aes(x = weight, y = height),\n    data = data,\n    alpha = .25\n  ) +\n  labs(x = \"Weight\", y = \"Height\")\n\n\n\n\n\n\n\n\n\nWhile this graph is pretty cool, I haven’t ever seen one in a social psychology paper, probably because academic psychologists are mostly interested in the parameters (e.g., means, correlations) rather than predicting individual observations."
  },
  {
    "objectID": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#summary",
    "href": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#summary",
    "title": "Bayesian tutorial: Single predictor regression",
    "section": "Summary",
    "text": "Summary\nIn this post I showed how to run a single predictor model in brms. The addition of a predictor meant that the previous intercept-only model had to be updated by turning the \\(\\mu\\) parameter into a regression equation. This then required an additional prior for the slope. To help set a prior on the slope, I created a prior predictive check of the slope. Running the model itself was straightforward and I provided several visualizations to help understand the results, including visualizing the posteriors of the slope parameter, the slope across the range of weights, and individual predicted heights.\nIn the next post I’ll show how to use brms to analyze correlations.\nThis post was last updated on 2024-03-19."
  },
  {
    "objectID": "content/posts/30-predicting-values-of-intercept-only-models/predicting-values-of-intercept-only-models.html",
    "href": "content/posts/30-predicting-values-of-intercept-only-models/predicting-values-of-intercept-only-models.html",
    "title": "Predicting values of intercept only models",
    "section": "",
    "text": "I frequently need to calculate a single proportion or single mean with confidence intervals. My preferred way for getting these is to run intercept-only models, such as a logistic regression for proportions and standard regression for means. In this post I show to run these models and obtain the estimates with confidence intervals, using the same workflow from my tutorial blog posts.\nRun the following setup code if you want to follow along.\nCode\nlibrary(tidyverse)\nlibrary(marginaleffects)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(modelr)\n\noptions(\n  mc.cores = 4,\n  brms.threads = 4,\n  brms.backend = \"cmdstanr\",\n  brms.file_refit = \"on_change\"\n)"
  },
  {
    "objectID": "content/posts/30-predicting-values-of-intercept-only-models/predicting-values-of-intercept-only-models.html#data",
    "href": "content/posts/30-predicting-values-of-intercept-only-models/predicting-values-of-intercept-only-models.html#data",
    "title": "Predicting values of intercept only models",
    "section": "Data",
    "text": "Data\nI found some survey data including a question about whether the respondent follows a vegan diet. The relevant column is called vegan and contains a 1 for ‘Yes’ and a 0 for “No”. With this data we can determine the proportion of vegans and the associated confidence interval.\n\n\nCode\ndata &lt;- read_csv(\"data.csv\")\nhead(data)\n\n\n\n\n\n\nid\ndate\nvegan\n\n\n\n\n800009\n02-07-2018\n0\n\n\n800015\n02-07-2018\n0\n\n\n800054\n02-07-2018\n0\n\n\n800057\n04-07-2018\n0\n\n\n800073\n03-07-2018\n0\n\n\n800085\n03-07-2018\n0"
  },
  {
    "objectID": "content/posts/30-predicting-values-of-intercept-only-models/predicting-values-of-intercept-only-models.html#frequentist",
    "href": "content/posts/30-predicting-values-of-intercept-only-models/predicting-values-of-intercept-only-models.html#frequentist",
    "title": "Predicting values of intercept only models",
    "section": "Frequentist",
    "text": "Frequentist\nMy frequentist approach consists of running a logistic regression using the glm() function. The outcome variable is the vegan column and there are no predictors; only an intercept.\n\n\nCode\nmodel &lt;- glm(vegan ~ 1, family = binomial(), data = data)\n\n\nWe can use the predictions() function from the marginaleffects package to obtain the proportion. By default, this function calculates the regression-adjusted predicted values for every observation in the original dataset. That’s not what we want; we want only one prediction. We can specify what we want to calculate predictors for using the newdata argument. My preferred way for specifying predictor values is using helper functions like datagrid(). With this function you can specify which predictors you want to include and for which values of each predictor you want to calculate predictions. The problem is that we don’t have any predictors, so what to specify? If we use the datagrid() function from marginaleffects, the answer is nothing.\n\n\nCode\npredictions(model, newdata = datagrid())\n\n\n\n\n\n\nrowid\nestimate\np.value\ns.value\nconf.low\nconf.high\nvegan\n\n\n\n\n1\n0.0120167\n0\n961.5991\n0.0095016\n0.0151874\n0.0120167\n\n\n\n\n\n\nThis gives us the estimate of interest, as well as a 95% confidence interval."
  },
  {
    "objectID": "content/posts/30-predicting-values-of-intercept-only-models/predicting-values-of-intercept-only-models.html#bayesian",
    "href": "content/posts/30-predicting-values-of-intercept-only-models/predicting-values-of-intercept-only-models.html#bayesian",
    "title": "Predicting values of intercept only models",
    "section": "Bayesian",
    "text": "Bayesian\nNow let’s do the same thing but using a Bayesian approach, without using the marginaleffects package. Below we run a model using the brm() function from brms.\n\n\nCode\nmodel &lt;- brm(\n  vegan ~ 1,\n  family = bernoulli(link = \"logit\"),\n  data = data,\n  prior = prior(student_t(5, 0, 1.5), class = \"Intercept\"),\n  file = \"models/model.rds\",\n  silent = 2\n)\n\n\nWarning: Rows containing NAs were excluded from the model.\n\n\nTo get predicted values, I’ll use the data_grid() fuction from the modelr package and the add_epred_draws() and median_qi() functions from the tidybayes package. The logic is to specify a data frame using data_grid() with predictor values and then add predicted values using add_epred_draws() to this data frame, which are then summarized using median_qi(). However, if we run the following code, we get an error.\n\n\nCode\ndata_grid() |&gt;\n  add_epred_draws(model) |&gt;\n  median_qi()\n\n\nError in `data_grid()`:\n! Must supply at least one of `...` and `.model`\n\n\nThat’s because the function data_grid() can’t be empty. Using datagrid() from the marginaleffects package also wouldn’t work.\nTo fix the error, we need to specify the model using the .model argument of the function.\n\n\nCode\ndata_grid(.model = model) |&gt;\n  add_epred_draws(model) |&gt;\n  median_qi()\n\n\n\n\n\n\n.row\n.epred\n.lower\n.upper\n.width\n.point\n.interval\n\n\n\n\n1\n0.0121273\n0.0095111\n0.0151626\n0.95\nmedian\nqi\n\n\n\n\n\n\nThat works.\nWe could also create the data frame ourselves without using data_grid().\n\n\nCode\ntibble(.rows = 1) |&gt;\n  add_epred_draws(model) |&gt;\n  median_qi()\n\n\n\n\n\n\n.row\n.epred\n.lower\n.upper\n.width\n.point\n.interval\n\n\n\n\n1\n0.0121273\n0.0095111\n0.0151626\n0.95\nmedian\nqi\n\n\n\n\n\n\nBut it looks weird to me to create a data frame with 1 row and no values in it (although technically that’s what the data grid functions also do)."
  },
  {
    "objectID": "content/posts/30-predicting-values-of-intercept-only-models/predicting-values-of-intercept-only-models.html#summary",
    "href": "content/posts/30-predicting-values-of-intercept-only-models/predicting-values-of-intercept-only-models.html#summary",
    "title": "Predicting values of intercept only models",
    "section": "Summary",
    "text": "Summary\nYou can run intercept-only regression models to obtain estimates of single proportions or means. These estimates, together with their confidence intervals, can be obtained using prediction functions and telling them to predict values from empty data frames, which can be created using helper functions like datagrid() from the marginaleffects package and data_grid() from the modelr package.\nThis post was last updated on 2024-03-18."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "",
    "text": "This post is the first of a series of tutorial posts on Bayesian statistics. I’m not an expert on this topic, so this tutorial is partly, if not mostly, a way for me to figure it out myself.\nThe goal will be to go through each step of the data analysis process and make things as intuitive and clear as possible. I’ll use the brms package to run the models and I will rely heavily on the book Statistical Rethinking by Richard McElreath.\nThe basic idea behind Bayesian statistics is that we start off with prior beliefs about the parameters in the model and then update those parameter beliefs using data. That means that for all models you need to figure out what your beliefs are before running any analyses. This is very different from frequentist statistics and probably the most off-putting part of running Bayesian analyses. However, my goal is to make this relatively easy by focusing on visualizing priors and how they change as a function of the Bayesian process. I’ll also try to come up with some methods to simplify the construction of priors, with the goal to have them be reasonable and non-controversial.\nIn some cases I might not even use a prior I personally believe in. Instead I’ll use a prior that represents a particular position or skepticism so that the results of the analysis can be used to change the mind of the skeptic, rather than me changing whatever I happen to believe.\nWith this in mind, let’s begin."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#setup",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#setup",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "Setup",
    "text": "Setup\nIn the code chunk below I show some setup code to get started, starting with the packages. After loading the packages I set the default ggplot2 theme and some colors. Finally, I set several brms specific options to speed up the functions and automatically re-run models if a model has changed. If a model has not changed, the results will be read from a file instead of re-running the model.\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(modelr)\n\ntheme_set(theme_minimal())\ncolors &lt;- c(\"#d1e1ec\", \"#b3cde0\", \"#6497b1\", \"#005b96\", \"#03396c\", \"#011f4b\")\n\noptions(\n  mc.cores = 4,\n  brms.threads = 4,\n  brms.backend = \"cmdstanr\",\n  brms.file_refit = \"on_change\"\n)"
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#data",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#data",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "Data",
    "text": "Data\nThe data I’ll use is the same data Richard McElreath uses in Chapter 4 of his amazing book Statistical Rethinking. The data consists of partial census data of the !Kung San, compiled from interviews conducted by Nancy Howell in the late 1960s. Just like in the book, I will focus only on people 18 years or older.\n\n\nCode\ndata &lt;- read_csv(\"Howell1.csv\")\ndata &lt;- filter(data, age &gt;= 18)\n\nhead(data)\n\n\n\n\nPartial census data for the Dobe area !Kung San compiled by Nancy Howell in the late 1960s.\n\n\nheight\nweight\nage\nmale\n\n\n\n\n151.765\n47.82561\n63\n1\n\n\n139.700\n36.48581\n63\n0\n\n\n136.525\n31.86484\n65\n0\n\n\n156.845\n53.04191\n41\n1\n\n\n145.415\n41.27687\n51\n0\n\n\n163.830\n62.99259\n35\n1"
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#an-intercept-only-model",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#an-intercept-only-model",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "An intercept-only model",
    "text": "An intercept-only model\nLet’s focus the first question on the heights in the data. What are the heights of the Dobe area !Kung San?\nThe way to address this question is by constructing a model in which heights are regressed on only the intercept, i.e., an intercept-only model. You may be familiar with the R formula for this type of model: height ~ 1.\nWith this formula and the data we can use brms to figure out which priors we need to set by running the get_prior() function. This is probably the easiest way to figure which priors you need when you’re just starting out using brms.\n\n\nCode\nget_prior(height ~ 1, data = data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\nstudent_t(3, 154.3, 8.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 8.5)\nsigma\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\nThe output shows us that we need to set two priors, one for the Intercept and one for sigma. brms already determined a default prior for each parameter (they are required for a Bayesian analysis), so we could immediately run an analysis if we want to but it is recommended to construct your own priors because you can often do better than the default priors.\nAlso, using the get_prior() function is not the best way to think about which priors we need. Using the function will give us the answer, but it doesn’t really improve our understanding of why these two priors are needed. In this case I also omitted an important specification of the heights, which is that they are normally distributed (the default in get_prior()). So let’s instead write down the model in a different way, which explicitly specifies how we think the heights are distributed and which parameters we need to set priors on. If we think the heights are normally distributed, we define our model like this:\n\\[heights_i ∼ Normal(\\mu, \\sigma)\\]\nWe explicitly note that the individual heights come from a normal distribution, which is determined by the parameters \\(\\mu\\) and \\(\\sigma\\). This then also immediately tells us that we need to set two priors, one on \\(\\mu\\) and one on \\(\\sigma\\).\nIn our intercept-only model, the \\(\\mu\\) parameter refers to our intercept and the \\(\\sigma\\) parameter refers to, well, sigma. Sigma is not often discussed in the literature I’m familiar with, but we’ll figure it out below. In fact, let’s discuss each of these parameters in turn and figure out what kind of prior makes sense."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#the-intercept-mu-prior",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#the-intercept-mu-prior",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "The intercept (\\(\\mu\\)) prior",
    "text": "The intercept (\\(\\mu\\)) prior\nThe prior for the intercept refers to, in this case, the average height of the !Kung San.\nbrms has set the default intercept prior as a Student t-distribution with 3 degrees of freedom, a mean of 154.3 and a standard deviation of 8.5. That means brms starts off with a ‘belief’ that the average of the heights is 154.3, but with quite some uncertainty reflected in the standard deviation of 8.5 and the fact that the distribution is a Student t-distribution. A Student t-distribution has thicker tails compared to a normal distribution, meaning that numbers in the tails of the distribution are more likely compared to a normal distribution, at least when the degrees of freedom are low. At higher degrees of freedom, the t-distribution becomes more and more like the normal distribution. So, the thicker tails of the t-distributions means smaller and taller average heights are relatively more plausible.\nBut this is the default prior. brms determines this prior by peeking at the data to create a weak prior that is easily updated by the data. We can do a bit better than the default prior, though.\nWhat do I believe the average height to be? As a Dutch person, I might think that the average height is around 175 centimeters. This is probably too tall to use as an average for the !Kung San because we’re known for being quite tall. So I think the average should be lower than 175, perhaps 170. I am not very sure, though. After all, I am far from an expert on people’s heights; I am only using my layman knowledge here. An average of 165 seems possible to me too. So let’s describe my belief in the form of a distribution in which multiple averages are possible, to varying extents. We could use a Student t-distribution with small degrees of freedom if we want to allow for the possibility of being very wrong (remember, it has thicker tails, so it assigns more probability to a wider range of average heights). We’re not super uncertain about people’s heights, though, so let’s use a normal distribution.\nAs we saw in defining our height model, a normal distribution requires that we set two parameters: the \\(\\mu\\) and the \\(\\sigma\\). The \\(\\mu\\) we already covered (i.e., 170), so that leaves \\(\\sigma\\). Let’s set this to 10 and see what happens by visualizing this prior. Below I plot both the default brms prior and our own with \\(\\mu\\) = 170 and \\(\\sigma\\) = 10.\n\n\nCode\nheight_prior_intercept &lt;- tibble(\n  height_mean = seq(from = 100, to = 250, by = 0.1),\n  mine = dnorm(height_mean, mean = 170, sd = 10),\n  default = dstudent_t(height_mean, df = 30, mu = 154.3, sigma = 8.5)\n)\n\nheight_prior_intercept &lt;- pivot_longer(\n  height_prior_intercept,\n  cols = -height_mean,\n  names_to = \"prior\"\n)\n\nggplot(\n  height_prior_intercept,\n  aes(x = height_mean, y = value, linetype = fct_rev(prior))\n) +\n  geom_line() +\n  labs(x = \"Average height\", y = \"\", linetype = \"Prior\") +\n  scale_x_continuous(breaks = seq(100, 250, 20))\n\n\n\n\n\nTwo priors for \\(\\mu\\)\n\n\n\n\nMy prior indicates that I believe the average height to be higher than the default prior. In terms of the standard deviation, we both seem to be about equally uncertain about this average. Looking at this graph I think this prior of mine is not very plausible. Apparently I assign quite a chunk of plausibility to an average of 180 cm, or even 190 cm, which is very unlikely. An average of 160 cm is more plausible to me than an average of 180, so I should probably lower the mu, or use more of a skewed distribution. This is one of the benefits of visualizing the prior, it can make you think again about your prior so that you may improve on it. Based on the graph, I will change the mean of my prior to 160. I can probably also lower the standard deviation, but I’ll leave it at 10 to show how easily the data will update this prior."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#the-sigma-sigma-prior",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#the-sigma-sigma-prior",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "The sigma (\\(\\sigma\\)) prior",
    "text": "The sigma (\\(\\sigma\\)) prior\nWhat about the sigma prior? What even is sigma? Sigma is the estimated standard deviation of the errors or, in other words, the standard deviation of the residuals of the model. In the simple case of an intercept-only model, this is identical to the standard deviation of the outcome (heights, in this case).\nI think setting the standard deviation of the distribution of heights (not the mean of the heights) is quite difficult. There are parts that are easy, such as the fact that the standard deviation has to be 0 or larger (it can’t be negative), but exactly how large it should be, I don’t know.\nI do know it is unlikely to be close to 0, and unlikely to be very large. That’s because I know people’s heights do vary, so I know the sigma can’t be 0. I also know it’s not super large because we don’t see people who are taller than 2 meters very often. This means the peak of our prior should be somewhere above 0, with a tail to allow higher values but not too high. We can use a normal distribution for this with a mean above 0 and a particular standard deviation, and ignore everything that’s smaller than 0 (brms automatically ignores negative values for \\(\\sigma\\)).\nAs I mentioned before, there is a downside of using a normal distribution, though. Normal distributions have long tails, but there is actually very little density in those tails. If we are quite uncertain about our belief about sigma, we should use a t-distribution, or perhaps even a cauchy distribution (actually, the cauchy distribution is a special case of the t-distribution; they are equivalent if the degree of freedom is 1). The lower the degrees of freedom, the more probability we assign to higher and lower values.\nA t-distribution requires three parameters: \\(\\mu\\), \\(\\sigma\\), and the degrees of freedom. I set \\(\\mu\\) to 5, \\(\\sigma\\) to 5, and the degrees of freedom to 1. Below I plot this prior and brms’s default prior to get a better grasp of these priors.\n\n\nCode\nheight_prior_sigma &lt;- tibble(\n  height_sigma = seq(from = 0, to = 50, by = .1),\n  default = dstudent_t(height_sigma, df = 3, mu = 0, sigma = 8.5),\n  mine = dstudent_t(height_sigma, df = 1, mu = 5, sigma = 5)\n)\n\nheight_prior_sigma &lt;- pivot_longer(\n  height_prior_sigma,\n  cols = -height_sigma,\n  names_to = \"prior\"\n)\n\nggplot(\n  height_prior_sigma,\n  aes(x = height_sigma, y = value, linetype = fct_rev(prior))\n) +\n  geom_line() +\n  labs(x = \"Standard deviation of heights\", y = \"\", linetype = \"Prior\")\n\n\n\n\n\nTwo priors for \\(\\sigma\\)\n\n\n\n\nAs you can see, both distributions have longish tails, allowing for the possibility of high standard deviations. There are some notable differences between the two priors, though. My prior puts more weight on a standard deviation larger than 0, while the default prior reflects a belief in which a standard deviation of 0 is most likely. However, both priors are quite weak."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#prior-predictive-check",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#prior-predictive-check",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "Prior predictive check",
    "text": "Prior predictive check\nSo far we have inspected each prior in isolation, but we can also use our priors to simulate heights and see if the distribution of heights makes sense. This is called a prior predictive check.\nWe can perform a prior predictive check using the brm() function from brms.The brm() function is the main work horse of the brms package. It enables us to run Bayesian analyses by using a common notation style familiar to those who use R. This is also one of the reasons why the brms package is so great; it’s very easy to get started with running Bayesian analyses.\nThe brm() function requires a model specification and the data. Optionally, but usefully, we should also specify the response distribution (a normal distribution by default) and the priors.\nHowever, we’re not ready to actually run the model just yet. Instead, we will kinda trick brms into running an analysis, but tell it to only sample from the prior using the sample_prior argument. This will give us ‘predicted’ responses based entirely on our priors and not the data.\nAdditionally, we also set a seed to make the results reproducible and a file to store the results into so that if we run the analysis again, we can simply read the results from the file rather than running the analysis again. I will also set the silent argument to 2 to hide some logs that aren’t useful.\n\n\nCode\nmodel_height_prior &lt;- brm(\n  height ~ 1,\n  data = data,\n  family = gaussian,\n  prior = c(\n    prior(normal(160, 10), class = \"Intercept\", lb = 0, ub = 250),\n    prior(cauchy(5, 5), class = \"sigma\")\n  ),\n  sample_prior = \"only\",\n  seed = 4,\n  silent = 2,\n  file = \"models/model_height_prior.rds\"\n)\n\n\nThe next part is a little tricky. The goal is to obtain a large sample of predicted heights so we can visualize its distribution. By default, brms will draw 4000 draws to approximate distributions. We could use the predict() function to get these draws (e.g., predict(model_height_prior), but I prefer to use the tidybayes package because it’s a really nice package that simplifies a lot of things about working with brms models.\nThe tidybayes function I’ll use is add_predicted_draws(). The function takes a data frame and a model object. The function adds predictions for each row in the data frame. This is kinda tricky because we only have an intercept-only model. If you have predictors in the model you could give the function a data frame with values for each predictor that you want to obtain predicted values for. We don’t have that, so we need to give it an empty data frame. To simplify this, we’ll use a function from the modelr package called data_grid(). This function can be used to construct a data frame with predictors from the model. In this case we don’t have any predictors, but we can still specify the model. It will then create an empty data frame for us that we can add predictions to. Note that this means the data frame starts off empty but we then add draws to the data frame, creating a larger data frame.\nAfter obtaining the draws, I plot the draws using a simple histogram.\n\n\nCode\nheights_prior &lt;- data_grid(.model = model_height_prior) |&gt;\n  add_predicted_draws(model_height_prior, value = \"height\")\n\nggplot(heights_prior, aes(x = height)) +\n  geom_histogram(binwidth = 1, fill = colors[3]) +\n  labs(x = \"Height\", y = \"\") +\n  coord_cartesian(xlim = c(50, 250))\n\n\n\n\n\nPrior predictive check\n\n\n\n\nOur priors result in a normal distribution of heights with the bulk of the observations ranging from about 120 cm to 205 cm. That seems fairly reasonable to me, as someone who doesn’t know too much about the heights of the !Kung San."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#running-the-model",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#running-the-model",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "Running the model",
    "text": "Running the model\nNow that the priors are in order we can run the model with the code below. Notice that this time I omit the sample_prior argument so we only obtain the posterior results.\n\n\nCode\nmodel_height &lt;- brm(\n  data = data,\n  family = gaussian,\n  height ~ 1,\n  prior = c(\n    prior(normal(160, 10), class = \"Intercept\"),\n    prior(cauchy(5, 5), class = \"sigma\")\n  ),\n  seed = 4,\n  silent = 2,\n  file = \"models/model_height.rds\"\n)\n\n\nAfter running the model, we first check whether the chains look like caterpillars because that indicates we have samples from the entire distribution space of the posteriors.\n\n\nCode\nplot(model_height)\n\n\n\n\n\n\n\n\n\nThe chains look good.\nWe can call up the estimates and the 95% confidence intervals by printing the model object. Do note that the confidence intervals don’t have the same meaning as frequentist confidence intervals. The intervals here simply indicate what the most likely values are (given the model, priors, and data). By default, the function returns 95% intervals, meaning that 95% of the draws are between the lower and upper bounds.\n\n\nCode\nsummary(model_height)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.61      0.41   153.83   155.39 1.00     3857     2972\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.77      0.29     7.24     8.36 1.00     3076     2558\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we see the Intercept and sigma estimates. Apparently our posterior estimate for the Intercept is 154.61 and the estimate for \\(\\sigma\\) is 7.77. We also see the 95% CIs, but let’s visualize these results instead.\nInspecting the chains also showed us the posterior distributions of the two parameters, but let’s create our own graphs that compare both the prior and posterior distributions. We can use the spread_draws() function from tidybayes to get draws from each parameter in the model. In the code below I do that twice, once to get the draws from our previous model that sampled from the prior only and once from our new model. The result for each is a data frame and I’ll add a column to indicate whether the draw is from the prior or posterior. If you don’t know what the parameters are called, you can use the get_variables() function to get a list of the names you can extract draws from.\n\n\nCode\ndraws_prior &lt;- model_height_prior |&gt;\n  spread_draws(b_Intercept, sigma) |&gt;\n  mutate(distribution = \"prior\")\n\ndraws_posterior &lt;- model_height |&gt;\n  spread_draws(b_Intercept, sigma) |&gt;\n  mutate(distribution = \"posterior\")\n\ndraws &lt;- bind_rows(draws_prior, draws_posterior) |&gt;\n  mutate(distribution = fct_relevel(distribution, \"prior\"))\n\nggplot(draws, aes(x = b_Intercept, fill = distribution)) +\n  geom_histogram(binwidth = 0.25, position = \"identity\") +\n  labs(\n    x = \"Intercept (i.e., average height)\",\n    y = \"\",\n    fill = \"Distribution\"\n  ) +\n  scale_fill_manual(values = c(colors[2], colors[5])) +\n  coord_cartesian(xlim = c(140, 190))\n\n\n\n\n\n\n\n\n\nHere we see that the posterior distribution of average heights is much more narrow and centered around 155 cm. So not only should we switch from thinking the average is lower than 160, we can also be much more confident about the mean.\nHow about sigma?\n\n\nCode\nggplot(draws, aes(x = sigma, fill = distribution)) +\n  geom_histogram(binwidth = 0.25, position = \"identity\") +\n  labs(\n    x = \"Sigma (i.e., height standard deviation)\",\n    y = \"\",\n    fill = \"Distribution\"\n  ) +\n  scale_fill_manual(values = c(colors[2], colors[5])) +\n  coord_cartesian(xlim = c(0, 25))\n\n\n\n\n\n\n\n\n\nSimilarly, we see that the posterior for sigma is also much more narrow and around 8.\nA final step is to conduct a posterior predictive check. Since we also conducted a prior predictive check we can plot both and compare how our overall beliefs about the distribution of heights should change as a function of the data. Below I create a new data frame with draws from the posterior, just like when I created the prior predictive check, and merge it with the prior data frame from before.\n\n\nCode\nheights_posterior &lt;- data_grid(.model = model_height) |&gt;\n  add_predicted_draws(model_height, value = \"height\") |&gt;\n  mutate(distribution = \"posterior\")\n\nheights_prior &lt;- mutate(heights_prior, distribution = \"prior\")\n\nheights &lt;- bind_rows(heights_prior, heights_posterior) |&gt;\n  mutate(distribution = fct_relevel(distribution, \"prior\"))\n\nggplot(heights, aes(x = height, fill = distribution)) +\n  geom_histogram(binwidth = 1, position = \"identity\") +\n  labs(x = \"Height\", y = \"\", fill = \"Distribution\") +\n  scale_fill_manual(values = c(colors[2], colors[5])) +\n  coord_cartesian(xlim = c(100, 250))\n\n\n\n\n\nPrior and posterior predictive check\n\n\n\n\nThis is one of my favorite plots. It shows how we started with a belief about heights and what our new belief should be, after seeing the data. That is the main goal of doing data analysis."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#summary",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#summary",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "Summary",
    "text": "Summary\nIn this post I showed how to run an intercept-only model in brms. It consisted of the following steps:\n\nDefine the model\nUse the model to figure out which priors to set\nVisualize the priors and create a prior predictive check to potentially tweak the priors (using brms)\nRun the model\nInspect the output, including the chains\nObtain draws of the estimates and visualize their distribution\nCompare the prior predictive check to the posterior results to see how much to update based on the data\n\nIn the next post I’ll show how to add a predictor to the model.\nThis post was last updated on 2024-03-19."
  },
  {
    "objectID": "content/posts/8-the-right-order-of-method-sections/the-right-order-of-method-sections.html",
    "href": "content/posts/8-the-right-order-of-method-sections/the-right-order-of-method-sections.html",
    "title": "The right order of method sections",
    "section": "",
    "text": "I think the proper order of Method sections is:\n\nDesign\nProcedure\nMaterials\nData Analysis\nParticipants\n\nTwo things are notable here. One, there’s a Data Analysis section. Two, the Participants section is all the way at the end. Here I anticipate that your reaction will be that this is crazy, because that’s not how we do things. But that’s not a good enough reason of course. We should be thinking about whether the order makes sense in terms of whether the content of each section logically follows from each other. For some sections you first need to know information from the other sections in order for your decisions to make sense. Putting the Participants all the way at the beginning doesn’t make sense, and the reason for that is the power analysis.\nNow that power analyses are getting more popular, psychologists have to try and make them fit in their Method section. But rather than thinking about what actually goes into a power analysis, and how to present that information to the reader, they generally stick to the format they’re used to. Or perhaps it’s because they misunderstand how a power analysis works, thinking that you only have 1 power analysis per study, so you should present it together with the Design of the study. Since the Design and Participants are sometimes combined, I can see how this might be the case. That still doesn’t make sense, though, and to understand that, we need to understand power analyses.\nWhat goes into a power analysis? A power analysis consists of setting a few parameters, such as the effect size, alpha, and beta. The alpha and beta parameters are pretty constant across different power analyses, but the effect size isn’t. The effect size depends on the exact analysis you want to power for. A t-test is usually done with a Cohen’s d in mind, while a correlation test is done with a correlation in mind. With more sophisticated analyses, such as repeated measures analyses, you need to set additional parameters (e.g., the correlation between repeated measures). This means that your power analysis is dependent on the exact analysis you will do. Actually, a power analysis is always about a specific analysis, so by that logic alone, you should first present which analysis you will do. Not only that, but you also need to power for all analyses you do, not just 1. In other words, a power analysis is something that is tied to a statistical test, and not to the design of a study (which would mean you only need 1 power analysis per study). The result is obvious: you first need to discuss the analyses you want to run before you can talk about power. This means you need a Data Analysis section in your Method section. Here you can elaborate on the analyses you will run, which analyses are the primary ones that you want to power for, and perhaps elaborate on some secondary or exploratory analyses that you won’t power for. You can also use this section to then present the power analysis. After that you get your needed sample size and you can start to explain how you obtained that sample size (i.e., the Participants section).\nWhat do you need to know in order to understand the Data Analysis section? That would be the Design and Materials. You need to know about the design to know whether it is, for example, a between-subjects design or a within-subjects design. You also need to know what the independent variables and dependent variables are. More specifically, you want to know how they are measured. How many levels are there in the independent variables? Is the outcome measure categorical or continuous? These are some of the properties of the measures that determine the appropriate analysis technique. This, in turn, means the Design section and the Materials section need to come before the Data Analysis section.\nPutting all of this together, I think it makes the most sense to begin with the Design, followed by the Procedure and Materials (possibly combined). This should be followed by a Data Analysis section that includes the analyses and associated power analysis (for all primary analyses, at least). Once these aspects are known, it makes sense to end, rather than start, with the Participants section. So the right order of Method sections is:\n\nDesign\nProcedure\nMaterials\nData Analysis\nParticipants"
  },
  {
    "objectID": "content/posts/35-metalog-distribution/metalog-distribution.html",
    "href": "content/posts/35-metalog-distribution/metalog-distribution.html",
    "title": "The metalog distribution",
    "section": "",
    "text": "I recently discovered the metalog distribution via MakeDistribution. According to Wikipedia, the metalog distribution “is a flexible continuous probability distribution designed for ease of use in practice”. A distribution that was designed to be easy to use peaked my interest, so in this blog post I try to figure out how to use it and determine whether it is indeed a useful and easy to use distribution.\nRun the setup code below in case you want to follow along.\nCode\nlibrary(tidyverse)\nlibrary(rmetalog)\nlibrary(magrittr)\nlibrary(ggdist)\n\ntheme_set(theme_minimal())\n\nmetalog_to_df &lt;- function(x) {\n  metalog |&gt;\n    magrittr::use_series(\"M\") |&gt;\n    tibble::as_tibble() |&gt;\n    tidyr::pivot_longer(\n      cols = -y,\n      names_to = c(\".value\", \"term\"),\n      names_pattern = \"(.)([1-9+])\"\n    ) |&gt;\n    dplyr::mutate(term = as.numeric(term)) |&gt;\n    dplyr::rename(\n      quantile_value = M,\n      pdf_value = m\n    )\n}\n\nformat_label &lt;- function(x, digits = 2) {\n  trimws(format(round(x, digits), nsmall = digits))\n}\n\nformat_interval &lt;- function(x, digits = 2) {\n  paste0(\n    format_label(x$y, digits = digits), \"\\n[\",\n    format_label(x$ymin, digits = digits), \"; \",\n    format_label(x$ymax, digits = digits), \"]\"\n  )\n}"
  },
  {
    "objectID": "content/posts/35-metalog-distribution/metalog-distribution.html#the-problem-with-distributions",
    "href": "content/posts/35-metalog-distribution/metalog-distribution.html#the-problem-with-distributions",
    "title": "The metalog distribution",
    "section": "The problem with distributions",
    "text": "The problem with distributions\nA common problem with working with distributions is that you need to know which distribution to use for which use-case and then shape the distribution to your liking by giving the distribution’s parameters the right values. In some cases this is fairly straightforward, like when you want to model something as a normal distribution with a certain mean and standard devation.\nBut what if you want to model, say, how long you think a particular task will take in hours? Here a normal distribution is not well suited and instead you have to use something else, perhaps a Gamma distribution or a lognormal distribution. These distributions take parameters other than a mean and standard deviation and they don’t have much intuitive meaning (at least not to me)."
  },
  {
    "objectID": "content/posts/35-metalog-distribution/metalog-distribution.html#using-the-metalog-distribution",
    "href": "content/posts/35-metalog-distribution/metalog-distribution.html#using-the-metalog-distribution",
    "title": "The metalog distribution",
    "section": "Using the metalog distribution",
    "text": "Using the metalog distribution\nThe metalog distribution changes all of that because you can simply specify the distribution by giving it a set of quantile-value pairs that are used to form the distribution.\nFor example, let’s specify three quantile-value pairs below and see how they shape the distribution.\nWe use the rmetalog package, which has a function called metalog() to create the distribution. We give it the values, the quantiles, and a term limit. The latter refers to how many terms are used in the distribution, with larger term distributions having more flexibility, meaning they better match the quantile-value pairs. The maximum number of terms is the number of quantile-value pairs we give it, so in the example below the maximum is 3.\n\n\nCode\nvalues &lt;- c(3, 6, 12)\nquantiles &lt;- c(0.10, 0.5, .9)\n\nmetalog &lt;- metalog(\n  x = values,\n  prob = quantiles,\n  term_limit = 3,\n)\n\nsummary(metalog)\n\n\n -----------------------------------------------\n Summary of Metalog Distribution Object\n -----------------------------------------------\n \nParameters\n Term Limit:  3 \n Term Lower Bound:  2 \n Boundedness:  u \n Bounds (only used based on boundedness):  0 1 \n Step Length for Distribution Summary:  0.01 \n Method Use for Fitting:  any \n Number of Data Points Used:  3 \n Original Data Saved:  FALSE \n \n\n Validation and Fit Method\n term valid         method\n    2   yes Linear Program\n    3   yes Linear Program\n\n\nUsing summary() on the output of metalog() gives us some information about the distribution, but it doesn’t seem particularly useful to me, so let’s move on to visualizing the distribution using the plot() function. Unfortunately, using plot() on the output actually returns two plots. One shows the probability density function (PDF) and the other the cumulative density function (CDF). I prefer looking at the PDF, so that’s the one I extract and plot below.\n\n\nCode\np &lt;- plot(metalog) |&gt;\n  use_series(\"pdf\")\n\np\n\n\n\n\n\n\n\n\n\nThis shows us two PDF plots. By default the metalog function creates multiple distributions with a different number of terms. In this case, we get one with two terms and one with three terms. The graph with three terms matches exactly the quantile-value pairs we specified but the one with two terms doesn’t, which I’ll show below by calculating the PDFs at specific quantile values manually using dmetalog() and qmetalog(), which are equivalents of functions like dnorm() and qnorm(). I also use some custom functions I loaded in the setup code to extract the metalog values (quantile values and PDF values) as a data frame and plot them using ggplot().\n\n\nCode\npoints &lt;- tibble(\n  quantile_value = values,\n  pdf_value_2 = dmetalog(metalog, quantile_value, term = 2),\n  pdf_value_3 = dmetalog(metalog, quantile_value, term = 3)\n)\n\nlines &lt;- tibble(\n  quantile_value_2 = qmetalog(metalog, quantiles, term = 2),\n  quantile_value_3 = qmetalog(metalog, quantiles, term = 3),\n  pdf_value_2 = dmetalog(metalog, quantile_value_2, term = 2),\n  pdf_value_3 = dmetalog(metalog, quantile_value_3, term = 3),\n)\n\npoints &lt;- points |&gt;\n  pivot_longer(\n    cols = -quantile_value,\n    names_to = \"term\",\n    values_to = \"pdf_value\",\n    names_transform = parse_number\n  )\n\nlines &lt;- lines |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = c(\".value\", \"term\"),\n    names_pattern = \"(.+_.+)_([1-9]+)\"\n  ) |&gt;\n  mutate(term = as.numeric(term))\n\ndf &lt;- metalog_to_df(metalog)\n\nggplot(df, aes(x = quantile_value, y = pdf_value)) +\n  facet_wrap(~term) +\n  geom_segment(data = lines, aes(yend = 0), linetype = \"dashed\") +\n  geom_line() +\n  geom_point(data = points) +\n  labs(x = \"Quantile value\", y = \"PDF value\")\n\n\n\n\n\n\n\n\n\nThe points show what the PDF values are at each of the quantiles we specified (0.10, 0.50, and 0.90). The lines show the correct quantile-value pairs (e.g., at quantile value 6 the total area of the curve before that value should be 50%). We see that the distribution with three terms perfectly matches our quantile-value pairs while the one with two terms doesn’t. The one with two terms has two correct quantile-value pairs (at 3 and 12), but one incorrect one (at 6). I suppose this means that the distribution with the most terms always matches the quantile-value pairs exactly but that the ones with fewer terms does not and instead creates a distribution with fewer quantile-value pairs matched.\nThis is cool, though. By simply specifying three quantile-value pairs, we obtained a distribution that matches those values exactly. This means it becomes much easier to specify a distribution to your liking."
  },
  {
    "objectID": "content/posts/35-metalog-distribution/metalog-distribution.html#boundedness",
    "href": "content/posts/35-metalog-distribution/metalog-distribution.html#boundedness",
    "title": "The metalog distribution",
    "section": "Boundedness",
    "text": "Boundedness\nThe metalog distribution also supports setting bounds on the distribution. This means you can specify what the minimum and/or maximum value should be. Below I modify our existing metalog distribution by specifying a lower bound of 0.\n\n\nCode\nmetalog &lt;- metalog(\n  x = values,\n  prob = quantiles,\n  term_limit = 3,\n  boundedness = \"sl\",\n  bound = 0\n)\n\ndf &lt;- metalog_to_df(metalog)\n\ndf |&gt;\n  filter(term == 3) |&gt;\n  ggplot(aes(x = quantile_value, y = pdf_value)) +\n  geom_line() +\n  labs(x = \"Quantile value\", y = \"PDF value\")\n\n\n\n\n\n\n\n\n\nThe distribution now starts at 0, perfect."
  },
  {
    "objectID": "content/posts/35-metalog-distribution/metalog-distribution.html#simulation",
    "href": "content/posts/35-metalog-distribution/metalog-distribution.html#simulation",
    "title": "The metalog distribution",
    "section": "Simulation",
    "text": "Simulation\nWith the distribution specified to our liking, we can simulate values from the distribution using the rmetalog() function. In the code below I simulate 1000 values from the distribution and turn the values into a data frame to plot them as a histogram. I also calculate the quantile values at 10%, 50%, and 90%, which should roughly match the values we specified previously (3, 6, and 12).\n\n\nCode\nhours &lt;- rmetalog(metalog, n = 1000, term = 3)\n\ndf &lt;- tibble(hour = hours)\n\nmedian_qi &lt;- median_qi(hours, .width = .8)\n\nggplot(df, aes(x = hour)) +\n  stat_histinterval(.width = .8) +\n  geom_label(\n    data = median_qi,\n    aes(label = format_interval(median_qi), x = y, y = 0),\n    size = 3,\n    vjust = 0, fill = \"transparent\",\n    label.size = 0, label.padding = unit(0.75, \"lines\")\n  ) +\n  labs(x = \"Hours\", y = \"\")\n\n\n\n\n\n\n\n\n\nExcellent, we now have a distribution that you could say represents the number of hours to complete a task. We could use this to model how long a project would take to complete by creating separate distributions for each task, sampling values from them, and then summing them together to get a view of how long it will take to complete the project."
  },
  {
    "objectID": "content/posts/35-metalog-distribution/metalog-distribution.html#summary",
    "href": "content/posts/35-metalog-distribution/metalog-distribution.html#summary",
    "title": "The metalog distribution",
    "section": "Summary",
    "text": "Summary\nThe metalog distribution makes it easy to specify a distribution using only quantile-value pairs. You can use the metalog() function from the the rmetalog package to specify the distribution, including whether the distribution is bounded or not, and then simulate values from this distribution using the rmetalog() function."
  },
  {
    "objectID": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html",
    "href": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html",
    "title": "Simulation-based power curves",
    "section": "",
    "text": "In a previous post I covered how to perform simulation-based power analyses. A limitation in that post is that usually the question is not what our power is, but rather which sample size gives us the desired power. With the code from my previous post you can get to the right sample size by changing the sample size parameters and then checking whether this gives you enough power. It’s fine, but it’s a little bit of a hassle.\nA more substantive, and related, limitation is that statistical power isn’t about a single threshold number that you’re supposed to reach. Power is a curve, after all. The difference between having obtained 79% power or 80% power is only 1% and does not strongly affect the interpretation of your obtained power. The code from my previous post doesn’t really illustrate this point, so let’s do that here."
  },
  {
    "objectID": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#setup",
    "href": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#setup",
    "title": "Simulation-based power curves",
    "section": "Setup",
    "text": "Setup\nIf you want to follow along, run the following setup code:\n\n\nCode\n# Load packages\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(effectsize)\nlibrary(knitr)\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n# Set options\noptions(\n  knitr.kable.NA = \"\",\n  digits = 2\n)"
  },
  {
    "objectID": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-scenario",
    "href": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-scenario",
    "title": "Simulation-based power curves",
    "section": "The scenario",
    "text": "The scenario\nLet’s look at one of the scenarios from the previous post: the two t-test scenario. We have one treatment condition and two control conditions. Our goal is to have enough power to find a significant difference between the treatment condition and both control conditions. Let’s begin with a single simulation, just to see what the data would look like.\n\n\nCode\n# Parameters\nM_control1 &lt;- 5\nM_control2 &lt;- M_control1\nM_treatment &lt;- 5.5\nSD_control1 &lt;- 1\nSD_control2 &lt;- SD_control1\nSD_treatment &lt;- 1.5\nN &lt;- 50\n\n# Simulate once\ncontrol1 &lt;- mvrnorm(N, mu = M_control1, Sigma = SD_control1^2, \n  empirical = TRUE)\ncontrol2 &lt;- mvrnorm(N, mu = M_control2, Sigma = SD_control2^2, \n  empirical = TRUE)\ntreatment &lt;- mvrnorm(N, mu = M_treatment, Sigma = SD_treatment^2, \n  empirical = TRUE)\n\n# Prepare data\ncolnames(control1) &lt;- \"DV\"\ncolnames(control2) &lt;- \"DV\"\ncolnames(treatment) &lt;- \"DV\"\n\ncontrol1 &lt;- control1 %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"control 1\")\n\ncontrol2 &lt;- control2 %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"control 2\")\n\ntreatment &lt;- treatment %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"treatment\")\n\ndata &lt;- bind_rows(control1, control2, treatment)\n\n\nI’ve changed the code a little bit compared to my previous post. We still assume the same parameters in both control conditions, but now I use one of the control condition variables to set the value of the variable from the other control condition. This means you only need to set the value once and this reduces mistakes due to typos. In addition, we assume the same sample size in each condition and I also changed the parameter values.\nWe inspect the data by visualizing it and by calculating a standardized effect size that quantifies the difference between the treatment condition and each of the two control conditions.\n\n\nCode\n# Calculate standardized effect size\neffect_size &lt;- cohens_d(DV ~ condition, pooled_sd = FALSE,\n  data = filter(data, condition != \"control 1\"))\n\n# Visualize the data\nggplot(data, aes(x = condition, y = DV)) + \n  geom_jitter(width = .2, alpha = .25) + \n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  labs(x = \"Condition\")\n\n\n\n\n\n\n\n\nFigure 1: Three groups visualization\n\n\n\n\n\nThe effect size is a Cohen’s d -0.39 and although the Cohen’s d is negative (due to the ordering the levels in the condition column), the values in the treatment condition are higher than in the control conditions.\nAs a reminder, I think we should analyze this data with two Welch’s two sample t-tests: one for the difference between the treatment condition and control group 1 and also one between the treatment condition and control group 2. Below is some code to run these tests with the current data frame.\n\n\nCode\nt.test(DV ~ condition, data = filter(data, condition != \"control 1\"))\nt.test(DV ~ condition, data = filter(data, condition != \"control 2\"))"
  },
  {
    "objectID": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-power-analysis",
    "href": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-power-analysis",
    "title": "Simulation-based power curves",
    "section": "The power analysis",
    "text": "The power analysis\nNext we want to calculate the power. However, we don’t just want the power for a single sample size. We want to calculate the power for a range of sample sizes. So we begin by defining this range. In the example below, we create a variable called Ns that contains a sequence of values ranging from 50 to 250, in steps of 50. We also define S, which is the number of iterations in the power analysis. The higher this number, the more accurate the power analysis. Note that I’ve capitalized it this time. The final simulation parameter is i. I’ve defined this variable to keep track of how often we loop. This will be useful for figuring out where to store the p-values.\nWe will store the p-values in a data frame this time. Using the crossing() function, we can create a data frame that contains all possible combinations of the sample sizes and the number of simulation iterations. We can also already add some empty columns to store the p-values in.\nAfter that, we begin the loop. We want to loop over each sample size and we want to loop S times for each sample size, running the analyses in every single loop. This means we have a nested for loop. This is not particularly difficult, as we can simply put a for loop within a for loop. The only trick bit is to make sure that you store the p-values correctly. That’s where the i variable comes in. We initially gave it a value of 1, so the first p-values will be stored in the first row. At the end of each loop we increment it by 1, making sure that the next p-values will be stored in the next row of our p-values data frame.\n\n\nCode\n# Set simulation parameters\nNs &lt;- seq(from = 50, to = 250, by = 50)\nS &lt;- 1000\ni &lt;- 1 \n\n# Create a data frame to store the p-values in\np_values &lt;- crossing(\n  n = Ns,\n  s = 1:S,\n  p_value1 = NA,\n  p_value2 = NA\n)\n\n# Loop\nfor (n in Ns) {\n  for (s in 1:S) {\n    # Simulate\n    control1 &lt;- mvrnorm(n, mu = M_control1, Sigma = SD_control1^2)\n    control2 &lt;- mvrnorm(n, mu = M_control2, Sigma = SD_control2^2)\n    treatment &lt;- mvrnorm(n, mu = M_treatment, Sigma = SD_treatment^2)\n    \n    # Run tests\n    test1 &lt;- t.test(control1[, 1], treatment[, 1])\n    test2 &lt;- t.test(control2[, 1], treatment[, 1])\n    \n    # Extract p-values\n    p_values$p_value1[i] &lt;- test1$p.value\n    p_values$p_value2[i] &lt;- test2$p.value\n  \n    # Increment i\n    i &lt;- i + 1\n  }\n}\n\n\nThe result is a data frame with two columns containing p-values. To calculate the power, we sum the number of times we find a significant effect in both columns and divide it by the number of iterations per sample size (S). We need to do this for each sample size. The following code does the trick:\n\n\nCode\npower &lt;- p_values %&gt;%\n  mutate(success = if_else(p_value1 &lt; .05 & p_value2 &lt; .05, 1, 0)) %&gt;%\n  group_by(n) %&gt;%\n  summarize(power = sum(success) / S)"
  },
  {
    "objectID": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-power-curve",
    "href": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-power-curve",
    "title": "Simulation-based power curves",
    "section": "The power curve",
    "text": "The power curve\nWith the power for each sample size calculated, we can visualize the power curve.\n\n\nCode\nggplot(power, aes(x = n, y = power)) +\n  geom_line(linetype = 2) +\n  geom_point() +\n  geom_hline(yintercept = .80, linetype = 3) +\n  geom_hline(yintercept = .95, linetype = 3) +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +\n  labs(x = \"Sample size (n) per condition\", y = \"Power\")\n\n\n\n\n\n\n\n\nFigure 2: The power curve\n\n\n\n\n\nThat’s it. We can see at which point we have enough power (e.g., 80% or 95%). Do note that we calculated the sample size per condition. In the current scenario, this means you need to multiply the sample size by three in order to obtain your total sample size.\nIf you want a smoother graph, you can adjust the range of sample sizes that we stored in the Ns variable. You can, for example, lower the by argument in the seq() function to get a more fine-grained curve.\nThis post was last updated on 2022-04-29."
  },
  {
    "objectID": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html",
    "href": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html",
    "title": "Animals slaughtered in the Netherlands",
    "section": "",
    "text": "In this post I take a look at how many animals are slaughtered in the Netherlands. The goal is to find and clean the data to answer this question and to get a better grasp of exactly how many animals are killed here every hear. It’s both an exercise in data cleaning and calibrating one’s beliefs about this topic, as this is something that’s too easy to avoid thinking about, while probably being one of the most important things you should think about."
  },
  {
    "objectID": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data",
    "href": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data",
    "title": "Animals slaughtered in the Netherlands",
    "section": "Data",
    "text": "Data\nThe data on the number of animals slaughtered in the Netherlands can be found on StatLine. This is a database managed by CBS, the national statistical office of the Netherlands. Specifically, we’re going to take a look at the meat production numbers (vleesproductie). These numbers can be found in a table here. We can adapt what is in the table by changing various filters. By default it shows both the number of animals and the weight of the animals. I’m only interested in the number of animals, so I deselect the weight-related rows. I also see that they offer data on more dates than is shown by default, so I select all of the dates. I then download the data as a .csv file using the button in the top right corner. Now we can start cleaning the data for our purposes."
  },
  {
    "objectID": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#setup",
    "href": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#setup",
    "title": "Animals slaughtered in the Netherlands",
    "section": "Setup",
    "text": "Setup\nRun the following setup code if you want to follow along. You can download the data yourself or use my file.\n\n\nCode\n# Load packages\nlibrary(tidyverse)\n\n# Load data\ndata &lt;- read_csv2(\"meat-production-netherlands.csv\")\n\n# Set default ggplot\ntheme_set(theme_minimal())\n\n\nNote that we have to use read_csv2() because the data values are separated by a semi-colon. This is an annoying default in the Netherlands (and probably elsewhere in Europe)."
  },
  {
    "objectID": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data-cleaning",
    "href": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data-cleaning",
    "title": "Animals slaughtered in the Netherlands",
    "section": "Data cleaning",
    "text": "Data cleaning\nLet’s begin by inspecting the first few rows of the data.\n\n\nCode\nhead(data)\n\n\n First 6 rows of the data.\n  \n\n\n\nIt should be no surprise, but the data is in Dutch. Let’s translate the data, starting with the columns. One of the columns is called Aantal slachtingen (x 1 000), which means number of slaughtered animals in units of 1000. Instead of translating this directly, I will simply rename it to count and multiply the values by 1000.\n\n\nCode\ndata &lt;- data %&gt;%\n  rename(\n    animal = Slachtdieren,\n    period = Perioden,\n    count = `Aantal slachtingen (x 1 000)`\n  ) %&gt;%\n  mutate(count = count * 1000)\n\n\nLet’s clean up the period column next. It seems like it contains the year and the month (in Dutch). I can translate the month names to Dutch, but I first want to make sure that all data values are structured the same way. count() is a great function to inspect that.\n\n\nCode\ndata %&gt;%\n  count(period)\n\n\n\n  \n\n\n\nCuriously, not all rows in the data contain both the year and the month. Some only have the year. This is important because that means we can’t just sum the number of slaughtered animals per year because that means we’ll actually get twice the number of animals because we’ll sum both the animals slaughtered in that year and each month of that year. The last several months also have an asterisk in the month name. This asterisk indicates that the data for these months has not yet been finalized.\nWhat I want to do next is create a new column that only contains the year and another column that contains the month. Creating the year column is easy because we can use parse_number() to extract the year from the data. The month is a bit trickier, but we can use a regular expression to remove the year, leaving us with the month. We use str_remove() and tell it to remove a string pattern that consists of 4 numbers and a space. We can also use it to remove the asterisk from the more recent months, but first we add a column to say whether the numbers are final or not based on this asterisk. After doing that, we recode the month values that need to be translated and also convert the empty string to a missing value. Finally, we remove the period column because we don’t need it anymore.\n\n\nCode\ndata &lt;- mutate(data,\n    year = parse_number(period),\n    month = str_remove(period, \"[0-9]{4} ?\"),\n    final = if_else(str_detect(period, \"\\\\*\"), \"no\", \"yes\"),\n    month = str_remove(month, \"\\\\*\"),\n    month = recode(month,\n      \"augustus\" = \"august\",\n      \"februari\" = \"february\",\n      \"januari\" = \"january\",\n      \"juli\" = \"july\",\n      \"juni\" = \"june\",\n      \"maart\" = \"march\",\n      \"mei\" = \"may\",\n      \"oktober\" = \"october\",\n    ),\n    month = na_if(month, \"\"),\n    period = NULL\n  )\n\n\nNext are the animals. Let’s take a look at the unique values we have.\n\n\nCode\ncount(data, animal)\n\n\n\n  \n\n\n\nHmm… it looks like there are a few challenges here. First, we seem to have both total values and non-total values, so we should take care to separate these. Second, we need to figure out what each word means. Even my Dutch is not helping me in understanding each type of animal.\nLet’s first simply translate the values so we get a better grasp of what we are dealing with. The translations won’t be direct translations. Instead, I already think about what kind of categories make sense and how I want to plot the data later, so I translate the values into names that will also be useful later.\n\n\nCode\ndata &lt;- mutate(data, \n  animal = recode(animal, \n    \"Eenhoevigen\" = \"ungulates (mostly horses)\",\n    \"Geiten (totaal)\" = \"goats\",\n    \"Kalkoenen\" = \"turkeys\",\n    \"Kalveren jonger dan 9 maanden\" = \"calves (&lt; 9 months)\",\n    \"Kalveren van 9 tot en met 12 maanden\" = \"calves (9-12 months)\",\n    \"Koeien\" = \"cows\",\n    \"Overig pluimvee\" = \"poultry (misc)\",\n    \"Overige kippen\" = \"chicken (mostly layers)\",\n    \"Rundvee (totaal)\" = \"cattle\",\n    \"Schapen incl. lammeren\" = \"sheep\",\n    \"Schapenlammeren\" = \"lambs\",\n    \"Stieren\" = \"bulls\",\n    \"Totaal kalveren\" = \"calves\",\n    \"Totaal volwassen runderen\" = \"adult cattle (total)\",\n    \"Vaarzen\" = \"heifers\",\n    \"Varkens (totaal)\" = \"pigs\",\n    \"Vleeskuikens\" = \"broilers\"\n  )\n)\n\n\nTranslating the words was very helpful to better understand the data. One thing that’s clear is that some of the values are totals of other values. Below I list which values in the data are actually sums of other values:\n\nadult cattle: Total of cows, heifers, and bulls\ncattle: Total of adult cattle and calves\ncalves: Total of calves (&lt; 9 months) and calves (9-12 months)\n\nIf we are interested in what the totals are made of, we can remove the total columns and reconstruct them later if we want to. This works for the first two total columns, but not calves because they only started making the distinction between young and older calves in 2009. So let’s instead remove the values that the total values are made of and only keep the total values.\n\n\nCode\ndata &lt;- filter(data, !animal %in% c(\"adult cattle (total)\", \"cows\", \n  \"heifers\", \"bulls\", \"calves\",\"calves (&lt; 9 months)\", \n  \"calves (9-12 months)\", \"lambs\")\n)\n\n\nThis leaves us with the following animals.\n\n\nCode\ncount(data, animal)\n\n\n\n  \n\n\n\nThis looks fine to me, which means we are almost done with the data cleaning. At this point I want to create two separate data frames: one that only contains the annual data and one that contains the monthly data. This is easy to do because we can take all the annual data by simply selecting the rows with a missing value in the month column. Since the month column is useless in that data frame, we remove it.\n\n\nCode\ndata_annual &lt;- data %&gt;%\n  filter(is.na(month)) %&gt;%\n  select(-month)\n\ndata &lt;- filter(data, !is.na(month))\n\n\nAs a final step we can combine the year and month columns from the monthly data frame into a single column, which will be useful for plotting the data later. This requires a special function from the zoo package.\n\n\nCode\ndata &lt;- mutate(data,\n  month = str_to_sentence(month),\n  month = match(month, month.name),\n  year_month = paste(year, month, \"1\", sep = \"-\"),\n  year_month = lubridate::as_date(year_month),\n  year_month = zoo::as.yearmon(year_month)\n)"
  },
  {
    "objectID": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data-analysis",
    "href": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data-analysis",
    "title": "Animals slaughtered in the Netherlands",
    "section": "Data analysis",
    "text": "Data analysis\nWith the data cleaned up we can start to ask some questions. Let’s begin with a graph that shows as much of the data as possible. That means plotting the monthly data for each animal.\n\n\nCode\nggplot(data, aes(x = year_month, y = count)) +\n  geom_point(size = 1) +\n  geom_line(alpha = .25) +\n  facet_wrap(~ animal, scales = \"free\") \n\n\n\n\n\nNumber of slaughtered animals per month and animal\n\n\n\n\nA few observations:\n\nThe numbers are very high, particularly for some animals (e.g., broilers)\nSome animals used to be slaughtered in larger numbers than now\n\nThis is clearly the case for turkeys and poulty (misc), but also other other animals such as cattle and pigs\n\nSome numbers are relatively low (e.g., for horses)\nThe data sometimes fluctuates quite a bit from month to month, so annual view might show clearer patterns\n\nGiven these observations, let’s create a subset focusing only on six categories of animals that are still being slaughtered in large numbers and let’s also plot the annual data.\n\n\nCode\ndata_annual_subset &lt;- data_annual %&gt;%\n  filter(animal %in% c(\"broilers\", \"goats\", \"sheep\", \"cattle\", \"pigs\", \n    \"chicken (mostly layers)\")\n  ) %&gt;%\n  filter(final == \"yes\")\n\nggplot(data_annual_subset, aes(x = year, y = count)) +\n  geom_point(size = 1) +\n  geom_line(alpha = .25) +\n  facet_wrap(~ animal, scales = \"free\")\n\n\n\n\n\nNumber of slaughtered animals per year and animal\n\n\n\n\nOkay, parsing this graph I see that a lot of chicken are slaughtered every year, particularly broiler chicken. The numbers are so high that ggplot has switched to the scientific notation to represent the numbers. Interestingly, though, the number of slaughtered broiler chickens has decreased somewhat in the last two years. I don’t know why that is. I also see that some animals are slaughtered more and more over the years (e.g., cattle, non-broiler chicken, goats, and pigs), although I’m also surprised to see that for some animals we’ve had worse years, particularly for cattle and pigs. For those animals we see a huge drop around the year 2000. The reason for that drop was the outbreak of foot-and-mouth disease and subsequent regulation. I thought things were getting worse and worse, but apparently it was already worse a while ago.\nLet’s hone in on some exact numbers. Below we create a table to show the number of slaughtered animals in 2021, per animal.\n\n\nCode\ndata_annual %&gt;%\n  filter(year == 2021) %&gt;%\n  arrange(desc(count)) %&gt;%\n  select(animal, count)\n\n\n Number of slaughtered animals in 2021\n  \n\n\n\nOof. That’s over 500 million chicken! For reference, the Netherlands had a population of 17.17 million in 2021.\nHow many animals were slaughtered in total, in 2021?\n\n\nCode\ncount_total_2021 &lt;- data_annual %&gt;%\n  filter(year == 2021) %&gt;%\n  summarize(count_total = sum(count)) %&gt;%\n  pull(count_total)\n\n\nApparently a total of 541039500, or 17.16 animals per second. That means that about…\n\n0\n\n…have died since you started reading this blog post. That’s a bit of a bummer to end on, but then, this post was never going to have a happy ending.\nThis post was last updated on 2022-09-22."
  },
  {
    "objectID": "content/posts/20-bayesian-tutorial-2-groups/bayesian-tutorial-2-groups.html",
    "href": "content/posts/20-bayesian-tutorial-2-groups/bayesian-tutorial-2-groups.html",
    "title": "Bayesian tutorial: Two groups",
    "section": "",
    "text": "In this blog post I will cover how to use brms to analyze the difference between two groups. Interestingly, this might seem like a very simple analysis, but there are actually multiple ways to go about this. I’ll try to cover a few here.\nThe data we’ll use is the same as in the previous posts. This data contains the sex of the participant in a column called male, in addition to their height, weight, and age. This means we can investigate, say, whether there’s a difference in height between men and women.\nRun the following setup code if you want to follow along.\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\n\ntheme_set(theme_minimal())\nblue_1 &lt;- \"#d1e1ec\"\nblue_2 &lt;- \"#b3cde0\"\nblue_3 &lt;- \"#6497b1\"\nblue_4 &lt;- \"#005b96\"\nblue_5 &lt;- \"#03396c\"\nblue_6 &lt;- \"#011f4b\"\n\noptions(\n  mc.cores = 4,\n  brms.threads = 4,\n  brms.backend = \"cmdstanr\",\n  brms.file_refit = \"on_change\"\n)\n\ndata &lt;- read_csv(\"Howell1.csv\")\ndata &lt;- filter(data, age &gt;= 18)"
  },
  {
    "objectID": "content/posts/20-bayesian-tutorial-2-groups/bayesian-tutorial-2-groups.html#dummy-coding",
    "href": "content/posts/20-bayesian-tutorial-2-groups/bayesian-tutorial-2-groups.html#dummy-coding",
    "title": "Bayesian tutorial: Two groups",
    "section": "Dummy coding",
    "text": "Dummy coding\nProbably the most common method to analyze the difference between two groups is to regress the outcome on dummy coded data. That means the group column contains the information about both groups, usually using a 0 and 1 to indicate which group is which. The male column in the current data frame is dummy coded, with a 0 for females and a 1 for males. The R formula for regressing height on male is height ~ male. Let’s see what priors we need to set for this by using the get_prior() function.\n\n\nCode\nget_prior(height ~ male, data = data)\n\n\n                    prior     class coef group resp dpar nlpar lb ub\n                   (flat)         b                                 \n                   (flat)         b male                            \n student_t(3, 154.3, 8.5) Intercept                                 \n     student_t(3, 0, 8.5)     sigma                             0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nThe output shows we need to set a prior on sigma, the Intercept, and on the male coefficient. The intercept refers to the heights of female participants and the male coefficient refers to the difference between males and females, at least that’s what you would expect in standard regression. We’ll see that things are a bit more complicated. Sigma refers, as always, to the standard deviation of the residuals.\nWriting down this model shows that it’s actually the same as the simple regression model:\n\\[\\displaylines{heights_i ∼ Normal(\\mu_i, \\sigma) \\\\ \\mu_i = \\alpha + \\beta x_i}\\]\nDespite this similarity, there’s an issue here that we need to get into. Let’s demonstrate this issue by setting some priors and running the model while only sampling from the priors. This means the estimates we get for the intercept and male coefficient should match our priors.\n\n\nCode\nmodel_dummy_priors_default &lt;- brm(\n  height ~ male,\n  data = data,\n  family = gaussian,\n  prior = c(\n    prior(normal(165, 5), class = \"Intercept\"),\n    prior(normal(10, 5), coef = \"male\"),\n    prior(cauchy(5, 5), class = \"sigma\")\n  ),\n  sample_prior = \"only\",\n  cores = 4,\n  file = \"./models/model-dummy-priors-default.rds\"\n)\n\nmodel_dummy_priors_default\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ male \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   160.31      5.53   149.75   170.83 1.00     2970     2614\nmale         10.08      4.96     0.25    19.89 1.00     2986     2721\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    17.18     77.44     0.49    82.26 1.00     3059     1884\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLooking at the estimates we see that the estimate for male is indeed around 10 but the intercept estimate is not close to what we set it too. The intercept has an estimate of around 160, even though we set it to 165. How can this be? The reason is that brms internally centers the intercept so that it corresponds to the expected response when all predictors are held at their means. This is what the prior is set to, not to the mean of the heights of female participants. brms also re-generates the true intercept, which is the one we see in the output, so the intercept in the output does correctly indicate the mean height of female participants. Confusing.\nTo prevent this from happening, we need to suppress the default intercept and explicitly add one as a coefficient. This means we need to update our prior for the intercept to refer to a coef (instead of class).\n\n\nCode\nmodel_dummy_priors &lt;- brm(\n  height ~ 0 + Intercept + male,\n  data = data,\n  family = gaussian,\n  prior = c(\n    prior(normal(165, 5), coef = \"Intercept\"),\n    prior(normal(10, 5), coef = \"male\"),\n    prior(cauchy(5, 5), class = \"sigma\")\n  ),\n  sample_prior = \"only\",\n  seed = 4,\n  file = \"./models/model-dummy-priors.rds\"\n)\n\nmodel_dummy_priors\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 0 + Intercept + male \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   165.01      5.05   154.95   174.84 1.00     3028     2572\nmale         10.03      4.86     0.19    19.33 1.00     3394     2771\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    18.58    148.85     0.49    78.55 1.00     2741     1446\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe estimates (i.e., our priors) show that the average height for women is 165.01 and that for men is 165.01 + 10.03 = 175.04.\nBut now there’s a new problem. Let’s take a look at how uncertain we should be at these estimates by predicting the means for both men and women and then calculating the width of the 95% quartile interval.\n\n\nCode\ntibble(male = c(0, 1)) %&gt;%\n  add_epred_draws(model_dummy_priors) %&gt;%\n  median_qi() %&gt;%\n  mutate(width = .upper - .lower)\n\n\n# A tibble: 2 × 9\n   male  .row .epred .lower .upper .width .point .interval width\n  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1     0     1   165.   155.   175.   0.95 median qi         19.9\n2     1     2   175.   161.   189.   0.95 median qi         28.3\n\n\nWe see that the width for males is slightly wider than that for females. If you run the model multiple times at different seeds, you’ll see that this is something that happens consistently. The reason there is more uncertainty for the mean of male participants is that we’ve added a separate prior that only applies to males—the prior on the male coefficient. The mean of female participants is wholly determined by the data and the prior on the intercept, but for males it’s the data and the prior for the intercept + the prior for the male coefficient. This is not desirable.\nOn top of that it less intuitive to think about the priors as a mean for females and then a prior for how much males deviate from this mean. It seems nicer to just specify what we think the heights are for females and what the heights are for males, without thinking about the deviation.\nBoth of these issues can be solved by using index coding instead of dummy coding."
  },
  {
    "objectID": "content/posts/20-bayesian-tutorial-2-groups/bayesian-tutorial-2-groups.html#index-coding",
    "href": "content/posts/20-bayesian-tutorial-2-groups/bayesian-tutorial-2-groups.html#index-coding",
    "title": "Bayesian tutorial: Two groups",
    "section": "Index coding",
    "text": "Index coding\nIndex coding requires that we create a new column that is a factor or that contains string values to indicate the sex of the participant. Below I create a new column called sex that contains the values male and female as string values.\n\n\nCode\ndata &lt;- mutate(data, sex = if_else(male == 1, \"male\", \"female\"))\n\n\nTo solve the uncertainty issue, we regress height onto the index-coded sex column, and omit an intercept. This will result in a model that requires priors for each group separately.\n\n\nCode\nget_prior(height ~ 0 + sex, data = data)\n\n\n                prior class      coef group resp dpar nlpar lb ub       source\n               (flat)     b                                            default\n               (flat)     b sexfemale                             (vectorized)\n               (flat)     b   sexmale                             (vectorized)\n student_t(3, 0, 8.5) sigma                                  0         default\n\n\nAs you can see, we need a prior for sexfemale and sexmale, as well as sigma.\nWe should alter how we describe this model. We can now drop the \\(\\beta\\) parameter from the model and we should more explicitly indicate that we will model several \\(\\alpha\\) parameters, one for each sex.\n\\[\\displaylines{heights_i ∼ Normal(\\mu_i, \\sigma) \\\\ \\mu_i = \\alpha_{sex[i]}}\\]\nNext, I simply translated the previous priors to this new notation. The prior for the heights of female participants stays the same (a normal distribution with a mean of 165 and a standard deviation of 5). The prior for the heights of male participants is also still a normal distribution, but now with a mean of 175 (165 + 10) and the same standard deviation.\n\n\nCode\nmodel_index_prior &lt;- brm(\n  height ~ 0 + sex,\n  data = data,\n  family = gaussian,\n  prior = c(\n    prior(normal(165, 5), coef = \"sexfemale\"),\n    prior(normal(175, 5), coef = \"sexmale\"),\n    prior(cauchy(5, 5), class = \"sigma\")\n  ),\n  sample_prior = \"only\",\n  seed = 4,\n  file = \"./models/model-index-prior.rds\"\n)\n\nmodel_index_prior\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 0 + sex \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsexfemale   165.02      5.01   155.55   175.14 1.00     3168     2712\nsexmale     175.02      5.05   165.17   184.97 1.00     3279     2643\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    21.58    197.75     0.66    80.42 1.00     3452     1983\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s confirm that the uncertainty in the priors for the male and female average heights are the same.\n\n\nCode\ntibble(sex = c(\"male\", \"female\")) %&gt;%\n  add_epred_draws(model_index_prior) %&gt;%\n  median_qi() %&gt;%\n  mutate(width = .upper - .lower)\n\n\n# A tibble: 2 × 9\n  sex     .row .epred .lower .upper .width .point .interval width\n  &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 female     2   165.   156.   175.   0.95 median qi         19.6\n2 male       1   175.   165.   185.   0.95 median qi         19.8\n\n\nThey are. This means we can now run the model and also sample from the posterior.\n\n\nCode\nmodel_index &lt;- brm(\n  height ~ 0 + sex,\n  data = data,\n  family = gaussian,\n  prior = c(\n    prior(normal(165, 5), coef = \"sexfemale\"),\n    prior(normal(175, 5), coef = \"sexmale\"),\n    prior(cauchy(5, 5), class = \"sigma\")\n  ),\n  sample_prior = TRUE,\n  seed = 4,\n  file = \"./models/model-index.rds\"\n)\n\nmodel_index\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 0 + sex \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsexfemale   149.62      0.42   148.81   150.44 1.00     3767     3144\nsexmale     160.47      0.43   159.62   161.30 1.00     4002     3094\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.56      0.21     5.16     6.00 1.00     4697     3302\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe estimates show, more clearly now, that the average height for women is 149.62 and that for men is 160.47. The posteriors of these estimates are shown below.\n\n\nCode\ndraws &lt;- model_index %&gt;%\n  gather_draws(b_sexfemale, b_sexmale) %&gt;%\n  mutate(sex = str_extract(.variable, \"female|male\"))\n\nggplot(draws, aes(x = .value, fill = sex)) +\n  geom_histogram(binwidth = 0.1) +\n  labs(x = \"Average height\", y = \"\", fill = \"Sex\") +\n  scale_fill_manual(values = c(blue_2, blue_4))\n\n\n\n\n\nPosterior distributions of the coefficients for men and women."
  },
  {
    "objectID": "content/posts/20-bayesian-tutorial-2-groups/bayesian-tutorial-2-groups.html#calculating-a-difference-score",
    "href": "content/posts/20-bayesian-tutorial-2-groups/bayesian-tutorial-2-groups.html#calculating-a-difference-score",
    "title": "Bayesian tutorial: Two groups",
    "section": "Calculating a difference score",
    "text": "Calculating a difference score\nIn the previous section I’ve argued that we should use index coding so that we can more easily think about, and see the results, of the priors about the two groups. I realize, though, that we are often still interested in the difference between the groups. This is easy to obtain, though. We can simply take the posterior samples from each group and subtract them from each other.\nIn the code below I extract the draws of each parameter (b_sexfemale and b_sexmale) and calculate the difference score. I then extract the draws of the difference score and calculate the median and quartile interval.\n\n\nCode\nmodel_index %&gt;%\n  spread_draws(b_sexfemale, b_sexmale) %&gt;%\n  mutate(difference = b_sexmale - b_sexfemale) %&gt;%\n  pull(difference) %&gt;%\n  median_qi()\n\n\n       y   ymin   ymax .width .point .interval\n1 10.849 9.6629 12.005   0.95 median        qi\n\n\nAlternatively, we can also plot it as a distribution.\n\n\nCode\ndraws &lt;- model_index %&gt;%\n  spread_draws(b_sexfemale, b_sexmale) %&gt;%\n  mutate(difference = b_sexmale - b_sexfemale)\n\nggplot(draws, aes(x = difference)) +\n  geom_histogram(binwidth = 0.1, fill = blue_3) +\n  labs(x = \"Difference\", y = \"\")"
  },
  {
    "objectID": "content/posts/20-bayesian-tutorial-2-groups/bayesian-tutorial-2-groups.html#summary",
    "href": "content/posts/20-bayesian-tutorial-2-groups/bayesian-tutorial-2-groups.html#summary",
    "title": "Bayesian tutorial: Two groups",
    "section": "Summary",
    "text": "Summary\nJust like running a correlation, testing a group difference consists of running a simple regression. However, having groups as a predictor means the regression is not so simple after all. You have to think more carefully about what the priors mean (particularly the intercept) and you have to deal with greater uncertainty for some estimates, depending on how you code the group predictor. I’ve shown that explicitly including the intercept and using index coding makes thinking about this scenario a bit easier.\nThis post was last updated on 2023-08-07."
  },
  {
    "objectID": "content/posts/28-sad-animal-facts/sad-animal-facts.html",
    "href": "content/posts/28-sad-animal-facts/sad-animal-facts.html",
    "title": "Sad wild animal facts",
    "section": "",
    "text": "Let us understand, once for all, that the ethical process of society depends, not on imitating the cosmic process, still less on running away from it, but in combating it.\nThomas Huxley\nThis post serves to illustrate why I agree with the quote above. Below I list examples of what happens in nature that I think are just horrible and that I wish we could do something about.\nThe list is not complete and I intend to continuously update it when I encounter more sad facts.\nLast updated: March 19, 2024"
  },
  {
    "objectID": "content/posts/28-sad-animal-facts/sad-animal-facts.html#mammals",
    "href": "content/posts/28-sad-animal-facts/sad-animal-facts.html#mammals",
    "title": "Sad wild animal facts",
    "section": "Mammals",
    "text": "Mammals\n\nMeadow vole\n\nScientific name: Microtus pennsylvanicus\nOne study found that only 12% of meadow voles survive the first month, with most voles likely dying due to predation (Clutton-Brock et al., 1998).\n\n\nOrangutan\n\nScientific name: Pongo pygmaeus\nMale orangutans frequently force female orangutans to mate with them (Mitani, 1985).\n\n\nMeerkat\n\nScientific name: Suricata suricatta\nDominant females commonly kill pups born to subordinates and temporarily expel subordinate females from the group during the latter months of their own pregnancy (Turner et al., 2022)."
  },
  {
    "objectID": "content/posts/28-sad-animal-facts/sad-animal-facts.html#birds",
    "href": "content/posts/28-sad-animal-facts/sad-animal-facts.html#birds",
    "title": "Sad wild animal facts",
    "section": "Birds",
    "text": "Birds\n\nBrown-headed cowbird\n\nScientific name: Molothrus ater\nCowbirds lay their eggs in the nests of other birds and display ‘mafia’ like behavior by destroying eggs or nestlings of hosts if they eject their eggs (Hoover & Robinson, 2007).\n\n\nCommon cuckoo\n\nScientific name: Cuculus canorus\nThe common cuckoo female lays one egg is the nest of other birds and usually removes and eats one of the host’s eggs. In general, the cuckoo egg hatches before those of the host and the cuckoo chicks start evicting host offspring from the nest (Martín-Gálvez et al., 2005)."
  },
  {
    "objectID": "content/posts/28-sad-animal-facts/sad-animal-facts.html#insects",
    "href": "content/posts/28-sad-animal-facts/sad-animal-facts.html#insects",
    "title": "Sad wild animal facts",
    "section": "Insects",
    "text": "Insects\n\nForelius pusillus\nScientific name: Forelius pusillus\nIn the Brazilian ant Forelius pusillus, the nest entrance is closed at sunset. One to eight workers ﬁnish the job from the outside and, in doing so, sacriﬁce their lives (Tofilski et al., 2008)."
  },
  {
    "objectID": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html",
    "href": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html",
    "title": "Simulation-based power analyses",
    "section": "",
    "text": "Doing power analyses is hard. I know this from experience, both as a researcher and as a reviewer. As a researcher, I have found power analyses to be difficult because performing a good power analysis requires a thorough understanding of the (hypothesized) data. Understanding one’s data is often underestimated, I think. We’re very quick to design a study and start data collection, without often knowing what various aspects of our data will look like (e.g., likely correlations between measures, likely standard deviations). As a reviewer, I see that power analyses are difficult because of wrong ideas about what a power analysis actually means. The most common misconception I see is that researchers think they should power their study, rather than the set of analyses they will conduct (see Maxwell (2004) for more on this). I also see a lot of power analyses conducted with G*Power, which sometimes looks fine, but oftentimes produces results I know to be wrong (usually involving interaction tests). So what to do?\nMy favorite way to run power analyses is via simulation. Simulation-based power analyses are more difficult and take longer to setup and run, but they’re more pedagogical. Simulations require you to understand your data because you have to define the exact parameters that define your data set (e.g., means, standard deviations, correlations). It also creates a very intuitive understanding of what power is: Power is simply counting how often you find the results you expect to find.\nStill, running simulation-based power analyses might be too difficult for some. So in this blog post I present code to simulate data for a range of different scenarios.\n\n\nRun the following code to get started. The most important package here is MASS. It contains a function called mvrnorm() that enables us to simulate data from a multivariate normal distribution. This means we’ll simulate data for scenarios where we have a continuous outcome. I really like this function for simulating data because it has an argument called empirical that you can set to TRUE, which causes your simulated data to have the exact properties you set (e.g., exactly a mean of 4). This is a great way to check out your simulated data and see if it makes sense.\nWe will use the tidyverse because we need to prepare the data after simulating it. mvrnorm() returns a matrix with each simulated variable as a column. This means we sometimes need to prepare the data so that we can perform the tests we want to run or for visualization purposes.\nThe effectsize package will be used to inspect the data by calculating standardized effect sizes. This will allow us to check whether the parameters are plausible.\nFinally, we sometimes use the broom package to extract p-values from the statistical tests that we’ll run. This will be necessary to calculate the power because power is (usually) nothing more than the number of significant p-values divided by the number of tests we simulated data for. In a future post I might focus on Bayesian analyses, so we won’t be dealing with p-values then, although the logic will be the same.\nBesides loading packages, we also set the s variable. The value of this variable will determine how many times we’ll simulate the data during the power analysis. The higher this number, the more accurate our power estimates will be.\n\n\nCode\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(effectsize)\nlibrary(broom)\nlibrary(viridis)\n\ntheme_set(theme_minimal())\n\noptions(\n  knitr.kable.NA = \"\",\n  digits = 2\n)\n\n# Set number of loops in the power simulations\ns &lt;- 1000\n\n\nWith the setup out of the way, let’s cover our general approach to power analyses:\n\nSimulate the data with fixed properties\nCheck the data to see if the data is plausible\nRun the tests we want to run on the data\nRepeat steps 1 to 3 many times, save the p-values, and calculate power\n\nWe’ll do this for various scenarios. In each scenario we start by defining the parameters. I’ll focus on providing means, standard deviations, and correlations, because those are usually the parameters we report in the results section, so I’m guessing most researchers will have some intuitions about what these parameters mean and whether the results are plausible.\nThe mvrnorm() function requires that we pass it the sample size, the means, and a variance-covariance matrix. The first two are easy to understand, but the variance-covariance may not be. It’s relatively straightforward to convert means, SDs, and correlations to a variance-covariance matrix, though. Variance is simply the standard deviation squared and the covariance is the product of the standard deviations of the two variables and their correlation. You’ll see in some scenarios below that this is how I construct the variance-covariance matrix.\nNote that the result of each power analysis will be the power, and not the sample size needed to obtain a particular power. This is the same as calculating the post-hoc power in G*Power. If you want to figure out what the sample size is for a particular power (e.g., 80%) then you simply change the sample size parameter until you have the power you want.\n\n\n\nThe simplest scenario is where we want to simulate a set of normally distributed values and perform a one sample t-test. This requires that we set three parameters: a mean, a standard deviation, and a sample size. We give mvrnorm() the sample size (N), the mean (M), and the variance (SD^2). After simulating the data, we give the simulated data a column name and convert the matrix returned by mvrnorm() to a data frame.\n\n\nCode\n# Parameters\nM &lt;- 0.75\nSD &lt;- 5\nN &lt;- 90\n\n# Simulate once with empirical = TRUE\nsamples &lt;- mvrnorm(N, mu = M, Sigma = SD^2, empirical = TRUE)\n\n# Prepare data\ncolnames(samples) &lt;- \"DV\"\ndata &lt;- as_tibble(samples)\n\n\nThe next step is to inspect the data to see whether the parameters are plausible. This can be done by converting the parameters to a standardized effect size and by visualizing the data.\n\n\nCode\n# Calculate a standardized effect size\neffect_size &lt;- cohens_d(data$DV)\n\n# Plot the simulated data\nggplot(data, aes(x = DV)) +\n  geom_histogram(alpha = .5, color = \"gray20\", bins = 30) +\n  geom_vline(xintercept = M, linetype = \"dashed\")\n\n\n\n\n\n\n\n\nFigure 1: One sample visualization\n\n\n\n\n\nThe histogram roughly shows that we have a mean of 0.75 and a standard deviation of 5. We also calculated the Cohen’s d as a measure of the size of the effect. The size of this effect is equal to a Cohen’s d of 0.15.\nNext is the analysis we want to power for—the one-sample t-test. The function for this test is t.test().\n\n\nCode\nt.test(data$DV)\n\n\nTo calculate the power, we repeat the analysis s times. Each time we store the p-value so that later we can calculate the proportion of significant results. Since we don’t need to inspect the data each time, we skip the data preparation step and use the samples returned by mvrnorm() immediately in t.test() using R’s matrix notation (if you want, you can also prepare the data each time, if you find that easier to understand).\n\n\nCode\n# Create a vector to the store p-values in\np_values &lt;- vector(length = s)\n\n# Loop s times\nfor (i in 1:s) {\n  # Simulate\n  samples &lt;- mvrnorm(N, mu = M, Sigma = SD^2)\n\n  # Run test\n  test &lt;- t.test(samples[, 1])\n\n  # Extract p-value\n  p_values[i] &lt;- test$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values &lt;= .05) / s * 100\n\n\nWith the current parameters (N = 90, Cohen’s d = 0.15), we obtain a power of 32.2%. The power is simply how often we find a significant result, divided by the number of times we looped, multiplied by 100 to give a percentage. You can adjust the sample size parameter and re-run the code until you know which sample size gives you the desired power. You might also want to run the loop a few times to see how consistent your results are (if the results are inconsistent, increase the number of loops by increasing the value of s).\n\n\n\nThe next scenario is one in which there are two groups (e.g., a control condition and a treatment condition) and a single DV. Even in this simple scenario there are already several variations that are important to consider. Do we assume equal variances between groups? Do we assume equal samples sizes? Is the design between or within-subjects? We’ll start with assuming unequal variances between the two groups. This means we’ll run a Welch’s two sample t-test. To make it extra fun, we’ll also simulate unequal sample sizes.\nIf we are interested in a between-subjects design where we assume both unequal variances and samples sizes, we can use the code from the previous scenario and simply run it twice, once for each group. After simulating the data, we convert the simulated matrix of each group to a data frame, add a column indicating the group, and merge the two groups into a single data frame.\n\n\nCode\n# Parameters\nM_control &lt;- 5\nM_treatment &lt;- 4\nSD_control &lt;- 1.5\nSD_treatment &lt;- 3\nN_control &lt;- 50\nN_treatment &lt;- 40\n\n# Simulate once with empirical = TRUE\ncontrol &lt;- mvrnorm(N_control,\n  mu = M_control, Sigma = SD_control^2,\n  empirical = TRUE\n)\ntreatment &lt;- mvrnorm(N_treatment,\n  mu = M_treatment, Sigma = SD_treatment^2,\n  empirical = TRUE\n)\n\n# Prepare data\ncolnames(control) &lt;- \"DV\"\ncolnames(treatment) &lt;- \"DV\"\n\ncontrol &lt;- control %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"control\")\n\ntreatment &lt;- treatment %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"treatment\")\n\ndata &lt;- bind_rows(control, treatment)\n\n\nNext, we inspect the data by calculating a Cohen’s d and visualizing the results.\n\n\nCode\n# Calculate a standardized effect size\neffect_size &lt;- cohens_d(DV ~ condition, data = data, pooled_sd = FALSE)\n\n# Visualize the data\nggplot(data, aes(x = condition, y = DV)) +\n  geom_jitter(width = .2, alpha = .25) +\n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  labs(x = \"Condition\")\n\n\n\n\n\n\n\n\nFigure 2: Two groups visualization (unequal variance)\n\n\n\n\n\nThe difference between the two groups is equal to a Cohen’s d of 0.42.\nTo run a Welch’s two-sample t-test, we again use the t.test() function. R by default does not assume equal variances, so the default is a Welch’s two sample t-test.\n\n\nCode\nt.test(DV ~ condition, data = data)\n\n\nThe power analysis looks as follows:\n\n\nCode\n# Create an empty vector to store the p-values in\np_values &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  control &lt;- mvrnorm(N_control, mu = M_control, Sigma = SD_control^2)\n  treatment &lt;- mvrnorm(N_treatment, mu = M_treatment, Sigma = SD_treatment^2)\n\n  # Run test\n  test &lt;- t.test(control[, 1], treatment[, 1])\n\n  # Extract p-value\n  p_values[i] &lt;- test$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values &lt;= .05) / s * 100\n\n\nThis produces a power of 45.9% with the current parameters.\n\n\n\nInstead of assuming unequal variances, we can also assume equal variances and perform a two sample t-test. You could adapt the previous scenario by setting the parameters such that the variance in each group is identical, but let’s do something different in this scenario. In addition, let’s assume that the sample sizes in each group are equal. This means we can simulate the data using a slightly different approach. First, we’ll only need 4 parameters. Second, we don’t need to separately simulate the data for each group. We can instead use a single mvrnorm() call and provide it with the correct variance-covariance matrix. The crucial bit is to only set the variances and set the covariances to 0. If we do it this way, we do need to adjust how we prepare the data. mvnnorm() returns a matrix that, when converted to a data frame, results in a wide data frame. That is, the DV of each group is stored in separate columns. This is not tidy. We therefore restructure the data to make it long.\n\n\nCode\n# Parameters\nM_control &lt;- 5\nM_treatment &lt;- 4\nSD &lt;- 2\nN &lt;- 40\n\n# Prepare parameters\nmus &lt;- c(M_control, M_treatment)\nSigma &lt;- matrix(\n  nrow = 2, ncol = 2,\n  c(\n    SD^2, 0,\n    0, SD^2\n  )\n)\n\n# Simulate once with empirical = TRUE\nsamples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma, empirical = TRUE)\n\n# Prepare data\ncolnames(samples) &lt;- c(\"control\", \"treatment\")\ndata &lt;- as_tibble(samples)\n\ndata_long &lt;- pivot_longer(data,\n  cols = everything(), names_to = \"condition\",\n  values_to = \"DV\"\n)\n\n\nWe inspect the data with the code from before, substituting data with data_long.\n\n\nCode\n# Calculate a standardized effect size\neffect_size &lt;- cohens_d(DV ~ condition, data = data_long)\n\n# Visualize the data\nggplot(data_long, aes(x = condition, y = DV)) +\n  geom_jitter(width = .2, alpha = .25) +\n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  labs(x = \"Condition\")\n\n\n\n\n\n\n\n\nFigure 3: Two groups visualization (equal variance)\n\n\n\n\n\nWe see a difference between the two conditions with a Cohen’s d of 0.5.\nThis time we run a two sample t-test with equal variances assumed.\n\n\nCode\nt.test(DV ~ condition, data = data_long, var.equal = TRUE)\n\n\nAs before, the power analysis code is as follows:\n\n\nCode\n# Create an empty vector to store p-values in\np_values &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  samples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma)\n\n  # Run test\n  test &lt;- t.test(samples[, 1], samples[, 2], var.equal = TRUE)\n\n  # Extract p-value\n  p_values[i] &lt;- test$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values &lt;= .05) / s * 100\n\n\nThis produces a power of 60.3% with the current parameters.\n\n\n\nA paired t-test is appropriate when we have, for example, data from two groups and we have the same participants in both groups. In other words, the observations belonging to the same participant are likely to be correlated. To calculate power for this scenario, we need to set a correlation parameter. This, in turn, requires that we change the variance-covariance matrix. We need to set the covariances to be equal to the squared standard deviation multiplied by the correlation (remember that a covariance is the standard deviation of one group times the standard deviation of the other group times the correlation between the two).\n\n\nCode\n# Parameters\nM_pre &lt;- 5\nM_post &lt;- 4\nSD &lt;- 2\nN &lt;- 40\nr &lt;- 0.75\n\n# Prepare parameters\nmus &lt;- c(M_pre, M_post)\nSigma &lt;- matrix(\n  ncol = 2, nrow = 2,\n  c(\n    SD^2, SD^2 * r,\n    SD^2 * r, SD^2\n  )\n)\n\n# Simulate once with empirical = TRUE\nsamples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma, empirical = TRUE)\n\n# Prepare data\ncolnames(samples) &lt;- c(\"pre\", \"post\")\ndata &lt;- as_tibble(samples)\n\ndata_long &lt;- pivot_longer(data,\n  cols = everything(),\n  names_to = \"condition\", values_to = \"DV\"\n) %&gt;%\n  mutate(condition = fct_relevel(condition, \"pre\"))\n\n\nLet’s plot the means in each group, with a line between the two points representing the means to signify that this data was measured within-subjects. We also calculate another Cohen’s d to get an impression of the standardized effect size.\n\n\nCode\n# Calculate a standardized effect size\neffect_size &lt;- cohens_d(DV ~ condition, data = data_long, paired = TRUE)\n\n# Visualize the data\nggplot(data_long, aes(x = condition, y = DV, group = 1)) +\n  geom_jitter(width = .2, alpha = .25) +\n  stat_summary(fun = \"mean\", geom = \"line\", linetype = 2) +\n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  labs(x = \"Condition\")\n\n\n\n\n\n\n\n\nFigure 4: Two groups visualization (paired)\n\n\n\n\n\nThe difference between the two groups is equal to a Cohen’s d of 0.71.\nRun the paired t-test with t.test() and set paired to TRUE. I generally favor long data frames, so that’s the data frame I use here to run the paired t-test. In the power analysis, I use the wide version to minimize the code (and speed up the power analysis).\n\n\nCode\nt.test(DV ~ condition, data = data_long, paired = TRUE)\n\n\nThe power analysis for this analysis looks as follows:\n\n\nCode\n# Create an empty vector to store the p-values in\np_values &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  samples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma)\n\n  # Run test\n  test &lt;- t.test(samples[, 1], samples[, 2], paired = TRUE)\n\n  # Extract p-value\n  p_values[i] &lt;- test$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values &lt;= .05) / s * 100\n\n\nThis produces a power of 98.7% with the current parameters.\n\n\n\nTo power for a single correlation, we can actually use most of the code from the previous scenario. The only difference is that we probably don’t care about mean differences, so we can set those to 0. If we also assume equal variances, we only need a total of 4 parameters.\n\n\nCode\n# Parameters\nM &lt;- 0\nSD &lt;- 1\nN &lt;- 40\nr &lt;- 0.5\n\n# Prepare parameters\nmus &lt;- c(M, M)\nSigma &lt;- matrix(\n  ncol = 2, nrow = 2,\n  c(\n    SD^2, SD^2 * r,\n    SD^2 * r, SD^2\n  )\n)\n\n# Simulate once with empirical = TRUE\nsamples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma, empirical = TRUE)\n\n# Prepare data\ncolnames(samples) &lt;- c(\"var1\", \"var2\")\ndata &lt;- as_tibble(samples)\n\n\nThis time, we plot the data with a scatter plot—a suitable graph for displaying the relationship between two numeric variables.\n\n\nCode\n# Visualize the data\nggplot(data, aes(x = var1, y = var2)) +\n  geom_point(alpha = .25) +\n  geom_smooth(method = \"lm\", color = \"black\")\n\n\n\n\n\n\n\n\nFigure 5: Correlation visualization\n\n\n\n\n\nTo perform the statistical test, we run cor.test().\n\n\nCode\ncor.test(data$var1, data$var2)\n\n\nThe power analysis:\n\n\nCode\n# Create an empty vector to store the p-values in\np_values &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  samples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma)\n\n  # Run test\n  test &lt;- cor.test(samples[, 1], samples[, 2])\n\n  # Extract p-value\n  p_values[i] &lt;- test$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values &lt;= .05) / s * 100\n\n\nThis produces a power of 92.7% with the current parameters.\n\n\n\nIt gets more interesting when you have three groups that you want to compare. For example, imagine a study with two control conditions and a treatment condition. You probably want to compare the treatment condition to the two control conditions. What is the appropriate analysis in this case? Well, that probably depends on who you ask. Someone might suggest performing an ANOVA to look at the omnibus test, followed up by something like a Tukey HSD. Or maybe you can do an ANOVA/regression in which you compare the treatment condition to the two control conditions combined, using the proper contrast. Both don’t make sense to me. In the former case, I don’t understand why you would first do an omnibus test if you’re going to follow it up with more specific analyses anyway and in the latter case you run into the problem of not knowing whether your treatment condition differs from both conditions, which you are likely to predict. Instead, I think the best course of action is to just run two t-tests.\nThe big thing to take away from this scenario is that we should power for finding a significant effect on both tests. We don’t power for the ‘design’ of the study or a single analysis. No, our hypotheses our only confirmed if we find significant differences between the treatment condition and both control conditions, which we test with two t-tests.\nLet’s further assume that the variance in the treatment condition is larger than the variance in the control conditions (which is plausible). Let’s also assume some dropout in the treatment condition (also possibly plausible). This means we should test the differences with Welch’s two sample t-tests.\n\n\nCode\n# Parameters\nM_control1 &lt;- 5\nM_control2 &lt;- 5\nM_treatment &lt;- 5.6\nSD_control1 &lt;- 1\nSD_control2 &lt;- 1\nSD_treatment &lt;- 1.3\nN_control1 &lt;- 50\nN_control2 &lt;- 50\nN_treatment &lt;- 40\n\n# Simulate once\ncontrol1 &lt;- mvrnorm(N_control1,\n  mu = M_control1, Sigma = SD_control1^2,\n  empirical = TRUE\n)\ncontrol2 &lt;- mvrnorm(N_control2,\n  mu = M_control2, Sigma = SD_control2^2,\n  empirical = TRUE\n)\ntreatment &lt;- mvrnorm(N_treatment,\n  mu = M_treatment, Sigma = SD_treatment^2,\n  empirical = TRUE\n)\n\n# Prepare data\ncolnames(control1) &lt;- \"DV\"\ncolnames(control2) &lt;- \"DV\"\ncolnames(treatment) &lt;- \"DV\"\n\ncontrol1 &lt;- control1 %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"control 1\")\n\ncontrol2 &lt;- control2 %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"control 2\")\n\ntreatment &lt;- treatment %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"treatment\")\n\ndata &lt;- bind_rows(control1, control2, treatment)\n\n\nWe again inspect the data by visualizing it and calculating standardized effect sizes (two this time; although they are actually identical with the current parameters).\n\n\nCode\n# Calculate standardized effect sizes\neffect_size1 &lt;- cohens_d(DV ~ condition,\n  pooled_sd = FALSE,\n  data = filter(data, condition != \"control 2\")\n)\neffect_size2 &lt;- cohens_d(DV ~ condition,\n  pooled_sd = FALSE,\n  data = filter(data, condition != \"control 1\")\n)\n\n# Visualize the data\nggplot(data, aes(x = condition, y = DV)) +\n  geom_jitter(width = .2, alpha = .25) +\n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  labs(x = \"Condition\")\n\n\n\n\n\n\n\n\nFigure 6: Three groups visualization\n\n\n\n\n\nThe treatment condition differs from the two control conditions with a difference equal to a Cohen’s d of -0.52.\nThe statistical analysis consists of two Welch’s two sample t-tests:\n\n\nCode\nt.test(DV ~ condition, data = filter(data, condition != \"control 1\"))\nt.test(DV ~ condition, data = filter(data, condition != \"control 2\"))\n\n\nThe power analysis is now more interesting because we want to have enough power to find a significant effect on both t-tests. So that means we’ll store the p-values of both tests and then count how often we find a p-value below .05 for both tests.\n\n\nCode\n# Create two empty vectors to store the p-values in\np_values1 &lt;- vector(length = s)\np_values2 &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  control1 &lt;- mvrnorm(N_control1, mu = M_control1, Sigma = SD_control1^2)\n  control2 &lt;- mvrnorm(N_control2, mu = M_control2, Sigma = SD_control2^2)\n  treatment &lt;- mvrnorm(N_treatment, mu = M_treatment, Sigma = SD_treatment^2)\n\n  # Run tests\n  test1 &lt;- t.test(control1[, 1], treatment[, 1])\n  test2 &lt;- t.test(control2[, 1], treatment[, 1])\n\n  # Extract p-values\n  p_values1[i] &lt;- test1$p.value\n  p_values2[i] &lt;- test2$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values1 &lt;= .05 & p_values2 &lt;= .05) / s * 100\n\n\nThe resulting power is 57.9%. Note that this is very different from the power of finding a significant effect of only one of the two tests; which would be equal to a power of 80%. An important lesson to learn here is that with multiple tests, your power may quickly go down, depending on the power for each individual test. You can also calculate the overall power if you know the power of each individual test. If you know you have 80% power for each of two tests, then the overall power will be 80% * 80% = 64%. This only works if your analyses are completely independent, though.\n\n\n\nNext, let’s look at an interaction effect between two categorical predictors in a regression. Say we have a control condition and a treatment condition and we ran the study in the Netherlands and in Germany. With such a design there is the possibility of an interaction effect. Maybe there’s a difference between the control condition and the treatment condition in the Netherlands but not in Germany, or perhaps it is completely reversed, or perhaps only weakened. The exact pattern determines the strength of the interaction effect. If an effect in one condition completely flips in another condition, we have the strongest possible interaction effect (i.e., a crossover interaction). If the effect is merely weaker in one condition rather than another, then we only have a weak interaction effect (i.e., an attenuated interaction effect).\nNot only does the expected pattern of the interaction determine the expected effect size of the interaction, it also affects which analyses you should run. Finding a significant interaction effect does not mean that the interaction effect you found actually matches what you hypothesized. If you expect a crossover interaction, but you only find an attenuated interaction, you’re wrong. And vice versa as well. The issue is more complicated when you expect an interaction in which the effect is present is one condition but absent in another. You then should test whether the effect is indeed absent, which is a bit tricky with frequentist statistics (although see this). Hypothesizing a crossover interaction is probably the easiest. I think you don’t even need to run an interaction test in that case. Instead, you can just run two t-tests and test whether both are significant, with opposite signs.\nIn this scenario, let’s cover what is possibly the most common interaction in psychology—an attenuated interaction with the effect being present in both conditions, but smaller in one than in the other. This means we want a significant difference between the two conditions in each country, as well as a significant interaction effect.\n\n\nCode\n# Parameters\nM_control_NL &lt;- 4\nM_control_DE &lt;- 4\nM_treatment_NL &lt;- 5\nM_treatment_DE &lt;- 6\nSD &lt;- 2\nN &lt;- 40\n\n# Prepare parameters\nmus &lt;- c(M_control_NL, M_control_DE, M_treatment_NL, M_treatment_DE)\nSigma &lt;- matrix(\n  ncol = 4, nrow = 4,\n  c(\n    SD^2, 0, 0, 0,\n    0, SD^2, 0, 0,\n    0, 0, SD^2, 0,\n    0, 0, 0, SD^2\n  )\n)\n\n# Simulate once\nsamples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma, empirical = TRUE)\n\n# Prepare data\ncolnames(samples) &lt;- c(\n  \"control_NL\", \"control_DE\", \"treatment_NL\", \"treatment_DE\"\n)\n\ndata &lt;- samples %&gt;%\n  as_tibble() %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = c(\"condition\", \"country\"),\n    names_sep = \"_\",\n    values_to = \"DV\"\n  )\n\n\nWhen it comes to interaction effects, it’s definitely a good idea to visualize the data. In addition, we calculate the effect size of the difference between the control and treatment condition for each country.\n\n\nCode\n# Calculate effect size per country\neffect_size_NL &lt;- cohens_d(DV ~ condition, data = filter(data, country == \"NL\"))\neffect_size_DE &lt;- cohens_d(DV ~ condition, data = filter(data, country == \"DE\"))\n\n# Visualize the interaction effect\nggplot(data, aes(x = condition, y = DV)) +\n  geom_jitter(width = .2, alpha = .25) +\n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  facet_grid(~country) +\n  labs(x = \"Condition\")\n\n\n\n\n\n\n\n\nFigure 7: 2x2 interaction visualization\n\n\n\n\n\nThe graph shows that the difference between the control and treatment condition indeed seems to be larger in Germany than in the Netherlands. In the Netherlands, the effect size is equal to a Cohen’s d of -0.5. In Germany, it’s -1.\nA regression analysis can be used to test the interaction effect and whether the effect is present in each country. We do need the run the regression twice in order to get the effect of treatment in each country. By default, Germany is the reference category (DE comes before NL). So if we switch the reference category to NL, we get the effect of treatment in the Netherlands.\n\n\nCode\n# Regression with DE as the reference category\nmodel_DE &lt;- lm(DV ~ condition * country, data = data)\nsummary(model_DE)\n\n# Regression with NL as the reference category\ndata &lt;- mutate(data, country = fct_relevel(country, \"NL\"))\n\nmodel_NL &lt;- lm(DV ~ condition * country, data = data)\nsummary(model_NL)\n\n\nOur interest is in the two treatment effects and the interaction effect (which is the same in both models). This means that we want to save 3 p-values in the power analysis.\n\n\nCode\n# Create three empty vectors to store the p-values in\np_values_NL &lt;- vector(length = s)\np_values_DE &lt;- vector(length = s)\np_values_interaction &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  samples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma)\n\n  # Prepare data\n  colnames(samples) &lt;- c(\n    \"control_NL\", \"control_DE\", \"treatment_NL\",\n    \"treatment_DE\"\n  )\n\n  data &lt;- samples %&gt;%\n    as_tibble() %&gt;%\n    pivot_longer(\n      cols = everything(),\n      names_to = c(\"condition\", \"country\"),\n      names_sep = \"_\",\n      values_to = \"DV\"\n    )\n\n  # Run tests\n  model_DE &lt;- lm(DV ~ condition * country, data = data)\n\n  data &lt;- mutate(data, country = fct_relevel(country, \"NL\"))\n  model_NL &lt;- lm(DV ~ condition * country, data = data)\n\n  # Extract p-values\n  model_NL_tidy &lt;- tidy(model_NL)\n  model_DE_tidy &lt;- tidy(model_DE)\n\n  p_values_NL[i] &lt;- model_NL_tidy$p.value[2]\n  p_values_DE[i] &lt;- model_DE_tidy$p.value[2]\n  p_values_interaction[i] &lt;- model_NL_tidy$p.value[4]\n}\n\n# Calculate power\npower &lt;- sum(p_values_NL &lt;= .05 & p_values_DE &lt;= .05 &\n  p_values_interaction &lt;= .05) / s * 100\n\n\nThe overall power for this scenario is 9.6%. If you instead only look at the power of the interaction test, you get a power of 35.1%. The difference shows that it matters whether you follow up your interaction test with the analyses that confirm the exact pattern of the interaction test. Also note that these analyses are not independent, so it’s not straightforward to calculate the overall power. Simulation makes it relatively easy.\n\n\n\nAnother scenario involves having multiple groups (e.g., conditions) and a continuous measure that interacts with the group. In other words, this scenario consists of having different correlations, with the correlation between a measure and an outcome depending on the group.\nWe can simulate a scenario like that by simulating multiple correlations and then merging the data together. In the scenario below, I simulate a correlation of size 0 in one group (i.e., control group) and a correlation of .5 in another group (i.e., treatment group).\n\n\nCode\n# Parameters\nM_outcome &lt;- 4\nSD_outcome &lt;- 1\nM_control &lt;- 4\nSD_control &lt;- 1\nM_treatment &lt;- 4\nSD_treatment &lt;- 1\n\nr_control &lt;- 0.1\nr_treatment &lt;- 0.5\n\nN &lt;- 40\n\n# Prepare parameters\nmus_control &lt;- c(M_control, M_outcome)\nSigma_control &lt;- matrix(\n  ncol = 2, nrow = 2,\n  c(\n    SD_control^2, SD_control * SD_outcome * r_control,\n    SD_control * SD_outcome * r_control, SD_outcome^2\n  )\n)\n\nmus_treatment &lt;- c(M_treatment, M_treatment)\nSigma_treatment &lt;- matrix(\n  ncol = 2, nrow = 2,\n  c(\n    SD_treatment^2, SD_treatment * SD_outcome * r_treatment,\n    SD_treatment * SD_outcome * r_treatment, SD_outcome^2\n  )\n)\n\n# Simulate once with empirical = TRUE\nsamples_control &lt;- mvrnorm(\n  N,\n  mu = mus_control,\n  Sigma = Sigma_control, empirical = TRUE\n)\nsamples_treatment &lt;- mvrnorm(\n  N,\n  mu = mus_treatment,\n  Sigma = Sigma_treatment, empirical = TRUE\n)\n\n# Prepare data\ncolnames(samples_control) &lt;- c(\"measure\", \"outcome\")\ndata_control &lt;- as_tibble(samples_control)\ndata_control &lt;- mutate(data_control, condition = \"Control\")\n\ncolnames(samples_treatment) &lt;- c(\"measure\", \"outcome\")\ndata_treatment &lt;- as_tibble(samples_treatment)\ndata_treatment &lt;- mutate(data_treatment, condition = \"Treatment\")\n\ndata &lt;- bind_rows(data_control, data_treatment)\n\n\nLet’s visualize the simulated data to see whether we indeed observe a correlation in the treatment condition and none in the control condition.\n\n\nCode\nggplot(data, aes(x = measure, y = outcome)) +\n  facet_grid(~condition) +\n  geom_point(alpha = .25) +\n  geom_smooth(method = \"lm\", color = \"black\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 8: 2 (group) x 1 (continuous) interaction visualization\n\n\n\n\n\nLooks correct.\nAnalyzing this data is a bit trickier. To confirm our hypotheses we need to show that: 1. There is no correlation in the Control condition 2. There is a positive correlation in the Treatment condition 3. There is a significant interaction effect.\nThe first one is rather difficult because it’s not straightforward to prove a null using frequentist statistics. We could do an equivalence test of some sort, but I’ll just keep it simple and count the test as successful if we find a non-significant p-value.\nBesides that, this scenario is similar to the previous one. We run two regression models in order to get the relevant p-value. The first model is to obtain the p-value of the slope between the measure and outcome in the control condition, as well as the p-value of the interaction. The second model is to obtain the p-value of the slope in the treatment condition.\n\n\nCode\n# Create three empty vectors to store the p-values in\np_values_control &lt;- vector(length = s)\np_values_treatment &lt;- vector(length = s)\np_values_interaction &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  samples_control &lt;- mvrnorm(N, mu = mus_control, Sigma = Sigma_control)\n  samples_treatment &lt;- mvrnorm(N, mu = mus_treatment, Sigma = Sigma_treatment)\n\n  # Prepare data\n  colnames(samples_control) &lt;- c(\"measure\", \"outcome\")\n  data_control &lt;- as_tibble(samples_control)\n  data_control &lt;- mutate(data_control, condition = \"Control\")\n\n  colnames(samples_treatment) &lt;- c(\"measure\", \"outcome\")\n  data_treatment &lt;- as_tibble(samples_treatment)\n  data_treatment &lt;- mutate(data_treatment, condition = \"Treatment\")\n\n  data &lt;- bind_rows(data_control, data_treatment)\n\n  # Run tests\n  model_control &lt;- lm(outcome ~ condition * measure, data = data)\n  data &lt;- mutate(data, condition = fct_relevel(condition, \"Treatment\"))\n  model_treatment &lt;- lm(outcome ~ condition * measure, data = data)\n\n  # Extract p-values\n  model_control_tidy &lt;- tidy(model_control)\n  model_treatment_tidy &lt;- tidy(model_treatment)\n\n  p_values_control[i] &lt;- model_control_tidy$p.value[3]\n  p_values_treatment[i] &lt;- model_treatment_tidy$p.value[3]\n  p_values_interaction[i] &lt;- model_control_tidy$p.value[4]\n}\n\n# Calculate power\npower &lt;- sum(p_values_control &gt; .05 & p_values_treatment &lt;= .05 &\n  p_values_interaction &lt;= .05) / s * 100\n\n\nThe overall power for this scenario is 44.9%. It matters less now whether we power for the whole set of analyses or just the slope in the treatment condition because the interaction effect is wholly driven by this slope."
  },
  {
    "objectID": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#setup",
    "href": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#setup",
    "title": "Simulation-based power analyses",
    "section": "",
    "text": "Run the following code to get started. The most important package here is MASS. It contains a function called mvrnorm() that enables us to simulate data from a multivariate normal distribution. This means we’ll simulate data for scenarios where we have a continuous outcome. I really like this function for simulating data because it has an argument called empirical that you can set to TRUE, which causes your simulated data to have the exact properties you set (e.g., exactly a mean of 4). This is a great way to check out your simulated data and see if it makes sense.\nWe will use the tidyverse because we need to prepare the data after simulating it. mvrnorm() returns a matrix with each simulated variable as a column. This means we sometimes need to prepare the data so that we can perform the tests we want to run or for visualization purposes.\nThe effectsize package will be used to inspect the data by calculating standardized effect sizes. This will allow us to check whether the parameters are plausible.\nFinally, we sometimes use the broom package to extract p-values from the statistical tests that we’ll run. This will be necessary to calculate the power because power is (usually) nothing more than the number of significant p-values divided by the number of tests we simulated data for. In a future post I might focus on Bayesian analyses, so we won’t be dealing with p-values then, although the logic will be the same.\nBesides loading packages, we also set the s variable. The value of this variable will determine how many times we’ll simulate the data during the power analysis. The higher this number, the more accurate our power estimates will be.\n\n\nCode\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(effectsize)\nlibrary(broom)\nlibrary(viridis)\n\ntheme_set(theme_minimal())\n\noptions(\n  knitr.kable.NA = \"\",\n  digits = 2\n)\n\n# Set number of loops in the power simulations\ns &lt;- 1000\n\n\nWith the setup out of the way, let’s cover our general approach to power analyses:\n\nSimulate the data with fixed properties\nCheck the data to see if the data is plausible\nRun the tests we want to run on the data\nRepeat steps 1 to 3 many times, save the p-values, and calculate power\n\nWe’ll do this for various scenarios. In each scenario we start by defining the parameters. I’ll focus on providing means, standard deviations, and correlations, because those are usually the parameters we report in the results section, so I’m guessing most researchers will have some intuitions about what these parameters mean and whether the results are plausible.\nThe mvrnorm() function requires that we pass it the sample size, the means, and a variance-covariance matrix. The first two are easy to understand, but the variance-covariance may not be. It’s relatively straightforward to convert means, SDs, and correlations to a variance-covariance matrix, though. Variance is simply the standard deviation squared and the covariance is the product of the standard deviations of the two variables and their correlation. You’ll see in some scenarios below that this is how I construct the variance-covariance matrix.\nNote that the result of each power analysis will be the power, and not the sample size needed to obtain a particular power. This is the same as calculating the post-hoc power in G*Power. If you want to figure out what the sample size is for a particular power (e.g., 80%) then you simply change the sample size parameter until you have the power you want."
  },
  {
    "objectID": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#one-sample-t-test",
    "href": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#one-sample-t-test",
    "title": "Simulation-based power analyses",
    "section": "",
    "text": "The simplest scenario is where we want to simulate a set of normally distributed values and perform a one sample t-test. This requires that we set three parameters: a mean, a standard deviation, and a sample size. We give mvrnorm() the sample size (N), the mean (M), and the variance (SD^2). After simulating the data, we give the simulated data a column name and convert the matrix returned by mvrnorm() to a data frame.\n\n\nCode\n# Parameters\nM &lt;- 0.75\nSD &lt;- 5\nN &lt;- 90\n\n# Simulate once with empirical = TRUE\nsamples &lt;- mvrnorm(N, mu = M, Sigma = SD^2, empirical = TRUE)\n\n# Prepare data\ncolnames(samples) &lt;- \"DV\"\ndata &lt;- as_tibble(samples)\n\n\nThe next step is to inspect the data to see whether the parameters are plausible. This can be done by converting the parameters to a standardized effect size and by visualizing the data.\n\n\nCode\n# Calculate a standardized effect size\neffect_size &lt;- cohens_d(data$DV)\n\n# Plot the simulated data\nggplot(data, aes(x = DV)) +\n  geom_histogram(alpha = .5, color = \"gray20\", bins = 30) +\n  geom_vline(xintercept = M, linetype = \"dashed\")\n\n\n\n\n\n\n\n\nFigure 1: One sample visualization\n\n\n\n\n\nThe histogram roughly shows that we have a mean of 0.75 and a standard deviation of 5. We also calculated the Cohen’s d as a measure of the size of the effect. The size of this effect is equal to a Cohen’s d of 0.15.\nNext is the analysis we want to power for—the one-sample t-test. The function for this test is t.test().\n\n\nCode\nt.test(data$DV)\n\n\nTo calculate the power, we repeat the analysis s times. Each time we store the p-value so that later we can calculate the proportion of significant results. Since we don’t need to inspect the data each time, we skip the data preparation step and use the samples returned by mvrnorm() immediately in t.test() using R’s matrix notation (if you want, you can also prepare the data each time, if you find that easier to understand).\n\n\nCode\n# Create a vector to the store p-values in\np_values &lt;- vector(length = s)\n\n# Loop s times\nfor (i in 1:s) {\n  # Simulate\n  samples &lt;- mvrnorm(N, mu = M, Sigma = SD^2)\n\n  # Run test\n  test &lt;- t.test(samples[, 1])\n\n  # Extract p-value\n  p_values[i] &lt;- test$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values &lt;= .05) / s * 100\n\n\nWith the current parameters (N = 90, Cohen’s d = 0.15), we obtain a power of 32.2%. The power is simply how often we find a significant result, divided by the number of times we looped, multiplied by 100 to give a percentage. You can adjust the sample size parameter and re-run the code until you know which sample size gives you the desired power. You might also want to run the loop a few times to see how consistent your results are (if the results are inconsistent, increase the number of loops by increasing the value of s)."
  },
  {
    "objectID": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#welchs-two-sample-t-test",
    "href": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#welchs-two-sample-t-test",
    "title": "Simulation-based power analyses",
    "section": "",
    "text": "The next scenario is one in which there are two groups (e.g., a control condition and a treatment condition) and a single DV. Even in this simple scenario there are already several variations that are important to consider. Do we assume equal variances between groups? Do we assume equal samples sizes? Is the design between or within-subjects? We’ll start with assuming unequal variances between the two groups. This means we’ll run a Welch’s two sample t-test. To make it extra fun, we’ll also simulate unequal sample sizes.\nIf we are interested in a between-subjects design where we assume both unequal variances and samples sizes, we can use the code from the previous scenario and simply run it twice, once for each group. After simulating the data, we convert the simulated matrix of each group to a data frame, add a column indicating the group, and merge the two groups into a single data frame.\n\n\nCode\n# Parameters\nM_control &lt;- 5\nM_treatment &lt;- 4\nSD_control &lt;- 1.5\nSD_treatment &lt;- 3\nN_control &lt;- 50\nN_treatment &lt;- 40\n\n# Simulate once with empirical = TRUE\ncontrol &lt;- mvrnorm(N_control,\n  mu = M_control, Sigma = SD_control^2,\n  empirical = TRUE\n)\ntreatment &lt;- mvrnorm(N_treatment,\n  mu = M_treatment, Sigma = SD_treatment^2,\n  empirical = TRUE\n)\n\n# Prepare data\ncolnames(control) &lt;- \"DV\"\ncolnames(treatment) &lt;- \"DV\"\n\ncontrol &lt;- control %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"control\")\n\ntreatment &lt;- treatment %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"treatment\")\n\ndata &lt;- bind_rows(control, treatment)\n\n\nNext, we inspect the data by calculating a Cohen’s d and visualizing the results.\n\n\nCode\n# Calculate a standardized effect size\neffect_size &lt;- cohens_d(DV ~ condition, data = data, pooled_sd = FALSE)\n\n# Visualize the data\nggplot(data, aes(x = condition, y = DV)) +\n  geom_jitter(width = .2, alpha = .25) +\n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  labs(x = \"Condition\")\n\n\n\n\n\n\n\n\nFigure 2: Two groups visualization (unequal variance)\n\n\n\n\n\nThe difference between the two groups is equal to a Cohen’s d of 0.42.\nTo run a Welch’s two-sample t-test, we again use the t.test() function. R by default does not assume equal variances, so the default is a Welch’s two sample t-test.\n\n\nCode\nt.test(DV ~ condition, data = data)\n\n\nThe power analysis looks as follows:\n\n\nCode\n# Create an empty vector to store the p-values in\np_values &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  control &lt;- mvrnorm(N_control, mu = M_control, Sigma = SD_control^2)\n  treatment &lt;- mvrnorm(N_treatment, mu = M_treatment, Sigma = SD_treatment^2)\n\n  # Run test\n  test &lt;- t.test(control[, 1], treatment[, 1])\n\n  # Extract p-value\n  p_values[i] &lt;- test$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values &lt;= .05) / s * 100\n\n\nThis produces a power of 45.9% with the current parameters."
  },
  {
    "objectID": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#two-sample-t-test",
    "href": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#two-sample-t-test",
    "title": "Simulation-based power analyses",
    "section": "",
    "text": "Instead of assuming unequal variances, we can also assume equal variances and perform a two sample t-test. You could adapt the previous scenario by setting the parameters such that the variance in each group is identical, but let’s do something different in this scenario. In addition, let’s assume that the sample sizes in each group are equal. This means we can simulate the data using a slightly different approach. First, we’ll only need 4 parameters. Second, we don’t need to separately simulate the data for each group. We can instead use a single mvrnorm() call and provide it with the correct variance-covariance matrix. The crucial bit is to only set the variances and set the covariances to 0. If we do it this way, we do need to adjust how we prepare the data. mvnnorm() returns a matrix that, when converted to a data frame, results in a wide data frame. That is, the DV of each group is stored in separate columns. This is not tidy. We therefore restructure the data to make it long.\n\n\nCode\n# Parameters\nM_control &lt;- 5\nM_treatment &lt;- 4\nSD &lt;- 2\nN &lt;- 40\n\n# Prepare parameters\nmus &lt;- c(M_control, M_treatment)\nSigma &lt;- matrix(\n  nrow = 2, ncol = 2,\n  c(\n    SD^2, 0,\n    0, SD^2\n  )\n)\n\n# Simulate once with empirical = TRUE\nsamples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma, empirical = TRUE)\n\n# Prepare data\ncolnames(samples) &lt;- c(\"control\", \"treatment\")\ndata &lt;- as_tibble(samples)\n\ndata_long &lt;- pivot_longer(data,\n  cols = everything(), names_to = \"condition\",\n  values_to = \"DV\"\n)\n\n\nWe inspect the data with the code from before, substituting data with data_long.\n\n\nCode\n# Calculate a standardized effect size\neffect_size &lt;- cohens_d(DV ~ condition, data = data_long)\n\n# Visualize the data\nggplot(data_long, aes(x = condition, y = DV)) +\n  geom_jitter(width = .2, alpha = .25) +\n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  labs(x = \"Condition\")\n\n\n\n\n\n\n\n\nFigure 3: Two groups visualization (equal variance)\n\n\n\n\n\nWe see a difference between the two conditions with a Cohen’s d of 0.5.\nThis time we run a two sample t-test with equal variances assumed.\n\n\nCode\nt.test(DV ~ condition, data = data_long, var.equal = TRUE)\n\n\nAs before, the power analysis code is as follows:\n\n\nCode\n# Create an empty vector to store p-values in\np_values &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  samples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma)\n\n  # Run test\n  test &lt;- t.test(samples[, 1], samples[, 2], var.equal = TRUE)\n\n  # Extract p-value\n  p_values[i] &lt;- test$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values &lt;= .05) / s * 100\n\n\nThis produces a power of 60.3% with the current parameters."
  },
  {
    "objectID": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#paired-t-test",
    "href": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#paired-t-test",
    "title": "Simulation-based power analyses",
    "section": "",
    "text": "A paired t-test is appropriate when we have, for example, data from two groups and we have the same participants in both groups. In other words, the observations belonging to the same participant are likely to be correlated. To calculate power for this scenario, we need to set a correlation parameter. This, in turn, requires that we change the variance-covariance matrix. We need to set the covariances to be equal to the squared standard deviation multiplied by the correlation (remember that a covariance is the standard deviation of one group times the standard deviation of the other group times the correlation between the two).\n\n\nCode\n# Parameters\nM_pre &lt;- 5\nM_post &lt;- 4\nSD &lt;- 2\nN &lt;- 40\nr &lt;- 0.75\n\n# Prepare parameters\nmus &lt;- c(M_pre, M_post)\nSigma &lt;- matrix(\n  ncol = 2, nrow = 2,\n  c(\n    SD^2, SD^2 * r,\n    SD^2 * r, SD^2\n  )\n)\n\n# Simulate once with empirical = TRUE\nsamples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma, empirical = TRUE)\n\n# Prepare data\ncolnames(samples) &lt;- c(\"pre\", \"post\")\ndata &lt;- as_tibble(samples)\n\ndata_long &lt;- pivot_longer(data,\n  cols = everything(),\n  names_to = \"condition\", values_to = \"DV\"\n) %&gt;%\n  mutate(condition = fct_relevel(condition, \"pre\"))\n\n\nLet’s plot the means in each group, with a line between the two points representing the means to signify that this data was measured within-subjects. We also calculate another Cohen’s d to get an impression of the standardized effect size.\n\n\nCode\n# Calculate a standardized effect size\neffect_size &lt;- cohens_d(DV ~ condition, data = data_long, paired = TRUE)\n\n# Visualize the data\nggplot(data_long, aes(x = condition, y = DV, group = 1)) +\n  geom_jitter(width = .2, alpha = .25) +\n  stat_summary(fun = \"mean\", geom = \"line\", linetype = 2) +\n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  labs(x = \"Condition\")\n\n\n\n\n\n\n\n\nFigure 4: Two groups visualization (paired)\n\n\n\n\n\nThe difference between the two groups is equal to a Cohen’s d of 0.71.\nRun the paired t-test with t.test() and set paired to TRUE. I generally favor long data frames, so that’s the data frame I use here to run the paired t-test. In the power analysis, I use the wide version to minimize the code (and speed up the power analysis).\n\n\nCode\nt.test(DV ~ condition, data = data_long, paired = TRUE)\n\n\nThe power analysis for this analysis looks as follows:\n\n\nCode\n# Create an empty vector to store the p-values in\np_values &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  samples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma)\n\n  # Run test\n  test &lt;- t.test(samples[, 1], samples[, 2], paired = TRUE)\n\n  # Extract p-value\n  p_values[i] &lt;- test$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values &lt;= .05) / s * 100\n\n\nThis produces a power of 98.7% with the current parameters."
  },
  {
    "objectID": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#correlation",
    "href": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#correlation",
    "title": "Simulation-based power analyses",
    "section": "",
    "text": "To power for a single correlation, we can actually use most of the code from the previous scenario. The only difference is that we probably don’t care about mean differences, so we can set those to 0. If we also assume equal variances, we only need a total of 4 parameters.\n\n\nCode\n# Parameters\nM &lt;- 0\nSD &lt;- 1\nN &lt;- 40\nr &lt;- 0.5\n\n# Prepare parameters\nmus &lt;- c(M, M)\nSigma &lt;- matrix(\n  ncol = 2, nrow = 2,\n  c(\n    SD^2, SD^2 * r,\n    SD^2 * r, SD^2\n  )\n)\n\n# Simulate once with empirical = TRUE\nsamples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma, empirical = TRUE)\n\n# Prepare data\ncolnames(samples) &lt;- c(\"var1\", \"var2\")\ndata &lt;- as_tibble(samples)\n\n\nThis time, we plot the data with a scatter plot—a suitable graph for displaying the relationship between two numeric variables.\n\n\nCode\n# Visualize the data\nggplot(data, aes(x = var1, y = var2)) +\n  geom_point(alpha = .25) +\n  geom_smooth(method = \"lm\", color = \"black\")\n\n\n\n\n\n\n\n\nFigure 5: Correlation visualization\n\n\n\n\n\nTo perform the statistical test, we run cor.test().\n\n\nCode\ncor.test(data$var1, data$var2)\n\n\nThe power analysis:\n\n\nCode\n# Create an empty vector to store the p-values in\np_values &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  samples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma)\n\n  # Run test\n  test &lt;- cor.test(samples[, 1], samples[, 2])\n\n  # Extract p-value\n  p_values[i] &lt;- test$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values &lt;= .05) / s * 100\n\n\nThis produces a power of 92.7% with the current parameters."
  },
  {
    "objectID": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#t-tests",
    "href": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#t-tests",
    "title": "Simulation-based power analyses",
    "section": "",
    "text": "It gets more interesting when you have three groups that you want to compare. For example, imagine a study with two control conditions and a treatment condition. You probably want to compare the treatment condition to the two control conditions. What is the appropriate analysis in this case? Well, that probably depends on who you ask. Someone might suggest performing an ANOVA to look at the omnibus test, followed up by something like a Tukey HSD. Or maybe you can do an ANOVA/regression in which you compare the treatment condition to the two control conditions combined, using the proper contrast. Both don’t make sense to me. In the former case, I don’t understand why you would first do an omnibus test if you’re going to follow it up with more specific analyses anyway and in the latter case you run into the problem of not knowing whether your treatment condition differs from both conditions, which you are likely to predict. Instead, I think the best course of action is to just run two t-tests.\nThe big thing to take away from this scenario is that we should power for finding a significant effect on both tests. We don’t power for the ‘design’ of the study or a single analysis. No, our hypotheses our only confirmed if we find significant differences between the treatment condition and both control conditions, which we test with two t-tests.\nLet’s further assume that the variance in the treatment condition is larger than the variance in the control conditions (which is plausible). Let’s also assume some dropout in the treatment condition (also possibly plausible). This means we should test the differences with Welch’s two sample t-tests.\n\n\nCode\n# Parameters\nM_control1 &lt;- 5\nM_control2 &lt;- 5\nM_treatment &lt;- 5.6\nSD_control1 &lt;- 1\nSD_control2 &lt;- 1\nSD_treatment &lt;- 1.3\nN_control1 &lt;- 50\nN_control2 &lt;- 50\nN_treatment &lt;- 40\n\n# Simulate once\ncontrol1 &lt;- mvrnorm(N_control1,\n  mu = M_control1, Sigma = SD_control1^2,\n  empirical = TRUE\n)\ncontrol2 &lt;- mvrnorm(N_control2,\n  mu = M_control2, Sigma = SD_control2^2,\n  empirical = TRUE\n)\ntreatment &lt;- mvrnorm(N_treatment,\n  mu = M_treatment, Sigma = SD_treatment^2,\n  empirical = TRUE\n)\n\n# Prepare data\ncolnames(control1) &lt;- \"DV\"\ncolnames(control2) &lt;- \"DV\"\ncolnames(treatment) &lt;- \"DV\"\n\ncontrol1 &lt;- control1 %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"control 1\")\n\ncontrol2 &lt;- control2 %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"control 2\")\n\ntreatment &lt;- treatment %&gt;%\n  as_tibble() %&gt;%\n  mutate(condition = \"treatment\")\n\ndata &lt;- bind_rows(control1, control2, treatment)\n\n\nWe again inspect the data by visualizing it and calculating standardized effect sizes (two this time; although they are actually identical with the current parameters).\n\n\nCode\n# Calculate standardized effect sizes\neffect_size1 &lt;- cohens_d(DV ~ condition,\n  pooled_sd = FALSE,\n  data = filter(data, condition != \"control 2\")\n)\neffect_size2 &lt;- cohens_d(DV ~ condition,\n  pooled_sd = FALSE,\n  data = filter(data, condition != \"control 1\")\n)\n\n# Visualize the data\nggplot(data, aes(x = condition, y = DV)) +\n  geom_jitter(width = .2, alpha = .25) +\n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  labs(x = \"Condition\")\n\n\n\n\n\n\n\n\nFigure 6: Three groups visualization\n\n\n\n\n\nThe treatment condition differs from the two control conditions with a difference equal to a Cohen’s d of -0.52.\nThe statistical analysis consists of two Welch’s two sample t-tests:\n\n\nCode\nt.test(DV ~ condition, data = filter(data, condition != \"control 1\"))\nt.test(DV ~ condition, data = filter(data, condition != \"control 2\"))\n\n\nThe power analysis is now more interesting because we want to have enough power to find a significant effect on both t-tests. So that means we’ll store the p-values of both tests and then count how often we find a p-value below .05 for both tests.\n\n\nCode\n# Create two empty vectors to store the p-values in\np_values1 &lt;- vector(length = s)\np_values2 &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  control1 &lt;- mvrnorm(N_control1, mu = M_control1, Sigma = SD_control1^2)\n  control2 &lt;- mvrnorm(N_control2, mu = M_control2, Sigma = SD_control2^2)\n  treatment &lt;- mvrnorm(N_treatment, mu = M_treatment, Sigma = SD_treatment^2)\n\n  # Run tests\n  test1 &lt;- t.test(control1[, 1], treatment[, 1])\n  test2 &lt;- t.test(control2[, 1], treatment[, 1])\n\n  # Extract p-values\n  p_values1[i] &lt;- test1$p.value\n  p_values2[i] &lt;- test2$p.value\n}\n\n# Calculate power\npower &lt;- sum(p_values1 &lt;= .05 & p_values2 &lt;= .05) / s * 100\n\n\nThe resulting power is 57.9%. Note that this is very different from the power of finding a significant effect of only one of the two tests; which would be equal to a power of 80%. An important lesson to learn here is that with multiple tests, your power may quickly go down, depending on the power for each individual test. You can also calculate the overall power if you know the power of each individual test. If you know you have 80% power for each of two tests, then the overall power will be 80% * 80% = 64%. This only works if your analyses are completely independent, though."
  },
  {
    "objectID": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#regression-2-x-2-interaction",
    "href": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#regression-2-x-2-interaction",
    "title": "Simulation-based power analyses",
    "section": "",
    "text": "Next, let’s look at an interaction effect between two categorical predictors in a regression. Say we have a control condition and a treatment condition and we ran the study in the Netherlands and in Germany. With such a design there is the possibility of an interaction effect. Maybe there’s a difference between the control condition and the treatment condition in the Netherlands but not in Germany, or perhaps it is completely reversed, or perhaps only weakened. The exact pattern determines the strength of the interaction effect. If an effect in one condition completely flips in another condition, we have the strongest possible interaction effect (i.e., a crossover interaction). If the effect is merely weaker in one condition rather than another, then we only have a weak interaction effect (i.e., an attenuated interaction effect).\nNot only does the expected pattern of the interaction determine the expected effect size of the interaction, it also affects which analyses you should run. Finding a significant interaction effect does not mean that the interaction effect you found actually matches what you hypothesized. If you expect a crossover interaction, but you only find an attenuated interaction, you’re wrong. And vice versa as well. The issue is more complicated when you expect an interaction in which the effect is present is one condition but absent in another. You then should test whether the effect is indeed absent, which is a bit tricky with frequentist statistics (although see this). Hypothesizing a crossover interaction is probably the easiest. I think you don’t even need to run an interaction test in that case. Instead, you can just run two t-tests and test whether both are significant, with opposite signs.\nIn this scenario, let’s cover what is possibly the most common interaction in psychology—an attenuated interaction with the effect being present in both conditions, but smaller in one than in the other. This means we want a significant difference between the two conditions in each country, as well as a significant interaction effect.\n\n\nCode\n# Parameters\nM_control_NL &lt;- 4\nM_control_DE &lt;- 4\nM_treatment_NL &lt;- 5\nM_treatment_DE &lt;- 6\nSD &lt;- 2\nN &lt;- 40\n\n# Prepare parameters\nmus &lt;- c(M_control_NL, M_control_DE, M_treatment_NL, M_treatment_DE)\nSigma &lt;- matrix(\n  ncol = 4, nrow = 4,\n  c(\n    SD^2, 0, 0, 0,\n    0, SD^2, 0, 0,\n    0, 0, SD^2, 0,\n    0, 0, 0, SD^2\n  )\n)\n\n# Simulate once\nsamples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma, empirical = TRUE)\n\n# Prepare data\ncolnames(samples) &lt;- c(\n  \"control_NL\", \"control_DE\", \"treatment_NL\", \"treatment_DE\"\n)\n\ndata &lt;- samples %&gt;%\n  as_tibble() %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = c(\"condition\", \"country\"),\n    names_sep = \"_\",\n    values_to = \"DV\"\n  )\n\n\nWhen it comes to interaction effects, it’s definitely a good idea to visualize the data. In addition, we calculate the effect size of the difference between the control and treatment condition for each country.\n\n\nCode\n# Calculate effect size per country\neffect_size_NL &lt;- cohens_d(DV ~ condition, data = filter(data, country == \"NL\"))\neffect_size_DE &lt;- cohens_d(DV ~ condition, data = filter(data, country == \"DE\"))\n\n# Visualize the interaction effect\nggplot(data, aes(x = condition, y = DV)) +\n  geom_jitter(width = .2, alpha = .25) +\n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  facet_grid(~country) +\n  labs(x = \"Condition\")\n\n\n\n\n\n\n\n\nFigure 7: 2x2 interaction visualization\n\n\n\n\n\nThe graph shows that the difference between the control and treatment condition indeed seems to be larger in Germany than in the Netherlands. In the Netherlands, the effect size is equal to a Cohen’s d of -0.5. In Germany, it’s -1.\nA regression analysis can be used to test the interaction effect and whether the effect is present in each country. We do need the run the regression twice in order to get the effect of treatment in each country. By default, Germany is the reference category (DE comes before NL). So if we switch the reference category to NL, we get the effect of treatment in the Netherlands.\n\n\nCode\n# Regression with DE as the reference category\nmodel_DE &lt;- lm(DV ~ condition * country, data = data)\nsummary(model_DE)\n\n# Regression with NL as the reference category\ndata &lt;- mutate(data, country = fct_relevel(country, \"NL\"))\n\nmodel_NL &lt;- lm(DV ~ condition * country, data = data)\nsummary(model_NL)\n\n\nOur interest is in the two treatment effects and the interaction effect (which is the same in both models). This means that we want to save 3 p-values in the power analysis.\n\n\nCode\n# Create three empty vectors to store the p-values in\np_values_NL &lt;- vector(length = s)\np_values_DE &lt;- vector(length = s)\np_values_interaction &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  samples &lt;- mvrnorm(N, mu = mus, Sigma = Sigma)\n\n  # Prepare data\n  colnames(samples) &lt;- c(\n    \"control_NL\", \"control_DE\", \"treatment_NL\",\n    \"treatment_DE\"\n  )\n\n  data &lt;- samples %&gt;%\n    as_tibble() %&gt;%\n    pivot_longer(\n      cols = everything(),\n      names_to = c(\"condition\", \"country\"),\n      names_sep = \"_\",\n      values_to = \"DV\"\n    )\n\n  # Run tests\n  model_DE &lt;- lm(DV ~ condition * country, data = data)\n\n  data &lt;- mutate(data, country = fct_relevel(country, \"NL\"))\n  model_NL &lt;- lm(DV ~ condition * country, data = data)\n\n  # Extract p-values\n  model_NL_tidy &lt;- tidy(model_NL)\n  model_DE_tidy &lt;- tidy(model_DE)\n\n  p_values_NL[i] &lt;- model_NL_tidy$p.value[2]\n  p_values_DE[i] &lt;- model_DE_tidy$p.value[2]\n  p_values_interaction[i] &lt;- model_NL_tidy$p.value[4]\n}\n\n# Calculate power\npower &lt;- sum(p_values_NL &lt;= .05 & p_values_DE &lt;= .05 &\n  p_values_interaction &lt;= .05) / s * 100\n\n\nThe overall power for this scenario is 9.6%. If you instead only look at the power of the interaction test, you get a power of 35.1%. The difference shows that it matters whether you follow up your interaction test with the analyses that confirm the exact pattern of the interaction test. Also note that these analyses are not independent, so it’s not straightforward to calculate the overall power. Simulation makes it relatively easy."
  },
  {
    "objectID": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#regression-2-groups-1-continuous-interaction",
    "href": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html#regression-2-groups-1-continuous-interaction",
    "title": "Simulation-based power analyses",
    "section": "",
    "text": "Another scenario involves having multiple groups (e.g., conditions) and a continuous measure that interacts with the group. In other words, this scenario consists of having different correlations, with the correlation between a measure and an outcome depending on the group.\nWe can simulate a scenario like that by simulating multiple correlations and then merging the data together. In the scenario below, I simulate a correlation of size 0 in one group (i.e., control group) and a correlation of .5 in another group (i.e., treatment group).\n\n\nCode\n# Parameters\nM_outcome &lt;- 4\nSD_outcome &lt;- 1\nM_control &lt;- 4\nSD_control &lt;- 1\nM_treatment &lt;- 4\nSD_treatment &lt;- 1\n\nr_control &lt;- 0.1\nr_treatment &lt;- 0.5\n\nN &lt;- 40\n\n# Prepare parameters\nmus_control &lt;- c(M_control, M_outcome)\nSigma_control &lt;- matrix(\n  ncol = 2, nrow = 2,\n  c(\n    SD_control^2, SD_control * SD_outcome * r_control,\n    SD_control * SD_outcome * r_control, SD_outcome^2\n  )\n)\n\nmus_treatment &lt;- c(M_treatment, M_treatment)\nSigma_treatment &lt;- matrix(\n  ncol = 2, nrow = 2,\n  c(\n    SD_treatment^2, SD_treatment * SD_outcome * r_treatment,\n    SD_treatment * SD_outcome * r_treatment, SD_outcome^2\n  )\n)\n\n# Simulate once with empirical = TRUE\nsamples_control &lt;- mvrnorm(\n  N,\n  mu = mus_control,\n  Sigma = Sigma_control, empirical = TRUE\n)\nsamples_treatment &lt;- mvrnorm(\n  N,\n  mu = mus_treatment,\n  Sigma = Sigma_treatment, empirical = TRUE\n)\n\n# Prepare data\ncolnames(samples_control) &lt;- c(\"measure\", \"outcome\")\ndata_control &lt;- as_tibble(samples_control)\ndata_control &lt;- mutate(data_control, condition = \"Control\")\n\ncolnames(samples_treatment) &lt;- c(\"measure\", \"outcome\")\ndata_treatment &lt;- as_tibble(samples_treatment)\ndata_treatment &lt;- mutate(data_treatment, condition = \"Treatment\")\n\ndata &lt;- bind_rows(data_control, data_treatment)\n\n\nLet’s visualize the simulated data to see whether we indeed observe a correlation in the treatment condition and none in the control condition.\n\n\nCode\nggplot(data, aes(x = measure, y = outcome)) +\n  facet_grid(~condition) +\n  geom_point(alpha = .25) +\n  geom_smooth(method = \"lm\", color = \"black\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 8: 2 (group) x 1 (continuous) interaction visualization\n\n\n\n\n\nLooks correct.\nAnalyzing this data is a bit trickier. To confirm our hypotheses we need to show that: 1. There is no correlation in the Control condition 2. There is a positive correlation in the Treatment condition 3. There is a significant interaction effect.\nThe first one is rather difficult because it’s not straightforward to prove a null using frequentist statistics. We could do an equivalence test of some sort, but I’ll just keep it simple and count the test as successful if we find a non-significant p-value.\nBesides that, this scenario is similar to the previous one. We run two regression models in order to get the relevant p-value. The first model is to obtain the p-value of the slope between the measure and outcome in the control condition, as well as the p-value of the interaction. The second model is to obtain the p-value of the slope in the treatment condition.\n\n\nCode\n# Create three empty vectors to store the p-values in\np_values_control &lt;- vector(length = s)\np_values_treatment &lt;- vector(length = s)\np_values_interaction &lt;- vector(length = s)\n\n# Loop\nfor (i in 1:s) {\n  # Simulate\n  samples_control &lt;- mvrnorm(N, mu = mus_control, Sigma = Sigma_control)\n  samples_treatment &lt;- mvrnorm(N, mu = mus_treatment, Sigma = Sigma_treatment)\n\n  # Prepare data\n  colnames(samples_control) &lt;- c(\"measure\", \"outcome\")\n  data_control &lt;- as_tibble(samples_control)\n  data_control &lt;- mutate(data_control, condition = \"Control\")\n\n  colnames(samples_treatment) &lt;- c(\"measure\", \"outcome\")\n  data_treatment &lt;- as_tibble(samples_treatment)\n  data_treatment &lt;- mutate(data_treatment, condition = \"Treatment\")\n\n  data &lt;- bind_rows(data_control, data_treatment)\n\n  # Run tests\n  model_control &lt;- lm(outcome ~ condition * measure, data = data)\n  data &lt;- mutate(data, condition = fct_relevel(condition, \"Treatment\"))\n  model_treatment &lt;- lm(outcome ~ condition * measure, data = data)\n\n  # Extract p-values\n  model_control_tidy &lt;- tidy(model_control)\n  model_treatment_tidy &lt;- tidy(model_treatment)\n\n  p_values_control[i] &lt;- model_control_tidy$p.value[3]\n  p_values_treatment[i] &lt;- model_treatment_tidy$p.value[3]\n  p_values_interaction[i] &lt;- model_control_tidy$p.value[4]\n}\n\n# Calculate power\npower &lt;- sum(p_values_control &gt; .05 & p_values_treatment &lt;= .05 &\n  p_values_interaction &lt;= .05) / s * 100\n\n\nThe overall power for this scenario is 44.9%. It matters less now whether we power for the whole set of analyses or just the slope in the treatment condition because the interaction effect is wholly driven by this slope."
  },
  {
    "objectID": "content/posts/37-format-numbers/format-numbers.html",
    "href": "content/posts/37-format-numbers/format-numbers.html",
    "title": "Formatting numbers",
    "section": "",
    "text": "This blog post covers the function I use to format numbers. It’s kinda work-in-progress / test place for my function because I know I’m not happy with it yet and, as you can see below, I’ll show what output it produces for many different numbers, so I can also use it to see if it does what I want it to do.\nRun the following setup code if you want to follow along.\nCode\nlibrary(tidyverse)"
  },
  {
    "objectID": "content/posts/37-format-numbers/format-numbers.html#the-function",
    "href": "content/posts/37-format-numbers/format-numbers.html#the-function",
    "title": "Formatting numbers",
    "section": "The function",
    "text": "The function\nBelow is the function I use. It has 4 arguments:\n\nx: The number to format\ntrailing_digits: This is a bit of a weird one. It refers to how many decimals you want, excluding leading zeroes in the decimal part of the number. I often want a number like 0.002121 to be rounded to 0.0021 instead of 0.00, so instead of simply referring to the number of digits, I call it trailing digits.\ndigits: This one forces the number of decimals, so setting this to 2 will always show (only) 2 digits\nomit_zero: For numbers that can only range from -1 to 1, the leading zero is often non-informative, so this argument removes that leading zero.\n\n\n\nCode\nformat_number &lt;- function(\n    x,\n    trailing_digits = 2,\n    digits = NULL,\n    omit_zero = FALSE) {\n  # Convert the number to a string to use regex to extract different parts\n  # of the number (e.g., decimals)\n  string &lt;- as.character(x)\n\n  # Extract and count decimals\n  decimals &lt;- stringr::str_extract(string, \"(?&lt;=\\\\.).+\")\n  decimals_n &lt;- dplyr::if_else(is.na(decimals), 0, stringr::str_length(decimals))\n\n  # Count number of leading zeroes in the decimals\n  zeroes &lt;- stringr::str_extract(string, \"(?&lt;=\\\\.)0+\")\n  zeroes_n &lt;- dplyr::if_else(is.na(zeroes), 0, stringr::str_length(zeroes))\n\n  # If digits are set, it overrules trailing digits\n  if (!is.null(digits)) {\n    output &lt;- format(round(x, digits), nsmall = digits)\n  } else {\n    trailing_digits &lt;- ifelse(decimals_n &gt; trailing_digits, trailing_digits + zeroes_n, trailing_digits)\n    output &lt;- formatC(\n      round(x, digits = trailing_digits),\n      digits = trailing_digits, format = \"f\"\n    )\n  }\n\n  # Remove leading zero\n  if (omit_zero) output &lt;- stringr::str_remove(output, \"^0\")\n\n  return(output)\n}"
  },
  {
    "objectID": "content/posts/37-format-numbers/format-numbers.html#testing-it-out",
    "href": "content/posts/37-format-numbers/format-numbers.html#testing-it-out",
    "title": "Formatting numbers",
    "section": "Testing it out",
    "text": "Testing it out\nThe table below shows what kind of output is produced by the function depending on its input. It serves as an illustration of how the function works and as a test for me to see whether the function does what I want it to do.\n\n\n\n\n\n\n\n\n\n\nNumber\ntrailing_digits\ndigits\nomit_zero\nResult\n\n\n\n\n2\n0\n-\nFALSE\n2\n\n\n2\n2\n-\nFALSE\n2.00\n\n\n2\n-\n2\nFALSE\n2.00\n\n\n2.0\n2\n-\nFALSE\n2.00\n\n\n2.01\n1\n-\nFALSE\n2.01\n\n\n2.01\n-\n1\nFALSE\n2.0\n\n\n2.01\n2\n-\nFALSE\n2.01\n\n\n2.013232\n2\n-\nFALSE\n2.013\n\n\n12.00013232\n2\n-\nFALSE\n12.00013\n\n\n12.01003232\n2\n-\nFALSE\n12.010\n\n\n12.00120032\n2\n-\nFALSE\n12.0012\n\n\n0.434\n2\n-\nTRUE\n.43\n\n\n1.434\n2\n-\nFALSE\n0.43"
  },
  {
    "objectID": "content/posts/37-format-numbers/format-numbers.html#limitations",
    "href": "content/posts/37-format-numbers/format-numbers.html#limitations",
    "title": "Formatting numbers",
    "section": "Limitations",
    "text": "Limitations\nThe function isn’t complete yet. For example, I should also add something to deal with very large or very small numbers. The function is also not vectorized (i.e., it only accepts a single number), so there’s still room for improvement. I’ll update the post when I make adjustments to the function.\nThis post was last updated on 2024-05-17."
  },
  {
    "objectID": "content/posts/6-a-tidystats-example/a-tidystats-example.html",
    "href": "content/posts/6-a-tidystats-example/a-tidystats-example.html",
    "title": "A tidystats example",
    "section": "",
    "text": "Lorge and Curtiss (1936) examined how a quotation is perceived when it is attributed to a liked or disliked individual. In one condition the quotation was attributed to Thomas Jefferson and in the other it was attributed to Vladimir Lenin. They found that people agree more with the quotation when the quotation was attributed to Jefferson than Lenin. In the Many Labs replication study (Klein et al., 2014), the quotation was attributed to either George Washington, the liked individual, or Osama Bin Laden, the disliked individual. The also used a different quotation, which was:\n\nI have sworn to only live free, even if I find bitter the taste of death.\n\nWe are again interested in testing whether the source of the quotation affects how it is evaluated. The evaluation was assessed on a 9-point Likert scale ranging from 1 (strongly agree) to 9 (strongly disagree). I reverse coded this in the data that we’ll use. You can follow along by copy-pasting the code from this example.\nBefore getting into how tidystats should be used, let’s first simply analyze the data. I have designed tidystats to be minimally invasive. In other words, to use tidystats, you do not need to substantially change your data analysis workflow.\nWe’ll start with a basic setup where we load some packages and the data.\n\n\nCode\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(tidystats)\nlibrary(viridis)\nlibrary(knitr)\n\n# Load example data\ndata &lt;- data(quote_source)\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n# Set options\noptions(\n  knitr.kable.NA = \"-\",\n  digits = 2\n)\n\n\nOur main effect of interest is the difference in responses to the quote between the two conditions. Here I visualize this different with a violin plot.\n\n\nCode\nggplot(quote_source, aes(x = source, y = response, fill = source)) +\n  geom_violin(width = .5, alpha = .85) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  labs(x = \"Quote source\", y = \"Quote agreement\") +\n  scale_y_continuous(breaks = c(1, 3, 5, 7, 9)) +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\nFigure 1: Difference in responses between the two conditions\n\n\n\n\n\nThis looks like the effect is in the expected direction. Participants agreed more with the quotation when they believed the quote to be from George Washington compared to Osama Bin Laden.\nRegarding descriptives, tidystats comes with its own functions to calculate descriptives. One of them is the describe_data function, inspired by the describe function from the psych package. You can use it together with group_by from the dplyr package to calculate a set of descriptives for multiple groups.\n\n\nCode\nquote_source %&gt;%\n  group_by(source) %&gt;%\n  describe_data(response) %&gt;%\n  select(-var) %&gt;%\n  kable(col.names = c(\"Source\", \"Missing\", \"N\", \"M\", \"SD\", \"SE\", \"Min\", \"Max\", \n    \"Range\", \"Median\", \"Mode\", \"Skew\", \"Kurtosis\")) \n\n\n\n\nTable 1: Response descriptives\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nMissing\nN\nM\nSD\nSE\nMin\nMax\nRange\nMedian\nMode\nSkew\nKurtosis\n\n\n\n\nBin Laden\n18\n3083\n5.2\n2.1\n0.04\n1\n9\n8\n5\n5\n-0.08\n2.6\n\n\nWashington\n0\n3242\n5.9\n2.2\n0.04\n1\n9\n8\n6\n5\n-0.23\n2.2\n\n\n\n\n\n\n\n\nTo test whether the difference in agreement between the two sources is statistically significant, we perform a t-test. Normally, we would just run the t-test like so:\n\n\nCode\nt.test(response ~ source, data = quote_source)\n\n\nHowever, since we want to use tidystats to later save the statistics from this test, we will store the output of the t-test in a variable. This, and the final section of R code, will be the only thing you need to change in order to incorporate tidystats in your workflow.\nOnce you’ve stored the result of the t-test in a variable, you can look at the output by sending it the console, which will print the output.\n\n\nCode\nmain_test &lt;- t.test(response ~ source, data = quote_source)\nmain_test\n\n\n\n    Welch Two Sample t-test\n\ndata:  response by source\nt = -13, df = 6323, p-value &lt;2e-16\nalternative hypothesis: true difference in means between group Bin Laden and group Washington is not equal to 0\n95 percent confidence interval:\n -0.80 -0.59\nsample estimates:\n mean in group Bin Laden mean in group Washington \n                     5.2                      5.9 \n\n\nThis shows us that there is a statistically significant effect of the quote source, consistent with the hypothesis.\nNext, let’s run some additional analyses. One thing we can test is whether the effect is stronger in the US compared to non-US countries. To test this, we perform a regression analysis. Here we also store the result in a variable, but this is actually quite common in regression analyses because you want to apply the summary function to this variable in order to obtain the inferential statistics.\n\n\nCode\nus_moderation_test &lt;- lm(response ~ source * us_or_international, \n  data = quote_source)\nsummary(us_moderation_test)\n\n\n\nCall:\nlm(formula = response ~ source * us_or_international, data = quote_source)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.005 -1.228 -0.228  1.772  3.772 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                              5.2488     0.0849   61.83  &lt; 2e-16 ***\nsourceWashington                         0.4052     0.1172    3.46  0.00055 ***\nus_or_internationalUS                   -0.0210     0.0955   -0.22  0.82589    \nsourceWashington:us_or_internationalUS   0.3717     0.1323    2.81  0.00497 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.2 on 6321 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.0275,    Adjusted R-squared:  0.027 \nF-statistic: 59.5 on 3 and 6321 DF,  p-value: &lt;2e-16\n\n\nThere appears to be a significant interaction. Let’s inspect the interaction with a graph:\n\n\nCode\nggplot(quote_source, aes(x = us_or_international, y = response, \n    fill = source)) +\n  geom_violin(alpha = .85) +\n  stat_summary(fun.data = \"mean_cl_boot\", position = position_dodge(.9)) +\n  labs(x = \"Region\", y = \"Quote agreement\", fill = \"Source\") +\n  scale_y_continuous(breaks = c(1, 3, 5, 7, 9)) +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\n\n\n\n\n\n\nFigure 2: Source by region interaction\n\n\n\n\n\nWe see that the effect of the source appears to be larger in the US. Given that the positive source was George Washington, this makes sense.\nLet’s do one more analysis to see whether the effect is stronger in a lab setting compared to an online setting.\n\n\nCode\nlab_moderation_test &lt;- lm(response ~ source * lab_or_online, data = quote_source)\nsummary(lab_moderation_test)\n\n\n\nCall:\nlm(formula = response ~ source * lab_or_online, data = quote_source)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.967 -1.197 -0.197  1.737  3.803 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                            5.1971     0.0567   91.59   &lt;2e-16 ***\nsourceWashington                       0.6864     0.0791    8.68   &lt;2e-16 ***\nlab_or_onlineonline                    0.0664     0.0780    0.85     0.39    \nsourceWashington:lab_or_onlineonline   0.0172     0.1089    0.16     0.87    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.2 on 6321 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.0255,    Adjusted R-squared:  0.025 \nF-statistic: 55.1 on 3 and 6321 DF,  p-value: &lt;2e-16\n\n\nWe see no significant interaction in this case. This means we do not find evidence that running the study in an online setting significantly weakens the effect; good to know!"
  },
  {
    "objectID": "content/posts/6-a-tidystats-example/a-tidystats-example.html#analyzing-the-data",
    "href": "content/posts/6-a-tidystats-example/a-tidystats-example.html#analyzing-the-data",
    "title": "A tidystats example",
    "section": "",
    "text": "Lorge and Curtiss (1936) examined how a quotation is perceived when it is attributed to a liked or disliked individual. In one condition the quotation was attributed to Thomas Jefferson and in the other it was attributed to Vladimir Lenin. They found that people agree more with the quotation when the quotation was attributed to Jefferson than Lenin. In the Many Labs replication study (Klein et al., 2014), the quotation was attributed to either George Washington, the liked individual, or Osama Bin Laden, the disliked individual. The also used a different quotation, which was:\n\nI have sworn to only live free, even if I find bitter the taste of death.\n\nWe are again interested in testing whether the source of the quotation affects how it is evaluated. The evaluation was assessed on a 9-point Likert scale ranging from 1 (strongly agree) to 9 (strongly disagree). I reverse coded this in the data that we’ll use. You can follow along by copy-pasting the code from this example.\nBefore getting into how tidystats should be used, let’s first simply analyze the data. I have designed tidystats to be minimally invasive. In other words, to use tidystats, you do not need to substantially change your data analysis workflow.\nWe’ll start with a basic setup where we load some packages and the data.\n\n\nCode\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(tidystats)\nlibrary(viridis)\nlibrary(knitr)\n\n# Load example data\ndata &lt;- data(quote_source)\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n# Set options\noptions(\n  knitr.kable.NA = \"-\",\n  digits = 2\n)\n\n\nOur main effect of interest is the difference in responses to the quote between the two conditions. Here I visualize this different with a violin plot.\n\n\nCode\nggplot(quote_source, aes(x = source, y = response, fill = source)) +\n  geom_violin(width = .5, alpha = .85) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  labs(x = \"Quote source\", y = \"Quote agreement\") +\n  scale_y_continuous(breaks = c(1, 3, 5, 7, 9)) +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\nFigure 1: Difference in responses between the two conditions\n\n\n\n\n\nThis looks like the effect is in the expected direction. Participants agreed more with the quotation when they believed the quote to be from George Washington compared to Osama Bin Laden.\nRegarding descriptives, tidystats comes with its own functions to calculate descriptives. One of them is the describe_data function, inspired by the describe function from the psych package. You can use it together with group_by from the dplyr package to calculate a set of descriptives for multiple groups.\n\n\nCode\nquote_source %&gt;%\n  group_by(source) %&gt;%\n  describe_data(response) %&gt;%\n  select(-var) %&gt;%\n  kable(col.names = c(\"Source\", \"Missing\", \"N\", \"M\", \"SD\", \"SE\", \"Min\", \"Max\", \n    \"Range\", \"Median\", \"Mode\", \"Skew\", \"Kurtosis\")) \n\n\n\n\nTable 1: Response descriptives\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nMissing\nN\nM\nSD\nSE\nMin\nMax\nRange\nMedian\nMode\nSkew\nKurtosis\n\n\n\n\nBin Laden\n18\n3083\n5.2\n2.1\n0.04\n1\n9\n8\n5\n5\n-0.08\n2.6\n\n\nWashington\n0\n3242\n5.9\n2.2\n0.04\n1\n9\n8\n6\n5\n-0.23\n2.2\n\n\n\n\n\n\n\n\nTo test whether the difference in agreement between the two sources is statistically significant, we perform a t-test. Normally, we would just run the t-test like so:\n\n\nCode\nt.test(response ~ source, data = quote_source)\n\n\nHowever, since we want to use tidystats to later save the statistics from this test, we will store the output of the t-test in a variable. This, and the final section of R code, will be the only thing you need to change in order to incorporate tidystats in your workflow.\nOnce you’ve stored the result of the t-test in a variable, you can look at the output by sending it the console, which will print the output.\n\n\nCode\nmain_test &lt;- t.test(response ~ source, data = quote_source)\nmain_test\n\n\n\n    Welch Two Sample t-test\n\ndata:  response by source\nt = -13, df = 6323, p-value &lt;2e-16\nalternative hypothesis: true difference in means between group Bin Laden and group Washington is not equal to 0\n95 percent confidence interval:\n -0.80 -0.59\nsample estimates:\n mean in group Bin Laden mean in group Washington \n                     5.2                      5.9 \n\n\nThis shows us that there is a statistically significant effect of the quote source, consistent with the hypothesis.\nNext, let’s run some additional analyses. One thing we can test is whether the effect is stronger in the US compared to non-US countries. To test this, we perform a regression analysis. Here we also store the result in a variable, but this is actually quite common in regression analyses because you want to apply the summary function to this variable in order to obtain the inferential statistics.\n\n\nCode\nus_moderation_test &lt;- lm(response ~ source * us_or_international, \n  data = quote_source)\nsummary(us_moderation_test)\n\n\n\nCall:\nlm(formula = response ~ source * us_or_international, data = quote_source)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.005 -1.228 -0.228  1.772  3.772 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                              5.2488     0.0849   61.83  &lt; 2e-16 ***\nsourceWashington                         0.4052     0.1172    3.46  0.00055 ***\nus_or_internationalUS                   -0.0210     0.0955   -0.22  0.82589    \nsourceWashington:us_or_internationalUS   0.3717     0.1323    2.81  0.00497 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.2 on 6321 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.0275,    Adjusted R-squared:  0.027 \nF-statistic: 59.5 on 3 and 6321 DF,  p-value: &lt;2e-16\n\n\nThere appears to be a significant interaction. Let’s inspect the interaction with a graph:\n\n\nCode\nggplot(quote_source, aes(x = us_or_international, y = response, \n    fill = source)) +\n  geom_violin(alpha = .85) +\n  stat_summary(fun.data = \"mean_cl_boot\", position = position_dodge(.9)) +\n  labs(x = \"Region\", y = \"Quote agreement\", fill = \"Source\") +\n  scale_y_continuous(breaks = c(1, 3, 5, 7, 9)) +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\n\n\n\n\n\n\nFigure 2: Source by region interaction\n\n\n\n\n\nWe see that the effect of the source appears to be larger in the US. Given that the positive source was George Washington, this makes sense.\nLet’s do one more analysis to see whether the effect is stronger in a lab setting compared to an online setting.\n\n\nCode\nlab_moderation_test &lt;- lm(response ~ source * lab_or_online, data = quote_source)\nsummary(lab_moderation_test)\n\n\n\nCall:\nlm(formula = response ~ source * lab_or_online, data = quote_source)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.967 -1.197 -0.197  1.737  3.803 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                            5.1971     0.0567   91.59   &lt;2e-16 ***\nsourceWashington                       0.6864     0.0791    8.68   &lt;2e-16 ***\nlab_or_onlineonline                    0.0664     0.0780    0.85     0.39    \nsourceWashington:lab_or_onlineonline   0.0172     0.1089    0.16     0.87    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.2 on 6321 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.0255,    Adjusted R-squared:  0.025 \nF-statistic: 55.1 on 3 and 6321 DF,  p-value: &lt;2e-16\n\n\nWe see no significant interaction in this case. This means we do not find evidence that running the study in an online setting significantly weakens the effect; good to know!"
  },
  {
    "objectID": "content/posts/6-a-tidystats-example/a-tidystats-example.html#applying-tidystats",
    "href": "content/posts/6-a-tidystats-example/a-tidystats-example.html#applying-tidystats",
    "title": "A tidystats example",
    "section": "Applying tidystats",
    "text": "Applying tidystats\nNow let’s get to tidystats. We have three analyses we want to save: a t-test and two regression analyses. We stored each of these analyses in separate variables, called main_test, us_moderation_test, and lab_moderation_test.\nThe main idea is that we will add these three three variables to a list and then save the list as a file on our computer. You create an empty list using the list function. Once you have an empty list, you can add statistics to this list using the add_stats function. add_stats accepts a list as its first argument, followed by a variable containing a statistics model. In our case, this means we need to use the add_stats function three times, as we have three different analyses we want to save. Since this can get pretty repetitive, we will use the piping operator to pipe the three steps together and save some typing.\nBefore we do so, however, note that we can take this opportunity to add some meta-information to each test. For the sake of this example, let’s say that the t-test was our primary test. We also had a suspicion that the location (US vs. international) would matter, but it wasn’t our main interest. Nevertheless, we preregistered these two analyses. During data analysis, we figured that it might also matter whether the study was conducted in the lab or online, so we tested it. This means that this is an exploratory analysis. With add_stats, we can add this information when we add the test to our empty list.\nIn the end, the code looks like this:\n\n\nCode\nresults &lt;- list()\n\nresults &lt;- results %&gt;%\n  add_stats(main_test, type = \"primary\", preregistered = TRUE) %&gt;%\n  add_stats(us_moderation_test, type = \"secondary\", preregistered = TRUE) %&gt;%\n  add_stats(lab_moderation_test, type = \"secondary\", preregistered = FALSE)\n\n\nI recommend to do this at the end of the data analysis script in a section called ‘tidystats’. This confines most of the tidystats code to a single section, keeping it organized, and it will keep most of your script readable to those unfamiliar with tidystats.\nAfter all the analyses are added to the list, the list can be saved as a .json file to your computer’s disk. This is done with the write_stats function. The function requires the list as its first argument, followed by a file path. I’m a big fan of using RStudio Project files so that you can define relative file paths. In this case, I create the .json file in the ‘Data’ folder of my project folder.\n\n\nCode\nwrite_stats(results, \"tidystats-example.json\")\n\n\nIf you want to see what this file looks like, you can inspect it here. Open the file in a text editor to see how the statistics are structured. As you will see, it is not easy for our human eyes to quickly see the results, but it’s easy for computers.\nOnce you’ve saved the file, you can share the file with others or use it to report report the results in your manuscript using the Word add-in.\nThat marks the end of this tidystats example. If you have any questions, please check out the tidystats website or contact me via Twitter.\nThis post was last updated on 2022-04-29."
  },
  {
    "objectID": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html",
    "href": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html",
    "title": "Bayesian tutorial: Correlation",
    "section": "",
    "text": "In my previous blog post, I showed how to use brms and tidybayes to run a simple regression, i.e., a regression with a single predictor. This analysis required us to set three priors: an intercept prior, a sigma prior, and a slope prior. We can simplify this analysis by turning it into a correlational analysis. This will remove the intercept prior and lets us think about the prior for the slope as a standardized effect size, i.e., the correlation.\nTo run a correlational analysis we’ll need to standardize the outcome and predictor variable, so in the code below I run the setup code as usual and also standardize both variables that we’ll be correlating.\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(modelr)\n\ntheme_set(theme_minimal())\ncolors &lt;- c(\"#d1e1ec\", \"#b3cde0\", \"#6497b1\", \"#005b96\", \"#03396c\", \"#011f4b\")\n\noptions(\n  mc.cores = 4,\n  brms.threads = 4,\n  brms.backend = \"cmdstanr\",\n  brms.file_refit = \"on_change\"\n)\n\ndata &lt;- read_csv(\"Howell1.csv\")\ndata &lt;- data |&gt;\n  filter(age &gt;= 18) |&gt;\n  mutate(\n    height_z  = (height - mean(height)) / sd(height),\n    weight_z  = (weight - mean(weight)) / sd(weight)\n  )\nThe formula for our model is slightly different compared to the formula of the previous single-predictor model and that’s because we can omit the intercept. By standardizing both the outcome and predictor variables, the intercept is guarenteed to be 0. The regression line always passes through the mean of the predictor and outcome variable. The mean of both is 0 because of the standardization and the intercept is the value the outcome takes when the predictor is 0. We could still include a prior for the intercept and set it to 0 (using constant(0)) but we can also simply tell brms not to estimate it. The formula syntax then becomes: height_z ~ 0 + weight_z.\nLet’s confirm that this means we only need to set two priors.\nCode\nget_prior(height_z ~ 0 + weight_z, data = data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\nweight_z\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 2.5)\nsigma\n\n\n\n\n\n0\n\ndefault\nIndeed, we’re left with a prior for \\(\\sigma\\) and one for weight_z, which we can specify either via class b or the specific coefficient for weight_z.\nLet’s also write down our model more explicitly, which is the same as the single predictor regression but without the intercept (\\(\\alpha\\)). \\[\\displaylines{heights_i ∼ Normal(\\mu_i, \\sigma) \\\\ \\mu_i = \\beta x_i}\\]"
  },
  {
    "objectID": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html#setting-the-priors",
    "href": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html#setting-the-priors",
    "title": "Bayesian tutorial: Correlation",
    "section": "Setting the priors",
    "text": "Setting the priors\nLet’s start with the prior for the slope (\\(\\beta\\)). A correlation takes a value that ranges from -1 to 1. If you know absolutely nothing about what kind of correlation to expect, you could set a uniform prior that assign equal probability to every value from -1 to -1. Alternatively, we could use a prior that describes a belief that no correlation is most likely, but with some probability that higher correlations are possible too. This could be done with a normal distribution centered around 0. In the case of this particular model, in which height is regressed onto weight, we can probably expect a sizeable positive correlation. So let’s use a skewed normal distribution that puts most of the probability on a positive correlation but is wide enough to allow for a range of correlations, including a negative one. brms has the skew_normal() function to specify a prior that’s a skewed normal distribution. I fiddled around with the numbers a bit and the distribution below is sort of what makes sense to me.\n\n\nCode\nprior &lt;- tibble(r = seq(-1, 1, .01)) |&gt;\n  mutate(\n    prob = dskew_normal(r, xi = .7, omega = .4, alpha = -3)\n  )\n\nggplot(prior, aes(x = r, y = prob)) +\n  geom_line() +\n  labs(x = \"Slope\", y = \"\")\n\n\n\n\n\nPrior distribution for the correlation\n\n\n\n\nWhat should the prior for \\(\\sigma\\) be? With the variables standardized, \\(\\sigma\\) is limited to range from 0 to 1. If the predictor explains all the variance of the outcome variable, the residuals will be 0, meaning \\(\\sigma\\) will be 0. If the predictor explains no variance, \\(\\sigma\\) is equal to 1 because it will be similar to the standard deviation of the outcome variable, which is 1 because we’ve standardized it. Interestingly, this also means that the prior for \\(\\sigma\\) is completely dependent on the prior for the slope, because the slope is what determines how much variance is explained in the outcome variable. I don’t know exactly how to deal with this dependency, except to fear it and make sure to carefully inspect the output so that we don’t have any problems due to incompatible priors. One way to avoid it entirely is to use a uniform prior that assign equal plausibility to each value between 0 and 1, so let’s do that."
  },
  {
    "objectID": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html#running-the-model",
    "href": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html#running-the-model",
    "title": "Bayesian tutorial: Correlation",
    "section": "Running the model",
    "text": "Running the model\nWith the priors ready, we can run the model.\n\n\nCode\nmodel &lt;- brm(\n  height_z ~ 0 + weight_z,\n  data = data,\n  family = gaussian(),\n  prior = c(\n    prior(uniform(0, 1), class = \"sigma\", ub = 1),\n    prior(\n      skew_normal(.7, .4, -3),\n      class = \"b\", lb = -1, ub = 1\n    )\n  ),\n  sample_prior = TRUE,\n  seed = 4,\n  file = \"models/model.rds\"\n)\n\nmodel\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height_z ~ 0 + weight_z \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nweight_z     0.74      0.03     0.68     0.81 1.00     3242     2432\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.66      0.03     0.61     0.71 1.00     3189     2683\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe output shows that the estimate for the slope, i.e., the correlation, is 0.74. This is just one number though. Let’s visualize the entire distribution, including the prior.\n\n\nCode\ndraws &lt;- model %&gt;%\n  gather_draws(prior_b, b_weight_z) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    distribution = if_else(\n      str_detect(.variable, \"prior\"), \"prior\", \"posterior\"\n    ),\n    distribution = fct_relevel(distribution, \"prior\")\n  )\n\nggplot(draws, aes(x = .value, fill = distribution)) +\n  geom_histogram(binwidth = 0.01, position = \"identity\") +\n  labs(x = \"Correlation\", y = \"\", fill = \"Distribution\") +\n  scale_fill_manual(values = c(colors[2], colors[5]))\n\n\n\n\n\n\n\n\n\nIt looks like we can update towards a higher correlation and also be more certain about it because the range of the posterior is much narrower than that of our prior.\nWhat about sigma? We saw that the correlation between the predictor and outcome is 0.74. Squaring this number gives us the amount of variance explained (0.55), so if we subtract this from 1 we’re left with the variance that is unexplained (0.45). Squaring this number to bring it back to a standard deviation gives us 0.67, which matches the estimate for sigma that we saw in the output of brms."
  },
  {
    "objectID": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html#using-a-regularizing-prior",
    "href": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html#using-a-regularizing-prior",
    "title": "Bayesian tutorial: Correlation",
    "section": "Using a regularizing prior",
    "text": "Using a regularizing prior\nIn the previous section we used a personal and hopefully informed prior, at least to some degree. What would happen if we instead used a generic weakly informative prior? This is a prior centered at 0 with a standard deviation of 1.\n\n\nCode\nprior &lt;- tibble(r = seq(-3, 3, .01)) %&gt;%\n  mutate(prob = dnorm(r, mean = 0, sd = 1))\n\nggplot(prior, aes(x = r, y = prob)) +\n  geom_line() +\n  labs(x = \"Slope\", y = \"\")\n\n\n\n\n\nGeneric weakly informative prior for the correlation\n\n\n\n\nIt’s a very broad prior and centered at 0. Does it being centered around 0 push the final estimate closer to a null effect? Let’s see by running the model.\n\n\nCode\nmodel_generic_prior &lt;- brm(\n  height_z ~ 0 + weight_z,\n  data = data,\n  family = gaussian(),\n  prior = c(\n    prior(uniform(0, 1), class = \"sigma\", ub = 1),\n    prior(normal(0, 1), class = \"b\", lb = -1, ub = 1)\n  ),\n  sample_prior = TRUE,\n  seed = 4,\n  silent = 2,\n  file = \"models/model_generic_prior_z.rds\"\n)\n\n\nld: warning: duplicate -rpath '/Users/willem/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb' ignored\n\n\nCode\nmodel_generic_prior\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height_z ~ 0 + weight_z \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nweight_z     0.75      0.04     0.68     0.82 1.00     2699     2338\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.66      0.03     0.61     0.71 1.00     2980     2453\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nmodel_generic_prior |&gt;\n  gather_draws(b_weight_z, prior_b) |&gt;\n  mutate(\n    distribution = if_else(\n      str_detect(.variable, \"prior\"),\n      \"prior\", \"posterior\"\n    )\n  ) |&gt;\n  ggplot(aes(x = .value, fill = distribution)) +\n  geom_histogram(position = \"identity\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe previous estimate of the correlation was 0.74 and now it’s 0.75. Apparently the prior did not influence the final estimate. This hopefully alleviates some worries about priors always having a strong impact on the final results and it also shows you don’t always need to carefully construct a prior. Of course, in certain cases the prior will have a strong influence, for example when the prior is very strong or when there isn’t much data. The prior we used here was broad enough so it didn’t exert a strong influence on the final estimates."
  },
  {
    "objectID": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html#multiple-correlations",
    "href": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html#multiple-correlations",
    "title": "Bayesian tutorial: Correlation",
    "section": "Multiple correlations",
    "text": "Multiple correlations\nWhat if you want to test multiple correlations? There are two ways to do this, as far as I know. The first is to simply run separate models, each testing a single correlation. The second is by creating a model that tests multiple correlations at once.\n\nOne-by-one solution\nRunning multiple models to test each correlation is a bit of a chore, but it’s made easier with the update() function in brms, which makes it so that you don’t have to write as much code. In the code below I standardize age in addition to the two columns we already standardized and run three models in total, correlating height with weight, height with age, and age with weight. I’ll use the same generic prior from the last model and repeat the full code for that model, followed by two updates.\n\n\nCode\ndata &lt;- mutate(data, age_z = (age - mean(age)) / sd(age))\n\nr_height_weight &lt;- brm(\n  height_z ~ 0 + weight_z,\n  data = data,\n  family = gaussian(),\n  prior = c(\n    prior(uniform(0, 1), class = \"sigma\", ub = 1),\n    prior(normal(0, 1), class = \"b\", lb = -1, ub = 1)\n  ),\n  seed = 4,\n  silent = 2,\n  file = \"models/r_height_weight.rds\"\n)\n\nr_height_age &lt;- update(\n  r_height_weight,\n  height_z ~ 0 + age_z,\n  newdata = data,\n  seed = 4,\n  silent = 2,\n  control = list(adapt_delta = .9),\n  file = \"models/r_height_age.rds\"\n)\n\nr_weight_age &lt;- update(\n  r_height_weight,\n  weight_z ~ 0 + age_z,\n  newdata = data,\n  seed = 4,\n  silent = 2,\n  control = list(adapt_delta = .9),\n  file = \"models/r_weight_age.rds\"\n)\n\n\nInitially, the model correlating height with age produced a warning about divergent transitions. brms produces a helpful warning message with a link to more information about what exactly this means (in short, it means the sampler thinks it’s off in estimating the posterior). The message suggests we increase adapt_delta above 0.8, so I adjusted the code to set adapt_delta to 0.9. Re-running the model got rid of the warning messages.\nThe correlations are in the table below (the Estimate column).\n\n\nCode\nbind_rows(\n  as_tibble(fixef(r_height_weight)),\n  as_tibble(fixef(r_height_age)),\n  as_tibble(fixef(r_weight_age))\n) |&gt;\n  mutate(\n    Pair = c(\"height - weight\", \"height - age\", \"weight - age\"),\n    .before = Estimate\n  )\n\n\n\n\nThe three correlations and their 95% HDI modeled one-by-one.\n\n\nPair\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nheight - weight\n0.7545178\n0.0353659\n0.6869373\n0.8250272\n\n\nheight - age\n-0.1027416\n0.0520592\n-0.2060662\n0.0006455\n\n\nweight - age\n-0.1722622\n0.0513209\n-0.2722212\n-0.0697330\n\n\n\n\n\n\nAnd in the graph below, to show their entire posterior distribution.\n\n\nCode\ncorrelation_draws &lt;- bind_rows(\n  r_height_weight %&gt;%\n    spread_draws(b_weight_z) %&gt;%\n    rename(r = b_weight_z) %&gt;%\n    mutate(pair = \"height - weight\"),\n  r_height_age %&gt;%\n    spread_draws(b_age_z) %&gt;%\n    rename(r = b_age_z) %&gt;%\n    mutate(pair = \"height - age\"),\n  r_weight_age %&gt;%\n    spread_draws(b_age_z) %&gt;%\n    rename(r = b_age_z) %&gt;%\n    mutate(pair = \"weight - age\")\n)\n\nggplot(correlation_draws, aes(x = r)) +\n  facet_wrap(~pair, ncol = 1) +\n  geom_histogram(binwidth = 0.01, fill = colors[3]) +\n  labs(x = \"Correlation\", y = \"\") +\n  scale_x_continuous(breaks = seq(-1, 1, 0.2))\n\n\n\n\n\nThe posterior distributions of the three correlations after modeling them one-by-one.\n\n\n\n\n\n\nSimultaneous solution\nThe simultaneous solution is trickier but thankfully there’s a very helpful blog post by Solomon Kurz to explain it, so I’ll mostly just focus on running the code here and showing the result.\nInitially this approach put me off because I did not understand the prior, but then I realized we could simply sample the prior as well and visualize it to show what the prior looks like.\nModelling multiple correlations at once requires specifying the formula using multivariate syntax. You can take a look at the code below to see what this syntax looks like. Additionally, we need to append set_rescor(TRUE) to the formula to tell brms to calculate residual correlations, which will actually be the correlations we’re interested in.\n\n\nCode\nmodel &lt;- brm(\n  formula = bf(\n    mvbind(height_z, weight_z, age_z) ~ 0,\n    sigma ~ 0\n  ) + set_rescor(TRUE),\n  data = data,\n  family = gaussian(),\n  prior = prior(lkj(2), class = rescor),\n  sample_prior = TRUE,\n  seed = 4,\n  silent = 2,\n  file = \"models/multiple-correlations.rds\"\n)\n\n\nld: warning: duplicate -rpath '/Users/willem/.cmdstan/cmdstan-2.32.2/stan/lib/stan_math/lib/tbb' ignored\n\n\nCode\nmodel\n\n\n Family: MV(gaussian, gaussian, gaussian) \n  Links: mu = identity; sigma = log\n         mu = identity; sigma = log\n         mu = identity; sigma = log \nFormula: height_z ~ 0 \n         sigma ~ 0\n         weight_z ~ 0 \n         sigma ~ 0\n         age_z ~ 0 \n         sigma ~ 0\n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nResidual Correlations: \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nrescor(heightz,weightz)     0.75      0.02     0.71     0.78 1.00     3947\nrescor(heightz,agez)       -0.10      0.05    -0.20     0.00 1.00     3851\nrescor(weightz,agez)       -0.17      0.05    -0.26    -0.07 1.00     3997\n                        Tail_ESS\nrescor(heightz,weightz)     3079\nrescor(heightz,agez)        3014\nrescor(weightz,agez)        3153\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe model output shows the correlations in the Residual Correlations section. You’ll see that the estimates match the ones we found when running the correlations one-by-one. The 95% CIs also largely match, with some small differences (for more comparisons, see the previously linked blog post by Solomon Kurz).\nLet’s also plot the posteriors, including their prior.\n\n\nCode\ndraws &lt;- model %&gt;%\n  gather_draws(\n    prior_rescor, rescor__heightz__weightz, rescor__heightz__agez,\n    rescor__weightz__agez\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    .variable = case_match(\n      .variable,\n      \"prior_rescor\" ~ \"prior\",\n      \"rescor__heightz__weightz\" ~ \"height-weight\",\n      \"rescor__heightz__agez\" ~ \"height-age\",\n      \"rescor__weightz__agez\" ~ \"weight-age\"\n    ),\n    .variable = fct_relevel(.variable, \"prior\")\n  )\n\nggplot(draws, aes(x = .value)) +\n  facet_wrap(~.variable, ncol = 2) +\n  geom_histogram(aes(fill = .variable), binwidth = 0.01) +\n  labs(x = \"Correlation\", y = \"\") +\n  scale_x_continuous(breaks = seq(-1, 1, 0.2)) +\n  scale_fill_manual(values = c(colors[2], colors[5], colors[5], colors[5])) +\n  guides(fill = \"none\")\n\n\n\n\n\nThe posterior distributions of the three correlations, and their prior, after modeling them simultaneously.\n\n\n\n\nIt looks like we used a relatively wide prior centered around 0. That’s good to know because I had no idea what the lkj() prior was doing. Other than that the results look similar to what we found previously."
  },
  {
    "objectID": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html#summary",
    "href": "content/posts/19-bayesian-tutorial-correlations/bayesian-tutorial-correlations.html#summary",
    "title": "Bayesian tutorial: Correlation",
    "section": "Summary",
    "text": "Summary\nRunning a correlation in brms is the same as running a simple regression, except that the outcome and predictor are standardized. Because of the standardization, the intercept can be omitted, thus simplifying the model. The priors are also easier to set as the correlation must range from -1 to and 1 and sigma from 0 to 1. You can also run multiple correlations by running separate models or by modelling them all at once using brms’ multivariate syntax.\nThis post was last updated on 2024-04-13."
  },
  {
    "objectID": "content/posts/3-useful-power-analysis-papers/useful-power-analysis-papers.html",
    "href": "content/posts/3-useful-power-analysis-papers/useful-power-analysis-papers.html",
    "title": "Useful power analysis papers",
    "section": "",
    "text": "Maxwell, S. E. (2004). The persistence of underpowered studies in psychological research: Causes, consequences, and remedies. Psychological Methods, 9(2), 147–163. https://doi.org/10.1037/1082-989X.9.2.147\n\nThis paper is quite amazing. It covers almost everything you need to be aware of when it comes to the state of our field, including how badly powered most studies are, how this affects the interpretability of inconsistencies in the literature, the need for multisite projects, and so on!\nMost importantly, it is about the problem of multiple statistical tests. Many power analyses that I see in the literature only power for one analysis, even though a paper usually contains many more. If you want a shot at all of those analyses being able to show something, you need to power for it all.\n\nBlake, K. R., & Gangestad, S. (2020). On attenuated interactions, measurement error, and statistical power: Guidelines for social and personality psychologists. Personality and Social Psychology Bulletin. https://doi.org/10.1177/0146167220913363\n\nThis is a great paper on a common pitfall in power analyses for attenuated interaction tests. Attenuated interactions are interactions where you expect a predictor to have an effect in one condition, but not another. They show that in a 2 x 2 design, you should use a fourfold of the sample size that is needed to show the interaction effect. Of course, this only applies when you predict a perfect attenuated interaction (a complete absence of the effect in the other condition, rather than a diminished one) and that you do not have any measurement error.\nThis post was last updated on r format(Sys.Date(), \"%Y-%m-%d\")."
  },
  {
    "objectID": "content/posts/3-useful-power-analysis-papers/useful-power-analysis-papers.html#papers",
    "href": "content/posts/3-useful-power-analysis-papers/useful-power-analysis-papers.html#papers",
    "title": "Useful power analysis papers",
    "section": "",
    "text": "Maxwell, S. E. (2004). The persistence of underpowered studies in psychological research: Causes, consequences, and remedies. Psychological Methods, 9(2), 147–163. https://doi.org/10.1037/1082-989X.9.2.147\n\nThis paper is quite amazing. It covers almost everything you need to be aware of when it comes to the state of our field, including how badly powered most studies are, how this affects the interpretability of inconsistencies in the literature, the need for multisite projects, and so on!\nMost importantly, it is about the problem of multiple statistical tests. Many power analyses that I see in the literature only power for one analysis, even though a paper usually contains many more. If you want a shot at all of those analyses being able to show something, you need to power for it all.\n\nBlake, K. R., & Gangestad, S. (2020). On attenuated interactions, measurement error, and statistical power: Guidelines for social and personality psychologists. Personality and Social Psychology Bulletin. https://doi.org/10.1177/0146167220913363\n\nThis is a great paper on a common pitfall in power analyses for attenuated interaction tests. Attenuated interactions are interactions where you expect a predictor to have an effect in one condition, but not another. They show that in a 2 x 2 design, you should use a fourfold of the sample size that is needed to show the interaction effect. Of course, this only applies when you predict a perfect attenuated interaction (a complete absence of the effect in the other condition, rather than a diminished one) and that you do not have any measurement error.\nThis post was last updated on r format(Sys.Date(), \"%Y-%m-%d\")."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "",
    "text": "Not too long ago the House of Representatives of The Netherlands released a public portal to a lot of their data. The portal contains data on law proposals, motions, rapports, etc. I’ve been interested in this kind of data for a while now because I want to know more about the voting behavior of political parties. Specifically, I want to know which parties consistently vote in favor of improving animal rights. It’s relatively easy for a political party to say that they care about animal rights, but that doesn’t mean they consistently vote in favor of motions that improve animal rights. So let’s figure out how the open data portal works and which party to vote for.\nRun the following setup code if you want to follow along.\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(jsonlite)"
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-the-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-the-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Getting the data",
    "text": "Getting the data\nWe will use the OData API to obtain the data. Using this API is pretty easy in theory; it’s nothing more than constructing a URL and then retrieving the data using that URL. The only tricky bit is how to set it up. In order to know how to do that, we need to understand the API. The OData API links to an information model that shows what kind of data we can request. We can request different entities, such as a Zaak (case), Document, Activiteit (activity), and so on. Going through the documentation I figured out we want to request cases because they have a Besluit (decision) entity, which contain a Stemming (vote) entity. Now that we sort of know what we want, we need to figure out how to actually get it.\nThe documentation of the API is pretty good. They explain how to set up the URL, call a query, and even provide several examples.\nEach query starts with the base URL: https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/. We need to append additional functions to this URL to hone in on the exact data we want.\nThe first thing we’ll specify is that we want a Zaak (case), so we will append Zaak to the end of the base URL.\nNext, we will apply some filter functions. In the documentation they recommend that we always filter on entities that have not been removed. They keep removed entities in the database so they can track changes. In one of the examples we can see how this is done. We have to append the following to the URL: ?$filter=Verwijderd eq false. The (first) filter needs to start with a question mark and a dollar sign, followed by the function name (filter), an equal sign, and a condition. The condition in this case is Verwijderd eq false, in other words: Removed equals false.\nAdditional filters can be added using logical operators such as and, or, or not. We want to request only cases that are motions, so we’ll add and Soort eq 'Motie'. Notice that we use and because we want both conditions to be true. The filter itself means that we want the Soort (type) to be equal to ‘Motie’ (motion). If we were to stop here, we would get a bunch of different motions, many of which have nothing to do with animal welfare. So let’s add another filter: and contains(Titel, 'Dierenwelzijn'). This means we select only the motions whose title contains the word ‘Dierenwelzijn’ (animal welfare). We could run this, but then we will get a total of 250 cases. It turns out that this is the maximum number of entities you can retrieve. That’s not ideal because preferably we get all of the animal welfare-related motions and if we get 250 back it’s not clear whether we got all of them. So let’s add another filter: and year(GestartOp) eq 2021. This means we only want cases when they’ve started in 2021. This probably results in fewer than 250 relevant motions, meaning we obtained them all (of that year).\nThe final function we need to add is an expand function. Right now we’re only requesting the data of motions, but not the data of the decision that was made in the motion, or the voting data. To also include that in the request we need to use the expand function. It’s a bit tricky because we need to run the expand function twice, once to expand on the decision and once on the voting. The part we need to append to the URL is: &$expand=Besluit($expand=Stemming).\nNow our URL is pretty much done. We have to paste all the parts together and request the data. We also need to replace all spaces with %20 so that it becomes a valid URL. You don’t need to do this if you just want to paste the URL in the browser, but if you want to use R code like in the code below, we do need to do this.\nThe data will be returned in a JSON format by the API. In R there’s the jsonlite package to work with JSON data, so we’ll use that package. The following code sets up the URL and retrieves the data.\n\n\nCode\n# Set url components\nbase_url &lt;- \"https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/\"\nentity &lt;- \"Zaak\"\nfilter1 &lt;- \"?$filter=Verwijderd eq false\"\nfilter2 &lt;- \" and Soort eq 'Motie'\"\nfilter3 &lt;- \" and contains(Titel, 'Dierenwelzijn')\"\nfilter4 &lt;- \" and year(GestartOp) eq 2021\"\nexpand &lt;- \"&$expand=Besluit($expand=Stemming)\"\n\n# Construct url\nurl &lt;- paste0(base_url, entity, filter1, filter2, filter3, filter4, expand)\n\n# Escape all spaces by replacing them with %20\nurl &lt;- str_replace_all(url, \" \", \"%20\")\n\n# Get data\ndata &lt;- read_json(url)\n\n\nYou can inspect the retrieved data here."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-the-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-the-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nThe data is structured as a list with various attributes, including additional lists. I personally don’t like working with lists at all in R so I want to convert it to a data frame as soon as possible. My favorite way of converting lists to a data frame is by using map_df(). It’s a function that accepts a list as its first argument and a function as its second argument. The function will be applied to each element in the list and the results of that will automatically be merged into a data frame. So let’s create that function.\nIn the code below we create a function that accepts an element of the value attribute in data, which is a list of cases we requested. The function then creates a data frame with only some of the case attributes: the number, title, subject, and start date. You can figure out which attributes are available by checking the documentation or going through the data we just obtained. After creating this function we run map_df().\n\n\nCode\n# Create a custom function to extract data from each motion\nclean_zaak &lt;- function(zaak) {\n  df &lt;- tibble(\n    number = zaak$Nummer,\n    start_date = as_date(zaak$GestartOp),\n    title = zaak$Titel,\n    subject = zaak$Onderwerp\n  )\n}\n\n# Run the clean_zaak function on each case\ndf &lt;- map_df(data$value, clean_zaak)\n\n\nThe result is the following data frame:\n\n\nCode\ndf\n\n\n Subset of cases data\n  \n\n\n\nWe can see that all the dates are from 2021 and that the titles contain the word ‘Dierenwelzijn’, just like we filtered on. The subject column is more interesting. It shows us what the case was about (if you don’t see the column, click on the arrow next to the title). After inspecting some of the subjects it becomes obvious that not all cases are about improving animal welfare. One, for example, is about using mobile kill units to kill animals that can’t be transported to a slaughterhouse. Ideally, we should go over all the cases and judge whether the case is about something that improves animal welfare or not.\nAlternatively, we can rely on the heuristic (for now) that in general all the cases on animal welfare are about things that improve animal welfare. Since we’re relying on a heuristic, it would help if we get more data so we can have the exceptions to this heuristic be overruled by many more data points. So let’s retrieve much more data."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-even-more-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-even-more-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Getting even more data",
    "text": "Getting even more data\nBelow I loop over several years and retrieve the data for that year. After retrieving the data, it is saved to a file using the write_json() function. It has an auto_unbox argument so that attributes that only consist of 1 attribute aren’t stored as lists but directly as the type of attribute itself (e.g., a number or string). There’s also the pretty argument which makes sure the file is at least somewhat readable, rather than one single very long line of data.\n\n\nCode\n# Set years we want the data of\nyears &lt;- 2008:2021\n\n# Set url components\nbase_url &lt;- \"https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/\"\nentity &lt;- \"Zaak\"\nfilter1 &lt;- \"?$filter=Verwijderd eq false\"\nfilter2 &lt;- \" and Soort eq 'Motie'\"\nfilter3 &lt;- \" and contains(Titel, 'Dierenwelzijn')\"\nfilter4 &lt;- \" and year(GestartOp) eq \"\nexpand &lt;- \"&$expand=Besluit($expand=Stemming)\"\n\n# Loop over the years\nfor (year in years) {\n  # Construct the url\n  url &lt;- paste0(base_url, entity, filter1, filter2, filter3, filter4,\n    year, expand)\n  \n  # Escape all spaces\n  url &lt;- str_replace_all(url, \" \", \"%20\")\n  \n  # Get data\n  data &lt;- read_json(url)\n  \n  # Write the data to a file\n  write_json(\n    data, \n    path = paste0(\"motions-\", year, \".json\"), \n    auto_unbox = TRUE, \n    pretty = TRUE\n  )\n}"
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-even-more-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-even-more-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Cleaning even more data",
    "text": "Cleaning even more data\nNow that we have a bunch of data files, we need to read them in. A technique to read in multiple files of the same type is to use map_df() again. We can give it a list of files, created with list.files(), and apply a function to each file path. Not only can we use that to simply read in the data, we can immediately parse the data and convert it to a data frame. In the code below I go all inception on this problem and define multiple functions that each convert a list to a data frame. There’s a function for reading in a file, converting a case to a data frame, which calls a function to convert a decision to a data frame, which calls a function to convert a vote to a data frame. It may seem a bit complicated, but once you realize you can call functions within functions, it can actually make some tricky problems easy to solve; at least with relatively little code.\n\n\nCode\nread_file &lt;- function(file) {\n  data &lt;- read_json(file)\n  \n  df &lt;- map_df(data$value, clean_zaak)\n  \n  return(df)\n}\n\nclean_zaak &lt;- function(zaak) {\n  df &lt;- tibble(\n    motion_number = zaak$Nummer,\n    start_date = as_date(zaak$GestartOp),\n  )\n  \n  df &lt;- tibble(\n    df,\n    map_df(zaak$Besluit, clean_besluit)\n  )\n  \n  return(df)\n}\n\nclean_besluit &lt;- function(besluit) {\n  df &lt;- tibble(\n    decision_outcome = besluit$BesluitTekst\n  )\n  \n  if (length(besluit$Stemming) != 0) {\n    df &lt;- tibble(\n      df,\n      map_df(besluit$Stemming, clean_stemming)\n    )\n  }\n  \n  return(df)\n}\n\nclean_stemming &lt;- function(stemming) {\n  df &lt;- tibble(\n    party = stemming$ActorFractie,\n    vote = stemming$Soort,\n    mistake = stemming$Vergissing\n  )\n  \n  return(df)\n}\n\n# Create a list of the files we want to read\nfiles &lt;- list.files(pattern = \"motions-[0-9]+.json\")\n\n# Apply the read_file() function to each file, which calls each other function\ndf &lt;- map_df(files, read_file)\n\n\nLet’s clean up the resulting data frame some more because we kept more information than we actually need. For example, there are different types of decision outcomes, but we only care about the ones where a voting took place. Let’s also translate the votes to English and exclude votes of parties that did not participate (they are still included) and mistaken votes (apparently sometimes they make mistakes when voting).\n\n\nCode\ndf &lt;- df %&gt;%\n  filter(str_detect(decision_outcome, \"Verworpen|Aangenomen\")) %&gt;%\n  filter(vote != \"Niet deelgenomen\") %&gt;%\n  filter(!mistake) %&gt;%\n  mutate(\n    decision_outcome = str_extract(decision_outcome, \"Verworpen|Aangenomen\"),\n    decision_outcome = recode(\n      decision_outcome, \n      \"Verworpen\" = \"rejected\", \n      \"Aangenomen\" = \"accepted\"\n    ),\n    start_date = year(start_date),\n    vote = recode(vote, \"Tegen\" = \"nay\", \"Voor\" = \"aye\"),\n    vote = factor(vote),\n    mistake = NULL\n  )\n\n\nAnnoyingly, I discovered that the decision outcome data changed over the years in a trivial way. Starting in the year 2013, they added a period to the description of the decision outcome (e.g., ‘Verworpen.’). A silly change that actually resulted in me missing data from the years before 2013 while initially writing this post.\nWe now have the following data frame:\n\n\nCode\ndf"
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#analyzing-voting-behavior",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#analyzing-voting-behavior",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Analyzing voting behavior",
    "text": "Analyzing voting behavior\nNow we are ready to inspect the voting behavior of the political parties. For each party we calculate how often they voted ‘aye’ or ‘nay’ and calculate it as a percentage of the times they’ve voted. We then plot the percentage of times they voted ‘aye’.\n\n\nCode\nvoting &lt;- df %&gt;%\n  count(party, vote, .drop = FALSE) %&gt;%\n  pivot_wider(names_from = vote, values_from = n) %&gt;%\n  mutate(\n    votes = aye + nay,\n    aye_pct = aye / votes\n  )\n\nggplot(voting, aes(x = aye_pct, y = reorder(party, aye_pct))) +\n  geom_col(aes(alpha = votes)) +\n  labs(\n    x = \"Times voted 'aye' on an animal welfare motion (in %)\", \n    y = \"\",\n    alpha = \"Times voted\") +\n  scale_x_continuous(limits = c(0, 1), labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\nPercentage of times political parties voted ‘aye’ on motions related to animal welfare\n\n\n\n\nIt seems like the heuristic might be somewhat justified. I don’t know much about Van Kooten-Arissen or Group Krol/vKA, but PvdD stands for Partij voor de Dieren (party for the animals). It makes sense that they are among the top in voting in favor of improving animal welfare. At the same time there’s some evidence that the heuristic is indeed only a heuristic. The PvdD apparently voted ‘aye’ in 84.13% of the motions. That could mean there are some motions where it is in the interest of the animals to vote ‘nay’. I also find it a bit worrying that the PVV, a notorious right-wing party in the Netherlands, is so high on the list of voting ‘aye’ on animal welfare matters. In some manual inspections of the motions I saw they tend to disagree with some obvious animal welfare improvements, although perhaps my sample just happened to find these disagreements and had I inspected more motions I would have found the same results.\nAnother way we can look at this data is by using the PvdD as a benchmark for what the other political parties should vote for. We can assume that this party has the best interest for animals in mind, as that is their most important platform. Of course this would mean we can’t use the result to figure out whether we should vote for PvdD, but it can be useful to figure out which alternative party to vote for.\n\n\nCode\nvote_matches_PvdD &lt;- df %&gt;%\n  filter(party == \"PvdD\") %&gt;%\n  group_by(motion_number) %&gt;%\n  summarize(vote_PvdD = first(vote)) %&gt;%\n  right_join(df, by = \"motion_number\") %&gt;%\n  filter(party != \"PvdD\") %&gt;%\n  mutate(\n    match = if_else(vote == vote_PvdD, \"match\", \"no_match\"),\n    match = factor(match)\n  ) %&gt;%\n  count(party, match, .drop = FALSE) %&gt;%\n  pivot_wider(names_from = match, values_from = n) %&gt;%\n  mutate(\n    votes = match + no_match,\n    match_pct = match / votes\n  )\n\nggplot(vote_matches_PvdD, aes(x = match_pct, y = reorder(party, match_pct))) +\n  geom_col(aes(alpha = votes)) +\n  labs(\n    x = \"Times voted the same as the Party for the Animals (in %)\", \n    y = \"\",\n    alpha = \"Times voted\") +\n  scale_x_continuous(limits = c(0, 1), labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\nPercentage of times a political party voted the same as the Party for the Animals\n\n\n\n\nIt looks like the two graphs are fairly consistent. Van Kooten-Arissen and Groep Krol/vKA are still at the top. The same goes for the bigger parties. In fact, the ranking of the largest parties (who have voted the most times) is the same in the two graphs. That can give us some extra confidence that the heuristic from the first graph works.\nLet’s go back to our heuristic and create another graph that shows the voting behavior across the years. After all, it could very well be that a political party changed their values in the last decade. Let’s only use the 10 biggest parties for this graph because they’re more likely to have enough data for each year.\n\n\nCode\nvoting_years &lt;- df %&gt;%\n  group_by(party) %&gt;%\n  mutate(votes = n()) %&gt;%\n  filter(votes &gt; 300) %&gt;%\n  count(start_date, party, vote, .drop = FALSE) %&gt;%\n  pivot_wider(names_from = vote, values_from = n) %&gt;%\n  mutate(\n    votes = aye + nay,\n    aye_pct = aye / votes\n  ) \n\nggplot(\n    voting_years, \n    aes(x = start_date, y = aye_pct)\n  ) +\n  geom_line(alpha = .25) +\n  geom_point() +\n  facet_wrap(~ party, ncol = 2) +\n  labs(\n    x = \"\", \n    y = \"\"\n  ) +\n  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\nTimes each party voted ‘aye’ on animal welfare motions throughout the years\n\n\n\n\nLooks like most parties are fairly consistent. There’s some variation from year to year, but for most parties you can tell whether they are pro-animal or not. There are some exceptions to this, like the PvdA and D66, which seem to vary quite a bit. We can actually calculate this variation so we don’t have to guess it from the graph.\n\n\nCode\nvoting_years %&gt;%\n  group_by(party) %&gt;%\n  summarize(SD = sd(aye_pct)) %&gt;%\n  arrange(desc(SD))\n\n\n Party consistency across years\n  \n\n\n\nYup, looks like PvdA and D66 are the least consistent."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#limitations",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#limitations",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Limitations",
    "text": "Limitations\nIt bears repeating that this analysis of the data isn’t perfect. The best way to analyze this data would be to take a look at each individual motion and determine, based on your own values, whether an ‘aye’ vote or a ‘nay’ vote is in the best interest of animals. I hope to do this myself in the future.\nAnother limitation of this analysis is that we have only searched for motions that mention animal welfare in the title. There are many more motions that address animal welfare issues that we’ve missed with our method. The results could therefore be made more reliable by adding more data. In this post we have looked at 5453 votes in 391 motions. I suppose that’s decent, but we could do better."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#conclusion",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#conclusion",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we used publicly available data from the House of Representatives of the Netherlands to inspect the voting behavior of political parties on matters related to animal welfare. This is made possible by the amazing new data portal that makes this data freely and relatively easily available to everyone. Kudos to them for that.\nIf you’re interested in figuring out which party to vote for because you want to support animal welfare, then the largest parties to pay attention to are the PvdD, GroenLinks, and SP. They are large enough to have voted on issues a decent number of times, giving us some confidence that they vote in favor of improving animal welfare. More can be done to improve this interpretation of the data, but looking at actual voting behavior seems like a valuable piece of information when considering which party to vote for."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "",
    "text": "Many thanks to the people who replied to my tweet about why you should divide by \\(n - 1\\). Below I try to show the intuition behind why this is necessary. If you want to follow along in R, you can copy the code from each code section; beginning with some setup code.\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(viridis)\n\n# Create our own variance function that returns the population or sample \n# variance\nmy_var &lt;- function(x, population = FALSE) {\n  if (population) {\n    sum((x - mean(x))^2)/length(x)\n  } else {\n    sum((x - mean(x))^2)/(length(x) - 1)\n  }\n}\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n# Set options\noptions(\n  knitr.kable.NA = \"\",\n  digits = 2\n)"
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-formula",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-formula",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The formula",
    "text": "The formula\nThe formula for calculating the variance is:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n}\\]\nThe variance is a measure of the dispersion around the mean, and in that sense this formula makes sense. We calculate all the deviations from the mean (\\(x_i - \\overline{x}\\)), square them (for reasons I might go into in a different post) and sum them. We then divide this sum by the number of observations as a scaling factor. If we ignore this number, we could get a very high variance simply by observing a lot of data. So, to fix that problem, we divide by the total number of observations.\nHowever, this is the formula for the population variance. The formula for calculating the variance of a sample is:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n - 1}\\]\nWhy do we divide by n - 1?\nIf you Google this question, you will get a variety of answers. You might find a mathematical proof of why it needs to be \\(n - 1\\) or something about degrees of freedom. These kinds of answers don’t work for me. I trust them to be correct, but it doesn’t produce any insight. It does not actually help me understand 1) the problem and 2) why the solution is the solution that it is. So, below I am going to try to figure it out in a way that actually makes conceptual and intuitive sense (to me)."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-problem",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-problem",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The problem",
    "text": "The problem\nThe problem with using the population variance formula to calculate the variance of a sample is that it is biased. It is biased in that it produces an underestimation of the true variance. Let’s demonstrate that with some simulated data.\nWe simulate a population of 1000 data points from a uniform distribution with a range from 1 to 10. Below I show the histogram that represents our population.\n\n\nCode\n# Set the seed for reproducibility\nset.seed(1212)\n\n# Create a population consisting of values ranging from 1 to 10\npopulation &lt;- sample(1:10, size = 1000, replace = TRUE)\n\n# Calculate the population variance\nsigma &lt;- my_var(population, population = TRUE)\n\n# Visualize the population\nggplot(tibble(population = population), mapping = aes(x = population)) +\n  geom_bar(alpha = .85) +\n  labs(x = \"x\", y = \"n\") +\n  scale_x_continuous(breaks = 1:10)\n\n\n\n\n\n\n\n\nFigure 1: The population in our example\n\n\n\n\n\nThe variance is 8.76. Note that this is our population variance (often denoted as \\(\\sigma^2\\)). We want to estimate this value using samples drawn from our population, so let’s do that.\n\n\nCode\n# Draw a single sample from the population\nsample &lt;- sample(population, size = 5)\n\n\nTo start, we can draw a single sample of size 5. Say we do that and get the following values: 7, 6, 3, 5, 5. We can then calculate the variance in two ways, using division by \\(n\\) and division by \\(n - 1\\). In the former case, this will result in 1.76 and in the latter case it results in 2.2.\nNow let’s do that many many times. Below I show the results of draws from our population. I simulated drawing samples of size 2 to 10, each 1000 different times. I then plotted for each sample size the average biased variance (dividing by \\(n\\); Figure 2 (a)) and the average unbiased variance (dividing by \\(n - 1\\); Figure 2 (b)).\n\nCode\n# Create an empty data frame with the simulation parameters\nsamples &lt;- crossing(\n    n = 2:20,\n    i = 1:1000\n  )\n\n# Calculate the mean, sample variance, and population variance \n# for each combination of n and i\nsamples &lt;- samples %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    var_unbiased = my_var(sample(population, n), population = FALSE),\n    var_biased = my_var(sample(population, n), population = TRUE)\n  )\n\n# Plot the results in two separate plots\nggplot(samples, aes(x = n, y = var_biased)) +\n  geom_hline(yintercept = sigma, linetype = \"dashed\", alpha = .5) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\") +\n  coord_cartesian(ylim = c(0, sigma + 1)) +\n  scale_x_continuous(breaks = seq(from = 2, to = 20, by = 2))\n\nggplot(samples, aes(x = n, y = var_unbiased)) +\n  geom_hline(yintercept = sigma, linetype = \"dashed\", alpha = .5) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\") +\n  coord_cartesian(ylim = c(0, sigma + 1)) +\n  scale_x_continuous(breaks = seq(from = 2, to = 20, by = 2))\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Variance with division by n\n\n\n\n\n\n\n\n\n\n\n\n(b) Variance with division by n - 1\n\n\n\n\n\n\n\nFigure 2: A demonstration that dividing by 1 causes a bias\n\n\n\nWe see that the biased measure of variance is indeed biased. The average variance is lower than the true variance (indicated by the dashed line), for each sample size. We also see that the unbiased variance is indeed unbiased. On average, the sample variance matches that of the population variance.\nThe results of using the biased measure of variance reveals several clues for understanding the solution to the bias. We see that the amount of bias is larger when the sample size of the samples is smaller. So the solution should be a function of sample size, such that the required correction will be smaller as the sample size increases. We also see that that the bias at \\(n = 2\\) is half that of the true variance, \\(\\frac23\\) at \\(n = 3\\), \\(\\frac34\\) at \\(n = 4\\), and so on. Interesting.\nBut before we go into the solution, we still need to figure out what exactly causes the bias.\nIdeally we would estimate the variance of the sample by subtracting each value from the population mean. However, since we don’t know what the population mean is, we use the next best thing—the sample mean. This is where the bias comes in. When you use the sample mean, you’re guaranteed that the mean lies somewhere within the range of your data points. In fact, the mean of a sample minimizes the sum of squared deviations from the mean. This means that the sum of deviations from the sample mean is almost always smaller than the sum of deviations from the population mean. The only exception to that is when the sample mean happens to be the population mean.\nLet’s illustrate this with a few graphs. Below are two graphs in which I show 10 data points that represent our population. I also highlight two data points from this population, which represents our sample. In the left graph I show the deviations from the sample mean and in the right graph the deviations from the population mean.\n\nCode\n# Create the population\nx &lt;- c(1, 2, 4, 4, 4, 6, 8, 9, 10, 10)\n\n# Create a sample\nsample1 &lt;- tibble(\n    index = 1:10,\n    value = x,\n    sample = c(0, 0, 0, 0, 0, 0, 1, 0, 0, 1),\n    mean = mean(c(8, 10)),\n    mu = mean(x)\n  ) %&gt;%\n  mutate(\n    mean = ifelse(sample == 1, mean, NA),\n    mu = ifelse(sample == 1, mu, NA)\n  )\n\n# Plot the results\nggplot(sample1, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mu), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mean), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mean, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 5.8, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 9, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n\nggplot(sample1, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mean), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mu), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mu, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 5.8, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 9, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Deviations from the sample mean\n\n\n\n\n\n\n\n\n\n\n\n(b) Deviations from the population mean\n\n\n\n\n\n\n\nFigure 3: An illustration of how using a sample mean introduces bias\n\n\n\nWe see that in the left graph the sum of squared deviations is much smaller than in the right graph. The sum is \\((8 - 9)² + (10 - 9)² = 2\\) in the left graph and in the right graph it’s \\((8 - 5.8)² + (10 - 5.8)² = 22.48\\). The sum is smaller when using the sample mean compared to using the population mean.\nThis is true for any sample you draw from the population (again, except when the sample mean happens to be the same as the population mean). Let’s look at one more draw where the sample mean is closer to the population mean.\n\nCode\n# Create a second sample\nsample2 &lt;- tibble(\n    index = 1:10,\n    value = x,\n    sample = c(0, 1, 0, 0, 0, 0, 0, 0, 1, 0),\n    mean = mean(c(2, 10)),\n    mu = mean(x)\n  ) %&gt;%\n  mutate(\n    mean = ifelse(sample == 1, mean, NA),\n    mu = ifelse(sample == 1, mu, NA)\n  )\n\n# Plot the results\nggplot(sample2, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mu), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mean), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mean, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 4.92, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 6.7, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n  \nggplot(sample2, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mean), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mu), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mu, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 4.92, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 6.7, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Deviations from the sample mean\n\n\n\n\n\n\n\n\n\n\n\n(b) Deviations from the population mean\n\n\n\n\n\n\n\nFigure 4: Another illustration of how using a sample mean introduces bias\n\n\n\nHere the sum in the left graph is \\((2 - 6)² + (10 - 6)² = 32\\) and the sum in the right graph is \\((2 - 5.8)² + (10 - 5.8)² = 32.08\\). The difference is small now, but using the sample mean still results in a smaller sum compared to using the population mean.\nIn short, the source of the bias comes from using the sample mean instead of the population mean. The sample mean is always guaranteed to be in the middle of the observed data, thereby reducing the variance, and creating an underestimation."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-solution",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-solution",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The solution",
    "text": "The solution\nNow that we know that the bias is caused by using the sample mean, we can figure out how to solve the problem.\nLooking at the previous graphs, we see that if the sample mean is far from the population mean, the sample variance is smaller and the bias is large. If the sample mean is close to the population mean, the sample variance is larger and the bias is small. So, the more the sample mean moves around the population mean, the greater the bias.\nIn other words, besides the variance of the data points around the sample mean, there is also the variance of the sample mean around the population mean. We need both variances in order to accurately estimate the population variance.\nThe population variance is thus the sum of two variances:\n\\[\\sigma^2_{sample} + \\sigma^2_{\\vphantom{sample}mean} = \\sigma^2_{population}\\] Let’s confirm that this is true. For that we need to know how to calculate the variance of the sample mean around the population mean. This is relatively simple; it’s the variance of the population divided by n (\\(\\frac{\\sigma^2}n\\)). This makes sense because the greater the variance in the population, the more the mean can jump around, but the more data you sample, the closer you get to the population mean.\nNow that we can calculate both the variance of the sample and the variance of the sample mean, we can check whether adding them together results in the population variance.\nBelow I show a graph in which I again sampled from our population with varying sample sizes. For each sample, I calculated the sample variance (the biased one) and the variance of the mean of that sample (\\(\\frac{\\sigma^2}n\\))1. I did this 1000 times per sample size, took the average of each and put them on top of each other. I also added a dashed line to indicate the variance of the population, which is the benchmark we’re trying to reach.\n\n\nCode\n# Calculate the variance sources per sample size\nvariance_sources &lt;- samples %&gt;%\n  mutate(var_mean = var_unbiased / n) %&gt;%\n  group_by(n) %&gt;%\n  summarize(\n    var_biased = mean(var_biased),\n    var_unbiased = mean(var_unbiased),\n    var_mean = mean(var_mean)\n  ) %&gt;%\n  pivot_longer(cols = c(var_biased, var_mean), names_to = \"variance_source\", \n    values_to = \"variance\") %&gt;%\n  mutate(variance_source = recode(variance_source, \"var_biased\" = \n      \"sample\", \"var_mean\" = \"sample mean\"))\n\n# Plot the results\nggplot(variance_sources, aes(x = n, fill = variance_source, y = variance)) +\n  geom_col(alpha = .85) +\n  geom_hline(yintercept = sigma, linetype = \"dashed\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\", fill = \"Variance source:\") +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\n\n\n\n\n\n\nFigure 5: Sources of variance\n\n\n\n\n\nIndeed, we see that the variance of the sample and the variance of the mean of the sample together form the population variance."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-math",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-math",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The math",
    "text": "The math\nNow that we know that the variance of the population consists of the variance of the sample and the variance of the sample mean, we can figure out the correction factor we need to apply to make the biased variance measure unbiased.\nPreviously, we found an interesting pattern in the simulated samples, which is also visible in the previous figure. We saw that at sample size \\(n=2\\), the (biased) sample variance appears to be half that of the (unbiased) population variance. At sample size \\(n=3\\), it’s \\(\\frac23\\). At sample size \\(n=4\\), it’s \\(\\frac34\\), and so on.\nThis means that we can fix the biased variance measure by multiplying it with \\(\\frac{n}{(n-1)}\\). At \\(n = 2\\), we multiply the biased variance by \\(\\frac21 = 2\\). For sample size \\(n=3\\), we multiply by \\(\\frac32 = 1.5\\). At sample size \\(n=4\\), it’s \\(\\frac43 = 1 \\frac13\\).\nIn other words, to unbias the biased variance measure, we multiply it by a correction factor of \\(\\frac{n}{(n-1)}\\). But where does this correction factor come from?\nWell, because the sample variance misses the variance of the sample mean, we can expect that the variance of the sample is biased by an amount equal to the variance of the population minus the variance of the sample mean. In other words:\n\\[\\sigma^2 - \\frac{\\sigma^2}n\\]\nRewriting this 2, produces:\n\\[\\sigma^2\\cdot\\frac{n - 1}n\\] The variance of a sample will be biased by an amount equal to \\(\\frac{n - 1}n\\). To correct that bias we should multiply the sample variance by the inverse of this bias: \\(\\frac{n}{n-1}\\) 3. This is also called Bessel’s correction.\nSo, an unbiased measure of our sample variance is the biased sample variance times the correction factor:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n}\\cdot{\\frac n{n-1}}\\] Because the n in the denominator of the left term (the biased variance formula) cancels out the n in the numerator of the right term (the bias correction), the formula can be rewritten as:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n-1}\\]"
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#summary",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#summary",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "Summary",
    "text": "Summary\nWe calculate the variance of a sample by summing the squared deviations of each data point from the sample mean and dividing it by \\(n - 1\\). The \\(n - 1\\) actually comes from a correction factor \\(\\frac n{n-1}\\) that is needed to correct for a bias caused by taking the deviations from the sample mean rather than the population mean. Taking the deviations from the sample mean only constitutes the variance around the sample mean, but ignores the variation of the sample mean around the population mean, producing an underestimation equal to the size of the variance of the sample mean: \\(\\frac{\\sigma^2}{n}\\). The correction factor corrects for this underestimation, producing an unbiased estimate of the population variance.\nThis post was last updated on 2022-05-30."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#footnotes",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#footnotes",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere I cheat a little because in order to calculate the variance of the sample mean, I need to use the unbiased variance formula.↩︎\nHere are the steps to rewrite the formula: \\[\\sigma^2 - \\frac{\\sigma^2}n\\] Add an n to the numerator and denominator of the left term: \\[\\frac{\\sigma^2n}n - \\frac{\\sigma^2}n\\] Combine the terms: \\[\\frac{\\sigma^2n - \\sigma^2}n\\] Simplify the numerator: \\[\\frac{\\sigma^2(n - 1)}n\\] Move \\(\\sigma^2\\) out of the numerator: \\[\\sigma^2\\cdot\\frac{n - 1}n\\]↩︎\nThe inverse of \\(\\frac{n - 1}n\\) is \\(\\frac{1}{\\frac{n - 1}n}\\). Multiply both the numerator and the denominator by \\(n\\) and you get \\(\\frac{n}{n-1}\\).↩︎"
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html",
    "title": "Understanding regression (part 1)",
    "section": "",
    "text": "Statistical regression techniques are an important tool in data analysis. As a social scientist, I use it to test hypotheses by comparing differences between groups or testing relationships between variables. While it is easy to run regression analyses in a variety of software packages, like SPSS or R, it often remains a black box that is not well understood. I, in fact, do not believe I actually understand regression. Not fully understanding the mechanics of regression could be okay, though. After all, you also don’t need to know exactly how car engines work in order to drive a car. However, I think many users of regression have isolated themselves too much from the mechanics of regression. This may be the source of some errors, such as applying regression to data that is not suitable for the regression method. If you’re using regression to try and make inferences about the world, it’s probably a good idea to feel like you know what you’re doing.\nSo, there are some reasons to figure out regression. This post is Part 1 of a series of blog posts called ‘Understanding Regression’ in which I try to figure it out.\nFeel free to follow me along by copy-pasting the code from each step."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#setup",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#setup",
    "title": "Understanding regression (part 1)",
    "section": "Setup",
    "text": "Setup\nTo figure out regression, we need data. We could make up some data on the spot, but I’d rather use data that is a bit more meaningful (to me, anyway). Since I’m a big Pokémon fan, I’ll use a data set containing Pokémon statistics.\nIn case you’re following along, start by loading some packages and reading in the data. In the code section below I use the here package to read in the data, but I recommend that you simply specify the path to the file. After that, I subset the data to make the data a bit more manageable and define a custom mode function because R does not have one (and I need it later). Finally, I set some options such as the default ggplot() theme and printing options.\n\n\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(here)\nlibrary(knitr)\n\n# Read in Pokémon data\npokemon &lt;- read_csv(here(\"data\", \"pokemon.csv\"))\n\n# Create a subset with only the first 25 Pokémon\npokemon25 &lt;- filter(pokemon, pokedex &lt;= 25)\n\n# Load a custom function to calculate the mode\nmode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n# Set some options\noptions(\n  knitr.kable.NA = \"-\",\n  digits = 2\n)\n\n\nLet’s take a look at several attributes of some Pokémon to see what they’re about:\n\n\nCode\npokemon25 %&gt;%\n  filter(pokedex &lt;= 10) %&gt;%\n  select(name, type_primary, type_secondary, height, weight, \n    evolution) %&gt;%\n  kable(\n    digits = 2, \n    col.names = c(\"Name\", \"Type (primary)\", \"Type (secondary)\", \"Height\", \n      \"Weight\", \"Evolution stage\")\n  )\n\n\n\n\nTable 1: The first 10 Pokémon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType (primary)\nType (secondary)\nHeight\nWeight\nEvolution stage\n\n\n\n\nBulbasaur\nGrass\nPoison\n0.7\n6.9\n0\n\n\nIvysaur\nGrass\nPoison\n1.0\n13.0\n1\n\n\nVenusaur\nGrass\nPoison\n2.0\n100.0\n2\n\n\nCharmander\nFire\n-\n0.6\n8.5\n0\n\n\nCharmeleon\nFire\n-\n1.1\n19.0\n1\n\n\nCharizard\nFire\nFlying\n1.7\n90.5\n2\n\n\nSquirtle\nWater\n-\n0.5\n9.0\n0\n\n\nWartortle\nWater\n-\n1.0\n22.5\n1\n\n\nBlastoise\nWater\n-\n1.6\n85.5\n2\n\n\nCaterpie\nBug\n-\n0.3\n2.9\n0\n\n\n\n\n\n\n\n\nPokémon have different types (e.g., grass, fire, water), a height, some weight, and they are of a particular evolutionary stage (0, 1, or 2). This last variable refers to a Pokémon’s ability to evolve and when they do, they tend to become bigger and more powerful.\nLet’s say that we are interested in understanding the weight of different Pokémon. Below I have plotted the weight of the first 25 Pokémon, from Bulbasaur to Pikachu.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .85) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nFigure 1: Weights of the first 25 Pokémon\n\n\n\n\n\nWe see that the lightest Pokémon is Pidgey, with a weight of 1.8 kg. The heaviest Pokémon is Venusaur, with a weight of 100 kg. The average weight is 26.14 kg."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-simplest-model",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-simplest-model",
    "title": "Understanding regression (part 1)",
    "section": "The simplest model",
    "text": "The simplest model\nIn order to understand the weights of different Pokémon, we need to come up with a statistical model. In a way, this can be considered a description problem. How can we best describe the different weights that we have observed? The simplest description is a single number. We can say that all Pokémon have a weight of say… 6 kg. In other words:\n\nweight = 6 kg\n\nOf course, this is just one among many possible models. Below I plot three different models, including our weight = 6 kg model.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .85) +\n  geom_abline(intercept = 6, slope = 0, linetype = 2) +\n  geom_abline(intercept = 40, slope = 0, linetype = 2) +\n  geom_abline(intercept = 75, slope = 0, linetype = 2) +\n  annotate(\"text\", x = 28, y = 6.5, label = \"weight = 6 kg\", size = 3.5) +\n  annotate(\"text\", x = 28, y = 40.5, label = \"weight = 40 kg\", size = 3.5) +\n  annotate(\"text\", x = 28, y = 75.5, label = \"weight = 75 kg\", size = 3.5) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  coord_cartesian(xlim = c(1, 25), clip = \"off\") +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.margin = unit(c(1, 6, 1, 1), \"lines\")\n  )\n\n\n\n\n\n\n\n\nFigure 2: Three different weight models\n\n\n\n\n\nWhile a model like weight = 6 kg is a valid model, it is not a very good model. In fact, it only perfectly describes Pikachu’s weight and inaccurately describes the weight of the remaining 24 Pokémon. The other models, such as weight = 40 kg might be even worse; they do not even describe a single Pokémon’s weight correctly, although they do get closer to some of the heavier Pokémon. How do we decide which model is the better model? In order to answer that question, we need to consider the model’s error."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-error-of-models",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-error-of-models",
    "title": "Understanding regression (part 1)",
    "section": "The error of models",
    "text": "The error of models\nThe error of a model is the degree to which the model inaccurately describes the data. There are several ways to calculate that error, depending on how you define error. We will cover three of them.\nThe first definition of error is simply the sum of times that the model inaccurately describes the data. For each observation we check whether the model correctly describes it or not. We then sum the number of misses and consider that the amount of error for that model. With our weight = 6 kg the answer is 24; out of the 25 Pokémon only Pikachu has a weight of 6, which means the model is correct once and wrong 24 times.\nWe can now compare different models to one another by calculating the error for a range of models. Below I plot the number of errors for 100 different models, starting with the model weight = 1 kg, up to weight = 10 kg, in steps of 0.1. Ideally we would test more models (up to the heaviest Pokémon we know of), but for the sake of visualizing the result, I decided to only plot a small subset of models.\n\n\nCode\nerrors_binary &lt;- expand_grid(\n    model = seq(from = 1, to = 10, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %&gt;%\n  mutate(error = if_else(abs(weight - model) == 0, 0, 1)) %&gt;%\n  group_by(model) %&gt;%\n  summarize(error_sum = sum(error))\n\nggplot(errors_binary, aes(x = model, y = error_sum)) +\n  geom_line() + \n  coord_cartesian(ylim = c(0, 25)) +\n  scale_x_continuous(breaks = 1:10) +\n  labs(x = \"Model (weight = x kg)\", y = \"Error (sum of errors)\")\n\n\n\n\n\n\n\n\nFigure 3: Error #1: The sum of (binary) errors\n\n\n\n\n\nWe see that almost all models perform poorly. The errors range from 23 to 25. Most models seem to have an error of 25, which means they do not accurately describe any of the 25 Pokémon. Some have an error of 24, meaning they describe the weight of 1 Pokémon correctly. There is 1 model with an error of 23: weight = 6.9 kg. Apparently there are 2 Pokémon with a weight of 6.9, which means that this model outperforms the others.\nDespite there being a single model that outperforms the others in this set of models, it’s still a pretty poor model. After all, it is wrong 23 out of 25 times. Perhaps there are some models that outperform this model, but it’s unlikely. That’s because we’re defining error here in a very crude way. The model needs to exactly match the weight of the Pokémon, or else it counts as an error. Saying a weight is 6 kg, while it is in fact 10 kg, is as wrong as saying the weight is 60 kg.\nInstead of defining error in this way, we can redefine it so that it takes into account the degree of error. We can define error as the difference between the actual data point and the model’s value. So, in the case of our weight = 6 kg model, an actual weight of 10 kg would have an error of 10 - 6 = 4. This definition of error is often referred to as the residual.\nBelow I plot the residuals of the first 25 Pokémon for our weight = 6 kg model.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .5) +\n  geom_segment(aes(xend = pokedex, y = 6, yend = weight), linetype = 2) +\n  geom_point() +\n  geom_abline(intercept = 6, slope = 0) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nFigure 4: Residuals of the weight = 6 kg model\n\n\n\n\n\nWe can add up all of the (absolute) residuals to determine the model’s error. Just like with the binary definition of error, we can then compare multiple models. This is what you see in the graph below. For each model, this time ranging from weight = 1 kg to weight = 100 kg, the absolute residuals were calculated and added together.\n\n\nCode\nerrors_residuals &lt;- expand_grid(\n    model = seq(from = 1, to = 100, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %&gt;%\n  mutate(error = abs(weight - model)) %&gt;%\n  group_by(model) %&gt;%\n  summarize(error_sum = sum(error))\n\nggplot(errors_residuals, aes(x = model, y = error_sum)) +\n  geom_line() +\n  scale_y_continuous(breaks = seq(from = 500, to = 1900, by = 200)) +\n  labs(x = \"Model (weight = x kg)\", y = \"Error (sum of residuals)\")\n\n\n\n\n\n\n\n\nFigure 5: Error #2: The sum of residuals\n\n\n\n\n\nThis graph looks very different compared to the graph where we calculated the error defined as the sum of misses. Now we see that some kind of minimum appears. Unlike the binary definition of error, it now looks like there are fewer best models. More importantly, though, we have defined error in a less crude manner, meaning that the better models indeed capture the data much better than before.\nBut we might still not be entirely happy with this new definition of error either. Calculating the sum of absolute residuals for each model comes with another conceptual problem.\nWhen you sum the number of absolute errors, four errors of 1 are equal to a single error of 4. In other words, you could have a model that is slightly off multiple times or one that might make fewer, but larger, errors. Both would be counted as equally wrong. What do we think of that? Conceptually speaking, we might find it more problematic when a model is very wrong than when the model is slightly off multiple times. If we think that, we need another definition of error.\nTo address this issue, we can square the residuals before adding them together. That way, larger errors become relatively larger compared to smaller errors. Using our previous example, summing four residuals of 1 remains 4, but a single residual of 4 becomes 4 * 4 = 16. The model now gets punished more severely for making large mistakes.\nUsing this new definition of error, we again plot the error for each model, from 1 to 100.\n\n\nCode\nerrors_squared_residuals &lt;- expand_grid(\n    model = seq(from = 1, to = 100, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %&gt;%\n  mutate(error = abs(weight - model)^2) %&gt;%\n  group_by(model) %&gt;%\n  summarize(error_sum = sum(error))\n\nggplot(errors_squared_residuals, aes(x = model, y = error_sum)) +\n  geom_line() +\n  geom_vline(xintercept = mean(pull(pokemon25, weight)), linetype = 2) +\n  labs(x = \"Model\", y = \"Error (sum of squared residuals)\")\n\n\n\n\n\n\n\n\nFigure 6: Error #3: The sum of squared residuals\n\n\n\n\n\nWe see a smooth curve, with a clear minimum indicated by the vertical dashed line. This vertical line indicates the model that best describes the data. What is the value of the best model exactly? In this case, the answer is 26.14. And it turns out, there is an easy way to determine this value."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-data-driven-model",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-data-driven-model",
    "title": "Understanding regression (part 1)",
    "section": "The data-driven model",
    "text": "The data-driven model\nRather than setting a specific value and seeing how it fits the data, we can also use the data to determine the value that best fits the data. In the previous graph we saw that the best fitting model is one where the weight is equal to 26.14. This value turns out to be the mean of the different weights we have observed in our sample. Had we defined error as simply the sum of absolute residuals, this would be a different value. In fact, the best fitting value would then be equal to 13, or the median. And had we used the binary definition of error, the best fitting value would be the mode, which in our case is: 6.9.\nNote that there is not always a unique answer to which model is the best fitting model, depending on the error definition. For example, it is possible that there are multiple modes. If you use the binary definition of error, that would mean there are multiple equally plausible models. This can be another argument to not define a model’s error in such a crude way.\nThe table below shows an overview of which technique can be used to find the best fitting value, depending on the error definition.\n\n\nCode\ntibble(\n  error_definition = c(\"sum of errors\", \"sum of absolute residuals\", \n    \"sum of squared residuals\"),\n  estimation_technique = c(\"mode\", \"median\", \"mean\")\n) %&gt;%\n  kable(col.names = c(\"Error definition\", \"Estimation technique\"), digits = 2) \n\n\n\n\n\nError definition\nEstimation technique\n\n\n\n\nsum of errors\nmode\n\n\nsum of absolute residuals\nmedian\n\n\nsum of squared residuals\nmean\n\n\n\n\n\nWe can now update our model to refer to the estimation technique, rather than a fixed value. Given that the third definition of error seems to be most suitable, both pragmatically and conceptually, we’ll use the mean:\n\nweight = mean(weight)\n\nThis is also the value you get when you perform a regression analysis in R:\n\n\nCode\nlm(weight ~ 1, data = pokemon25)\n\n\n\nCall:\nlm(formula = weight ~ 1, data = pokemon25)\n\nCoefficients:\n(Intercept)  \n       26.1  \n\n\nBy regressing weight onto 1 we are telling R to run an intercept-only model. This means that R will estimate which line will best fit all the values in the outcome variable, just like we have done ourselves earlier by testing different models such as weight = 6 kg.\nThe result is an intercept value of 26.14, which matches the mean of the weights.\nSo, we now know where the intercept comes from when we run an intercept-only model. It is the mean of the data we are trying to model. Note that it is the mean because we defined the model’s error as the sum of squared residuals. Had we defined the error differently, such as the sum of absolute residuals or the sum of errors, the intercept would be the median or mode of the data instead. Why did we use the sum of squared residuals? We had a conceptual reason of wanting to punish larger residuals relatively more than several smaller errors. It turns out there is another reason to favor squared residuals, which has to do with a nice property of the mean vs. the median. This will be covered in Part 2 of ‘Understanding Regression’.\nThis post was last updated on 2022-04-29."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html",
    "href": "content/projects/statcheck/statcheck.html",
    "title": "statcheck",
    "section": "",
    "text": "Together with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer statistics-related typos."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#why-is-this-important",
    "href": "content/projects/statcheck/statcheck.html#why-is-this-important",
    "title": "statcheck",
    "section": "Why is this important?",
    "text": "Why is this important?\nSimilar to my tidystats project, our aim is to address a particular problem in statistics reporting: the reporting of incorrect statistics.\nAs has been shown by Michèle and her colleagues, statistics are often reported incorrectly (Nuijten et al., 2016). This is likely due to the fact that researchers do not have the necessary software tools to reliably take the output of statistics from their data analysis software and enter it into their text editor. Instead, researchers are likely to copy statistics from the output by hand or by copy-pasting the output. Both techniques are error-prone, resulting in many papers containing statistical typos. This is a problem because statistical output is used in meta-analyses to aggregate the evidence for particular theories, which sometimes also inform policy. In some cases, the errors may even be so large that it affects the conclusion drawn from the statistical test."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#what-am-i-working-on",
    "href": "content/projects/statcheck/statcheck.html#what-am-i-working-on",
    "title": "statcheck",
    "section": "What am I working on?",
    "text": "What am I working on?\nAdmittedly, I am simply joining Michèle and her efforts to help researchers make fewer typos. She and her colleagues have already done a lot of the work—we’re now just trying to make it even better. For example, Sacha Epskamp and Michèle developed statcheck. statcheck is an R package designed to catch statistical reporting mistakes. It works by first extracting statistics from a paper (e.g., t values, degrees of freedom, p-values). It then uses the test statistic and degrees of freedom to re-calculate the p-value and compare it to the reported p-value. If the two don’t match, there is probably a reporting mistake.\nYou can use the statcheck package in R to check your paper or you can use the web app. Using the web app consists of simply uploading your paper and checking the results. You can then go back to the paper and correct the mistakes.\nWith my experience creating tidystats, and particularly the tidystats Word add-in, we’ve started to create a Word add-in for statcheck. This add-in allows researchers to scan their document for statistical inconsistencies, find them, and fix them. This add-in is currently in beta and we hope to release it soon.\nWe are also working on improving statcheck together with the eScience Center. Together with their help we hope to expand statcheck so it can catch a greater variety of statistical inconsistencies. We have had some preparatory meetings with them and plan to fully begin this project soon."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#links",
    "href": "content/projects/statcheck/statcheck.html#links",
    "title": "statcheck",
    "section": "Links",
    "text": "Links\n\nThe web app\nThe R package on CRAN\nThe GitHub page of statcheck\nThe GitHub page of the statcheck Word add-in."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html",
    "title": "Cognitive dissonance",
    "section": "",
    "text": "Cognitive dissonance refers to a state of aversive arousal that is experienced when people realize they possess mutually inconsistent cognitions. This state is the foundation of cognitive dissonance theory (CDT)—a theory developed by Leon Festinger in 1957. Several of my projects are aimed at assessing the evidence for this theory and at applying this theory to other issues (although now that I’ve left academia, these projects have been deprioritized)."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#why-is-this-important",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#why-is-this-important",
    "title": "Cognitive dissonance",
    "section": "Why is this important?",
    "text": "Why is this important?\nThe theory of cognitive dissonance can explain many different phenomena that we should understand so that we may intervene and improve the lives of others. For example, cognitive dissonance theory has been used to explain religious beliefs, unhealthy behaviors, and people’s attitude towards animals.\nBefore the theory can be applied, however, it needs to be verified. We need to have sufficient evidence to believe in the theory. The social psychological evidence we have for the theory is, however, quite weak. The research stems from old research, mostly conducted in the 50s, 60s, and 70s. While this would not necessarily be a problem, it is a problem in the case of social psychology. The original studies were conducted with extremely low sample sizes and without pre-registration, or other tools that limit p-hacking. This means that many past findings may be false positives, which is supported by recent findings that show many findings in psychology do not replicate."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#what-am-i-working-on",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#what-am-i-working-on",
    "title": "Cognitive dissonance",
    "section": "What am I working on?",
    "text": "What am I working on?\nI am one of the lead investigators of a large-scaled replication project. In this project, we will try and replicate a seminal finding in the cognitive dissonance literature. Specifically, our aim is to replicate the classic finding that people who write a counterattitudinal essay (e.g., students arguing in favor of a tuition increase) become more in favor of the position they argued for. We have submitted this project as a registered report to Advanced in Methods and Practices in Psychological Science (AMPPS). There it has received an in-principle acceptance. Data collection is currently underway.\nI also hope to start up a meta-analysis project to produce live reviews of studies from the cognitive dissonance literature."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#links",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#links",
    "title": "Cognitive dissonance",
    "section": "Links",
    "text": "Links\n\nThe landing page of our large-scaled replication project\nThe stage-1 accepted manuscript"
  },
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "About",
    "section": "",
    "text": "I’m a Senior Behavioral Scientist at Rethink Priorities. Rethink Priorities is a research organization that houses a bunch of cool people who support and conduct research to inform policymakers and major foundations about how to best help people and nonhuman animals, in both the present and the long-term future. I am part of the survey team, which means I conduct research on attitude assessments and attitude change, using surveys and experimental studies.\nBefore joining Rethink Priorities, I was an assistant professor in the Department of Social Psychology at Tilburg University.\nOn this website you can find information about some of the projects I’m involved in. Some notable projects I’m working on are tidystats and a large-scaled replication study of cognitive dissonance. Besides writing about these projects, I also blog posts about various topics, including tutorials or opinion pieces.\nOne of my main research interests concern animal welfare. I think animal welfare, and their lack thereof, is one of the most pressing issues in the world at this moment and as a fruitful area of research where influential theories in social psychology (such as cognitive dissonance) can be applied and tested.\nI’m also interested in the methodology of psychological research and ways to improve how we conduct science. A notable project I’m working on is tidystats. This is a software solution to help researchers more easily and more reproducibly report statistics in scientific manuscripts. It’s main goal is to get researchers to report more statistics with fewer errors. I’m pretty proud of this project, so please check it out on the tidystats project or the tidystats website.\nI also have teaching experience thanks to my time as an assistant professor. I’m quite experienced in teaching undergraduate courses, in both small and large groups of students. Besides course work I have also provided many R workshops (although I have less time for that now).\nThis website is created using Quarto."
  },
  {
    "objectID": "content/blog.html",
    "href": "content/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nFormatting numbers\n\n\n\n\n\n\nR\n\n\nfunction\n\n\n\nA showcase of the function I use to format numbers.\n\n\n\n\n\nMay 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe metalog distribution\n\n\n\n\n\n\nstatistics\n\n\ndistributions\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nApr 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting values of intercept only models\n\n\n\n\n\n\nstatistics\n\n\nprediction\n\n\n\nA post on how to predict values of intercept-only models.\n\n\n\n\n\nMar 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSad wild animal facts\n\n\n\n\n\n\nwild animals\n\n\nwild animal suffering\n\n\n\nA collection of facts about wild animals that make me sad.\n\n\n\n\n\nMar 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian tutorial: Two groups\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\nThe fourth of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to model the difference between two groups.\n\n\n\n\n\nApr 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian tutorial: Correlation\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\nThe third of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to model a correlation.\n\n\n\n\n\nFeb 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian tutorial: Single predictor regression\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\nThe second of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to run a regression with a single predictor.\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian tutorial: Intercept-only model\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\nThe first of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to run an intercept-only regression model.\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nVoting behavior of Dutch political parties on animal welfare motions\n\n\n\n\n\n\nanimal welfare\n\n\ndata cleaning\n\n\nAPIs\n\n\npolitics\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAnimals slaughtered in the Netherlands\n\n\n\n\n\n\nanimal welfare\n\n\ndata cleaning\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation-based power curves\n\n\n\n\n\n\npower analysis\n\n\nsimulation\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nNov 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation-based power analyses\n\n\n\n\n\n\nstatistics\n\n\npower analysis\n\n\nsimulation\n\n\n\nSimulation-based power analyses make it easy to understand what power is: Power is simply counting how often you find the results you expect to find. Running simulation-based power analyses might be new for some, so in this blog post I present code to simulate data for a range of different scenarios.\n\n\n\n\n\nOct 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThe right order of method sections\n\n\n\n\n\n\nwriting\n\n\nstatistics\n\n\npower analysis\n\n\n\nMethod sections in academic (psychology) papers usually consist of the following sections: Participants, Design, Procedure, and Materials. They also tend to be presented in this order. But is this, generally speaking, the right order? I don’t think so.\n\n\n\n\n\nJul 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nA tidystats example\n\n\n\n\n\n\ntidystats\n\n\nstatistics\n\n\ntutorial\n\n\n\nI illustrate how to use my tidystats software to analyze and report the results of a replication study that was part of the Many Labs 1 project.\n\n\n\n\n\nApr 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWhy divide by \\(n - 1\\) to calculate the variance of a sample?“\n\n\n\n\n\n\nstatistics\n\n\n\nIn a recent tweet I asked the question why we use \\(n - 1\\) to calculate the variance of a sample. Many people contributed an answer, but many of them were of the type I feared. Most consisted of some statistical jargon that confuses me more, rather than less. Other responses were very useful, though, so I recommend checking out the replies to the tweet. In this post, I will try to describe my favorite way of looking at the issue.\n\n\n\n\n\nAug 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nUseful power analysis papers\n\n\n\n\n\n\nstatistics\n\n\npower analysis\n\n\n\nA curious thing happened in the field of social psychology: Social psychologists finally realized that statistical power is important. Unfortunately, they then skipped the step of figuring out how to do them correctly. Here I list some papers on power analyses that I hope help in improving the way we do them.\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding regression (part 1)\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nregression\n\n\n\nThis is Part 1 of a series of blog posts on how to understand regression. The goal is to develop an intuitive understanding of the different components of regression. In this first post, we figure out where the estimate of an intercept-only regression model comes from.\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\nNo matching items"
  }
]