[
  {
    "objectID": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html",
    "href": "content/posts/9-simulation-based-power-analyses/simulation-based-power-analyses.html",
    "title": "Simulation-based power analyses",
    "section": "",
    "text": "Conclusion\nIn this post I presented code to perform a simulated-based power analysis for several scenarios. In the future I hope to expand on the scenarios, but I think the scenarios included so far already reveal a few interesting things. In some cases, it’s rather trivial to simulate the data. The mvrnorm() function works wonders for simulating the data by letting you set empirical to TRUE, thereby allowing you to inspect the simulated data. More importantly, though, I think that simulation-based power analyses are pedagogical. It takes the magic out of power analyses because power is nothing more than counting how often you find the significant results you expect to find. Not only that, the simulation approach also means that if you can simulate the data, you can calculate the power. Maybe that’s easier said than done, but that’s where my example code comes in. Hopefully it provides you with the code you can adapt to your own scenario so you can run the correct power analysis.\nThis post was last updated on 2022-04-29.\n\n\n\n\n\n\n\nReferences\n\nMaxwell, S. E. (2004). The persistence of underpowered studies in psychological research: Causes, consequences, and remedies. Psychological Methods, 9(2), 147–163. https://doi.org/10.1037/1082-989X.9.2.147"
  },
  {
    "objectID": "content/posts/6-a-tidystats-example/a-tidystats-example.html",
    "href": "content/posts/6-a-tidystats-example/a-tidystats-example.html",
    "title": "A tidystats example",
    "section": "",
    "text": "Lorge and Curtiss (1936) examined how a quotation is perceived when it is attributed to a liked or disliked individual. In one condition the quotation was attributed to Thomas Jefferson and in the other it was attributed to Vladimir Lenin. They found that people agree more with the quotation when the quotation was attributed to Jefferson than Lenin. In the Many Labs replication study (Klein et al., 2014), the quotation was attributed to either George Washington, the liked individual, or Osama Bin Laden, the disliked individual. The also used a different quotation, which was:\n\nI have sworn to only live free, even if I find bitter the taste of death.\n\nWe are again interested in testing whether the source of the quotation affects how it is evaluated. The evaluation was assessed on a 9-point Likert scale ranging from 1 (strongly agree) to 9 (strongly disagree). I reverse coded this in the data that we’ll use. You can follow along by copy-pasting the code from this example.\nBefore getting into how tidystats should be used, let’s first simply analyze the data. I have designed tidystats to be minimally invasive. In other words, to use tidystats, you do not need to substantially change your data analysis workflow.\nWe’ll start with a basic setup where we load some packages and the data.\n\n\nCode\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(tidystats)\nlibrary(viridis)\nlibrary(knitr)\n\n# Load example data\ndata <- data(quote_source)\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n# Set options\noptions(\n  knitr.kable.NA = \"-\",\n  digits = 2\n)\n\n\nOur main effect of interest is the difference in responses to the quote between the two conditions. Here I visualize this different with a violin plot.\n\n\nCode\nggplot(quote_source, aes(x = source, y = response, fill = source)) +\n  geom_violin(width = .5, alpha = .85) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  labs(x = \"Quote source\", y = \"Quote agreement\") +\n  scale_y_continuous(breaks = c(1, 3, 5, 7, 9)) +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75) +\n  guides(fill = \"none\")\n\n\n\n\n\nFigure 1: Difference in responses between the two conditions\n\n\n\n\nThis looks like the effect is in the expected direction. Participants agreed more with the quotation when they believed the quote to be from George Washington compared to Osama Bin Laden.\nRegarding descriptives, tidystats comes with its own functions to calculate descriptives. One of them is the describe_data function, inspired by the describe function from the psych package. You can use it together with group_by from the dplyr package to calculate a set of descriptives for multiple groups.\n\n\nCode\nquote_source %>%\n  group_by(source) %>%\n  describe_data(response) %>%\n  select(-var) %>%\n  kable(col.names = c(\"Source\", \"Missing\", \"N\", \"M\", \"SD\", \"SE\", \"Min\", \"Max\", \n    \"Range\", \"Median\", \"Mode\", \"Skew\", \"Kurtosis\")) \n\n\n\n\nTable 1: Response descriptives\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nMissing\nN\nM\nSD\nSE\nMin\nMax\nRange\nMedian\nMode\nSkew\nKurtosis\n\n\n\n\nBin Laden\n18\n3083\n5.2\n2.1\n0.04\n1\n9\n8\n5\n5\n-0.08\n2.6\n\n\nWashington\n0\n3242\n5.9\n2.2\n0.04\n1\n9\n8\n6\n5\n-0.23\n2.2\n\n\n\n\n\n\nTo test whether the difference in agreement between the two sources is statistically significant, we perform a t-test. Normally, we would just run the t-test like so:\n\n\nCode\nt.test(response ~ source, data = quote_source)\n\n\nHowever, since we want to use tidystats to later save the statistics from this test, we will store the output of the t-test in a variable. This, and the final section of R code, will be the only thing you need to change in order to incorporate tidystats in your workflow.\nOnce you’ve stored the result of the t-test in a variable, you can look at the output by sending it the console, which will print the output.\n\n\nCode\nmain_test <- t.test(response ~ source, data = quote_source)\nmain_test\n\n\n\n    Welch Two Sample t-test\n\ndata:  response by source\nt = -13, df = 6323, p-value <2e-16\nalternative hypothesis: true difference in means between group Bin Laden and group Washington is not equal to 0\n95 percent confidence interval:\n -0.80 -0.59\nsample estimates:\n mean in group Bin Laden mean in group Washington \n                     5.2                      5.9 \n\n\nThis shows us that there is a statistically significant effect of the quote source, consistent with the hypothesis.\nNext, let’s run some additional analyses. One thing we can test is whether the effect is stronger in the US compared to non-US countries. To test this, we perform a regression analysis. Here we also store the result in a variable, but this is actually quite common in regression analyses because you want to apply the summary function to this variable in order to obtain the inferential statistics.\n\n\nCode\nus_moderation_test <- lm(response ~ source * us_or_international, \n  data = quote_source)\nsummary(us_moderation_test)\n\n\n\nCall:\nlm(formula = response ~ source * us_or_international, data = quote_source)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.005 -1.228 -0.228  1.772  3.772 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                              5.2488     0.0849   61.83  < 2e-16 ***\nsourceWashington                         0.4052     0.1172    3.46  0.00055 ***\nus_or_internationalUS                   -0.0210     0.0955   -0.22  0.82589    \nsourceWashington:us_or_internationalUS   0.3717     0.1323    2.81  0.00497 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.2 on 6321 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.0275,    Adjusted R-squared:  0.027 \nF-statistic: 59.5 on 3 and 6321 DF,  p-value: <2e-16\n\n\nThere appears to be a significant interaction. Let’s inspect the interaction with a graph:\n\n\nCode\nggplot(quote_source, aes(x = us_or_international, y = response, \n    fill = source)) +\n  geom_violin(alpha = .85) +\n  stat_summary(fun.data = \"mean_cl_boot\", position = position_dodge(.9)) +\n  labs(x = \"Region\", y = \"Quote agreement\", fill = \"Source\") +\n  scale_y_continuous(breaks = c(1, 3, 5, 7, 9)) +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\n\n\n\nFigure 2: Source by region interaction\n\n\n\n\nWe see that the effect of the source appears to be larger in the US. Given that the positive source was George Washington, this makes sense.\nLet’s do one more analysis to see whether the effect is stronger in a lab setting compared to an online setting.\n\n\nCode\nlab_moderation_test <- lm(response ~ source * lab_or_online, data = quote_source)\nsummary(lab_moderation_test)\n\n\n\nCall:\nlm(formula = response ~ source * lab_or_online, data = quote_source)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.967 -1.197 -0.197  1.737  3.803 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                            5.1971     0.0567   91.59   <2e-16 ***\nsourceWashington                       0.6864     0.0791    8.68   <2e-16 ***\nlab_or_onlineonline                    0.0664     0.0780    0.85     0.39    \nsourceWashington:lab_or_onlineonline   0.0172     0.1089    0.16     0.87    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.2 on 6321 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.0255,    Adjusted R-squared:  0.025 \nF-statistic: 55.1 on 3 and 6321 DF,  p-value: <2e-16\n\n\nWe see no significant interaction in this case. This means we do not find evidence that running the study in an online setting significantly weakens the effect; good to know!"
  },
  {
    "objectID": "content/posts/6-a-tidystats-example/a-tidystats-example.html#applying-tidystats",
    "href": "content/posts/6-a-tidystats-example/a-tidystats-example.html#applying-tidystats",
    "title": "A tidystats example",
    "section": "Applying tidystats",
    "text": "Applying tidystats\nNow let’s get to tidystats. We have three analyses we want to save: a t-test and two regression analyses. We stored each of these analyses in separate variables, called main_test, us_moderation_test, and lab_moderation_test.\nThe main idea is that we will add these three three variables to a list and then save the list as a file on our computer. You create an empty list using the list function. Once you have an empty list, you can add statistics to this list using the add_stats function. add_stats accepts a list as its first argument, followed by a variable containing a statistics model. In our case, this means we need to use the add_stats function three times, as we have three different analyses we want to save. Since this can get pretty repetitive, we will use the piping operator to pipe the three steps together and save some typing.\nBefore we do so, however, note that we can take this opportunity to add some meta-information to each test. For the sake of this example, let’s say that the t-test was our primary test. We also had a suspicion that the location (US vs. international) would matter, but it wasn’t our main interest. Nevertheless, we preregistered these two analyses. During data analysis, we figured that it might also matter whether the study was conducted in the lab or online, so we tested it. This means that this is an exploratory analysis. With add_stats, we can add this information when we add the test to our empty list.\nIn the end, the code looks like this:\n\n\nCode\nresults <- list()\n\nresults <- results %>%\n  add_stats(main_test, type = \"primary\", preregistered = TRUE) %>%\n  add_stats(us_moderation_test, type = \"secondary\", preregistered = TRUE) %>%\n  add_stats(lab_moderation_test, type = \"secondary\", preregistered = FALSE)\n\n\nI recommend to do this at the end of the data analysis script in a section called ‘tidystats’. This confines most of the tidystats code to a single section, keeping it organized, and it will keep most of your script readable to those unfamiliar with tidystats.\nAfter all the analyses are added to the list, the list can be saved as a .json file to your computer’s disk. This is done with the write_stats function. The function requires the list as its first argument, followed by a file path. I’m a big fan of using RStudio Project files so that you can define relative file paths. In this case, I create the .json file in the ‘Data’ folder of my project folder.\n\n\nCode\nwrite_stats(results, \"tidystats-example.json\")\n\n\nIf you want to see what this file looks like, you can inspect it here. Open the file in a text editor to see how the statistics are structured. As you will see, it is not easy for our human eyes to quickly see the results, but it’s easy for computers.\nOnce you’ve saved the file, you can share the file with others or use it to report report the results in your manuscript using the Word add-in.\nThat marks the end of this tidystats example. If you have any questions, please check out the tidystats website or contact me via Twitter.\nThis post was last updated on 2022-04-29."
  },
  {
    "objectID": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html",
    "href": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html",
    "title": "Animals slaughtered in the Netherlands",
    "section": "",
    "text": "In this post I take a look at how many animals are slaughtered in the Netherlands. The goal is to find and clean the data to answer this question and to get a better grasp of exactly how many animals are killed here every hear. It’s both an exercise in data cleaning and calibrating one’s beliefs about this topic, as this is something that’s too easy to avoid thinking about, while probably being one of the most important things you should think about."
  },
  {
    "objectID": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data",
    "href": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data",
    "title": "Animals slaughtered in the Netherlands",
    "section": "Data",
    "text": "Data\nThe data on the number of animals slaughtered in the Netherlands can be found on StatLine. This is a database managed by CBS, the national statistical office of the Netherlands. Specifically, we’re going to take a look at the meat production numbers (vleesproductie). These numbers can be found in a table here. We can adapt what is in the table by changing various filters. By default it shows both the number of animals and the weight of the animals. I’m only interested in the number of animals, so I deselect the weight-related rows. I also see that they offer data on more dates than is shown by default, so I select all of the dates. I then download the data as a .csv file using the button in the top right corner. Now we can start cleaning the data for our purposes."
  },
  {
    "objectID": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#setup",
    "href": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#setup",
    "title": "Animals slaughtered in the Netherlands",
    "section": "Setup",
    "text": "Setup\nRun the following setup code if you want to follow along. You can download the data yourself or use my file.\n\n\nCode\n# Load packages\nlibrary(tidyverse)\n\n# Load data\ndata <- read_csv2(\"meat-production-netherlands.csv\")\n\n# Set default ggplot\ntheme_set(theme_minimal())\n\n\nNote that we have to use read_csv2() because the data values are separated by a semi-colon. This is an annoying default in the Netherlands (and probably elsewhere in Europe)."
  },
  {
    "objectID": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data-cleaning",
    "href": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data-cleaning",
    "title": "Animals slaughtered in the Netherlands",
    "section": "Data cleaning",
    "text": "Data cleaning\nLet’s begin by inspecting the first few rows of the data.\n\n\nCode\nhead(data)\n\n\n\n\n  \n\n\n\nIt should be no surprise, but the data is in Dutch. Let’s translate the data, starting with the columns. One of the columns is called Aantal slachtingen (x 1 000), which means number of slaughtered animals in units of 1000. Instead of translating this directly, I will simply rename it to count and multiply the values by 1000.\n\n\nCode\ndata <- data %>%\n  rename(\n    animal = Slachtdieren,\n    period = Perioden,\n    count = `Aantal slachtingen (x 1 000)`\n  ) %>%\n  mutate(count = count * 1000)\n\n\nLet’s clean up the period column next. It seems like it contains the year and the month (in Dutch). I can translate the month names to Dutch, but I first want to make sure that all data values are structured the same way. count() is a great function to inspect that.\n\n\nCode\ndata %>%\n  count(period)\n\n\n\n\n  \n\n\n\nCuriously, not all rows in the data contain both the year and the month. Some only have the year. This is important because that means we can’t just sum the number of slaughtered animals per year because that means we’ll actually get twice the number of animals because we’ll sum both the animals slaughtered in that year and each month of that year. The last several months also have an asterisk in the month name. This asterisk indicates that the data for these months has not yet been finalized.\nWhat I want to do next is create a new column that only contains the year and another column that contains the month. Creating the year column is easy because we can use parse_number() to extract the year from the data. The month is a bit trickier, but we can use a regular expression to remove the year, leaving us with the month. We use str_remove() and tell it to remove a string pattern that consists of 4 numbers and a space. We can also use it to remove the asterisk from the more recent months, but first we add a column to say whether the numbers are final or not based on this asterisk. After doing that, we recode the month values that need to be translated and also convert the empty string to a missing value. Finally, we remove the period column because we don’t need it anymore.\n\n\nCode\ndata <- mutate(data,\n    year = parse_number(period),\n    month = str_remove(period, \"[0-9]{4} ?\"),\n    final = if_else(str_detect(period, \"\\\\*\"), \"no\", \"yes\"),\n    month = str_remove(month, \"\\\\*\"),\n    month = recode(month,\n      \"augustus\" = \"august\",\n      \"februari\" = \"february\",\n      \"januari\" = \"january\",\n      \"juli\" = \"july\",\n      \"juni\" = \"june\",\n      \"maart\" = \"march\",\n      \"mei\" = \"may\",\n      \"oktober\" = \"october\",\n    ),\n    month = na_if(month, \"\"),\n    period = NULL\n  )\n\n\nNext are the animals. Let’s take a look at the unique values we have.\n\n\nCode\ncount(data, animal)\n\n\n\n\n  \n\n\n\nHmm… it looks like there are a few challenges here. First, we seem to have both total values and non-total values, so we should take care to separate these. Second, we need to figure out what each word means. Even my Dutch is not helping me in understanding each type of animal.\nLet’s first simply translate the values so we get a better grasp of what we are dealing with. The translations won’t be direct translations. Instead, I already think about what kind of categories make sense and how I want to plot the data later, so I translate the values into names that will also be useful later.\n\n\nCode\ndata <- mutate(data, \n  animal = recode(animal, \n    \"Eenhoevigen\" = \"ungulates (mostly horses)\",\n    \"Geiten (totaal)\" = \"goats\",\n    \"Kalkoenen\" = \"turkeys\",\n    \"Kalveren jonger dan 9 maanden\" = \"calves (< 9 months)\",\n    \"Kalveren van 9 tot en met 12 maanden\" = \"calves (9-12 months)\",\n    \"Koeien\" = \"cows\",\n    \"Overig pluimvee\" = \"poultry (misc)\",\n    \"Overige kippen\" = \"chicken (mostly layers)\",\n    \"Rundvee (totaal)\" = \"cattle\",\n    \"Schapen incl. lammeren\" = \"sheep\",\n    \"Schapenlammeren\" = \"lambs\",\n    \"Stieren\" = \"bulls\",\n    \"Totaal kalveren\" = \"calves\",\n    \"Totaal volwassen runderen\" = \"adult cattle (total)\",\n    \"Vaarzen\" = \"heifers\",\n    \"Varkens (totaal)\" = \"pigs\",\n    \"Vleeskuikens\" = \"broilers\"\n  )\n)\n\n\nTranslating the words was very helpful to better understand the data. One thing that’s clear is that some of the values are totals of other values. Below I list which values in the data are actually sums of other values:\n\nadult cattle: Total of cows, heifers, and bulls\ncattle: Total of adult cattle and calves\ncalves: Total of calves (< 9 months) and calves (9-12 months)\n\nIf we are interested in what the totals are made of, we can remove the total columns and reconstruct them later if we want to. This works for the first two total columns, but not calves because they only started making the distinction between young and older calves in 2009. So let’s instead remove the values that the total values are made of and only keep the total values.\n\n\nCode\ndata <- filter(data, !animal %in% c(\"adult cattle (total)\", \"cows\", \n  \"heifers\", \"bulls\", \"calves\",\"calves (< 9 months)\", \n  \"calves (9-12 months)\", \"lambs\")\n)\n\n\nThis leaves us with the following animals.\n\n\nCode\ncount(data, animal)\n\n\n\n\n  \n\n\n\nThis looks fine to me, which means we are almost done with the data cleaning. At this point I want to create two separate data frames: one that only contains the annual data and one that contains the monthly data. This is easy to do because we can take all the annual data by simply selecting the rows with a missing value in the month column. Since the month column is useless in that data frame, we remove it.\n\n\nCode\ndata_annual <- data %>%\n  filter(is.na(month)) %>%\n  select(-month)\n\ndata <- filter(data, !is.na(month))\n\n\nAs a final step we can combine the year and month columns from the monthly data frame into a single column, which will be useful for plotting the data later. This requires a special function from the zoo package.\n\n\nCode\ndata <- mutate(data,\n  month = str_to_sentence(month),\n  month = match(month, month.name),\n  year_month = paste(year, month, \"1\", sep = \"-\"),\n  year_month = lubridate::as_date(year_month),\n  year_month = zoo::as.yearmon(year_month)\n)"
  },
  {
    "objectID": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data-analysis",
    "href": "content/posts/16-animals-slaughtered-netherlands/animals-slaughtered-netherlands.html#data-analysis",
    "title": "Animals slaughtered in the Netherlands",
    "section": "Data analysis",
    "text": "Data analysis\nWith the data cleaned up we can start to ask some questions. Let’s begin with a graph that shows as much of the data as possible. That means plotting the monthly data for each animal.\n\n\nCode\nggplot(data, aes(x = year_month, y = count)) +\n  geom_point(size = 1) +\n  geom_line(alpha = .25) +\n  facet_wrap(~ animal, scales = \"free\") \n\n\n\n\n\nNumber of slaughtered animals per month and animal\n\n\n\n\nA few observations:\n\nThe numbers are very high, particularly for some animals (e.g., broilers)\nSome animals used to be slaughtered in larger numbers than now\n\nThis is clearly the case for turkeys and poulty (misc), but also other other animals such as cattle and pigs\n\nSome numbers are relatively low (e.g., for horses)\nThe data sometimes fluctuates quite a bit from month to month, so annual view might show clearer patterns\n\nGiven these observations, let’s create a subset focusing only on six categories of animals that are still being slaughtered in large numbers and let’s also plot the annual data.\n\n\nCode\ndata_annual_subset <- data_annual %>%\n  filter(animal %in% c(\"broilers\", \"goats\", \"sheep\", \"cattle\", \"pigs\", \n    \"chicken (mostly layers)\")\n  ) %>%\n  filter(final == \"yes\")\n\nggplot(data_annual_subset, aes(x = year, y = count)) +\n  geom_point(size = 1) +\n  geom_line(alpha = .25) +\n  facet_wrap(~ animal, scales = \"free\")\n\n\n\n\n\nNumber of slaughtered animals per year and animal\n\n\n\n\nOkay, parsing this graph I see that a lot of chicken are slaughtered every year, particularly broiler chicken. The numbers are so high that ggplot has switched to the scientific notation to represent the numbers. Interestingly, though, the number of slaughtered broiler chickens has decreased somewhat in the last two years. I don’t know why that is. I also see that some animals are slaughtered more and more over the years (e.g., cattle, non-broiler chicken, goats, and pigs), although I’m also surprised to see that for some animals we’ve had worse years, particularly for cattle and pigs. For those animals we see a huge drop around the year 2000. The reason for that drop was the outbreak of foot-and-mouth disease and subsequent regulation. I thought things were getting worse and worse, but apparently it was already worse a while ago.\nLet’s hone in on some exact numbers. Below we create a table to show the number of slaughtered animals in 2021, per animal.\n\n\nCode\ndata_annual %>%\n  filter(year == 2021) %>%\n  arrange(desc(count)) %>%\n  select(animal, count)\n\n\n\n\n  \n\n\n\nOof. That’s over 500 million chicken! For reference, the Netherlands had a population of 17.17 million in 2021.\nHow many animals were slaughtered in total, in 2021?\n\n\nCode\ncount_total_2021 <- data_annual %>%\n  filter(year == 2021) %>%\n  summarize(count_total = sum(count)) %>%\n  pull(count_total)\n\n\nApparently a total of 541039500, or 17.16 animals per second. That means that about…\n\n0\n\n…have died since you started reading this blog post. That’s a bit of a bummer to end on, but then, this post was never going to have a happy ending.\nThis post was last updated on 2022-09-22."
  },
  {
    "objectID": "content/posts/3-useful-power-analysis-papers/useful-power-analysis-papers.html",
    "href": "content/posts/3-useful-power-analysis-papers/useful-power-analysis-papers.html",
    "title": "Useful power analysis papers",
    "section": "",
    "text": "Maxwell, S. E. (2004). The persistence of underpowered studies in psychological research: Causes, consequences, and remedies. Psychological Methods, 9(2), 147–163. https://doi.org/10.1037/1082-989X.9.2.147\n\nThis paper is quite amazing. It covers almost everything you need to be aware of when it comes to the state of our field, including how badly powered most studies are, how this affects the interpretability of inconsistencies in the literature, the need for multisite projects, and so on!\nMost importantly, it is about the problem of multiple statistical tests. Many power analyses that I see in the literature only power for one analysis, even though a paper usually contains many more. If you want a shot at all of those analyses being able to show something, you need to power for it all.\n\nBlake, K. R., & Gangestad, S. (2020). On attenuated interactions, measurement error, and statistical power: Guidelines for social and personality psychologists. Personality and Social Psychology Bulletin. https://doi.org/10.1177/0146167220913363\n\nThis is a great paper on a common pitfall in power analyses for attenuated interaction tests. Attenuated interactions are interactions where you expect a predictor to have an effect in one condition, but not another. They show that in a 2 x 2 design, you should use a fourfold of the sample size that is needed to show the interaction effect. Of course, this only applies when you predict a perfect attenuated interaction (a complete absence of the effect in the other condition, rather than a diminished one) and that you do not have any measurement error.\nThis post was last updated on 2022-04-29."
  },
  {
    "objectID": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html",
    "href": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html",
    "title": "Simulation-based power curves",
    "section": "",
    "text": "In a previous post I covered how to perform simulation-based power analyses. A limitation in that post is that usually the question is not what our power is, but rather which sample size gives us the desired power. With the code from my previous post you can get to the right sample size by changing the sample size parameters and then checking whether this gives you enough power. It’s fine, but it’s a little bit of a hassle.\nA more substantive, and related, limitation is that statistical power isn’t about a single threshold number that you’re supposed to reach. Power is a curve, after all. The difference between having obtained 79% power or 80% power is only 1% and does not strongly affect the interpretation of your obtained power. The code from my previous post doesn’t really illustrate this point, so let’s do that here."
  },
  {
    "objectID": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#setup",
    "href": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#setup",
    "title": "Simulation-based power curves",
    "section": "Setup",
    "text": "Setup\nIf you want to follow along, run the following setup code:\n\n\nCode\n# Load packages\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(effectsize)\nlibrary(knitr)\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n# Set options\noptions(\n  knitr.kable.NA = \"\",\n  digits = 2\n)"
  },
  {
    "objectID": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-scenario",
    "href": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-scenario",
    "title": "Simulation-based power curves",
    "section": "The scenario",
    "text": "The scenario\nLet’s look at one of the scenarios from the previous post: the two t-test scenario. We have one treatment condition and two control conditions. Our goal is to have enough power to find a significant difference between the treatment condition and both control conditions. Let’s begin with a single simulation, just to see what the data would look like.\n\n\nCode\n# Parameters\nM_control1 <- 5\nM_control2 <- M_control1\nM_treatment <- 5.5\nSD_control1 <- 1\nSD_control2 <- SD_control1\nSD_treatment <- 1.5\nN <- 50\n\n# Simulate once\ncontrol1 <- mvrnorm(N, mu = M_control1, Sigma = SD_control1^2, \n  empirical = TRUE)\ncontrol2 <- mvrnorm(N, mu = M_control2, Sigma = SD_control2^2, \n  empirical = TRUE)\ntreatment <- mvrnorm(N, mu = M_treatment, Sigma = SD_treatment^2, \n  empirical = TRUE)\n\n# Prepare data\ncolnames(control1) <- \"DV\"\ncolnames(control2) <- \"DV\"\ncolnames(treatment) <- \"DV\"\n\ncontrol1 <- control1 %>%\n  as_tibble() %>%\n  mutate(condition = \"control 1\")\n\ncontrol2 <- control2 %>%\n  as_tibble() %>%\n  mutate(condition = \"control 2\")\n\ntreatment <- treatment %>%\n  as_tibble() %>%\n  mutate(condition = \"treatment\")\n\ndata <- bind_rows(control1, control2, treatment)\n\n\nI’ve changed the code a little bit compared to my previous post. We still assume the same parameters in both control conditions, but now I use one of the control condition variables to set the value of the variable from the other control condition. This means you only need to set the value once and this reduces mistakes due to typos. In addition, we assume the same sample size in each condition and I also changed the parameter values.\nWe inspect the data by visualizing it and by calculating a standardized effect size that quantifies the difference between the treatment condition and each of the two control conditions.\n\n\nCode\n# Calculate standardized effect size\neffect_size <- cohens_d(DV ~ condition, pooled_sd = FALSE,\n  data = filter(data, condition != \"control 1\"))\n\n# Visualize the data\nggplot(data, aes(x = condition, y = DV)) + \n  geom_jitter(width = .2, alpha = .25) + \n  stat_summary(fun.data = \"mean_cl_boot\", geom = \"pointrange\") +\n  labs(x = \"Condition\")\n\n\n\n\n\nFigure 1: Three groups visualization\n\n\n\n\nThe effect size is a Cohen’s d -0.39 and although the Cohen’s d is negative (due to the ordering the levels in the condition column), the values in the treatment condition are higher than in the control conditions.\nAs a reminder, I think we should analyze this data with two Welch’s two sample t-tests: one for the difference between the treatment condition and control group 1 and also one between the treatment condition and control group 2. Below is some code to run these tests with the current data frame.\n\n\nCode\nt.test(DV ~ condition, data = filter(data, condition != \"control 1\"))\nt.test(DV ~ condition, data = filter(data, condition != \"control 2\"))"
  },
  {
    "objectID": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-power-analysis",
    "href": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-power-analysis",
    "title": "Simulation-based power curves",
    "section": "The power analysis",
    "text": "The power analysis\nNext we want to calculate the power. However, we don’t just want the power for a single sample size. We want to calculate the power for a range of sample sizes. So we begin by defining this range. In the example below, we create a variable called Ns that contains a sequence of values ranging from 50 to 250, in steps of 50. We also define S, which is the number of iterations in the power analysis. The higher this number, the more accurate the power analysis. Note that I’ve capitalized it this time. The final simulation parameter is i. I’ve defined this variable to keep track of how often we loop. This will be useful for figuring out where to store the p-values.\nWe will store the p-values in a data frame this time. Using the crossing() function, we can create a data frame that contains all possible combinations of the sample sizes and the number of simulation iterations. We can also already add some empty columns to store the p-values in.\nAfter that, we begin the loop. We want to loop over each sample size and we want to loop S times for each sample size, running the analyses in every single loop. This means we have a nested for loop. This is not particularly difficult, as we can simply put a for loop within a for loop. The only trick bit is to make sure that you store the p-values correctly. That’s where the i variable comes in. We initially gave it a value of 1, so the first p-values will be stored in the first row. At the end of each loop we increment it by 1, making sure that the next p-values will be stored in the next row of our p-values data frame.\n\n\nCode\n# Set simulation parameters\nNs <- seq(from = 50, to = 250, by = 50)\nS <- 1000\ni <- 1 \n\n# Create a data frame to store the p-values in\np_values <- crossing(\n  n = Ns,\n  s = 1:S,\n  p_value1 = NA,\n  p_value2 = NA\n)\n\n# Loop\nfor (n in Ns) {\n  for (s in 1:S) {\n    # Simulate\n    control1 <- mvrnorm(n, mu = M_control1, Sigma = SD_control1^2)\n    control2 <- mvrnorm(n, mu = M_control2, Sigma = SD_control2^2)\n    treatment <- mvrnorm(n, mu = M_treatment, Sigma = SD_treatment^2)\n    \n    # Run tests\n    test1 <- t.test(control1[, 1], treatment[, 1])\n    test2 <- t.test(control2[, 1], treatment[, 1])\n    \n    # Extract p-values\n    p_values$p_value1[i] <- test1$p.value\n    p_values$p_value2[i] <- test2$p.value\n  \n    # Increment i\n    i <- i + 1\n  }\n}\n\n\nThe result is a data frame with two columns containing p-values. To calculate the power, we sum the number of times we find a significant effect in both columns and divide it by the number of iterations per sample size (S). We need to do this for each sample size. The following code does the trick:\n\n\nCode\npower <- p_values %>%\n  mutate(success = if_else(p_value1 < .05 & p_value2 < .05, 1, 0)) %>%\n  group_by(n) %>%\n  summarize(power = sum(success) / S)"
  },
  {
    "objectID": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-power-curve",
    "href": "content/posts/10-simulation-based-power-curves/simulation-based-power-curves.html#the-power-curve",
    "title": "Simulation-based power curves",
    "section": "The power curve",
    "text": "The power curve\nWith the power for each sample size calculated, we can visualize the power curve.\n\n\nCode\nggplot(power, aes(x = n, y = power)) +\n  geom_line(linetype = 2) +\n  geom_point() +\n  geom_hline(yintercept = .80, linetype = 3) +\n  geom_hline(yintercept = .95, linetype = 3) +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +\n  labs(x = \"Sample size (n) per condition\", y = \"Power\")\n\n\n\n\n\nFigure 2: The power curve\n\n\n\n\nThat’s it. We can see at which point we have enough power (e.g., 80% or 95%). Do note that we calculated the sample size per condition. In the current scenario, this means you need to multiply the sample size by three in order to obtain your total sample size.\nIf you want a smoother graph, you can adjust the range of sample sizes that we stored in the Ns variable. You can, for example, lower the by argument in the seq() function to get a more fine-grained curve.\nThis post was last updated on 2022-04-29."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "",
    "text": "Not too long ago the House of Representatives of The Netherlands released a public portal to a lot of their data. The portal contains data on law proposals, motions, rapports, etc. I’ve been interested in this kind of data for a while now because I want to know more about the voting behavior of political parties. Specifically, I want to know which parties consistently vote in favor of improving animal rights. It’s relatively easy for a political party to say that they care about animal rights, but that doesn’t mean they consistently vote in favor of motions that improve animal rights. So let’s figure out how the open data portal works and which party to vote for.\nRun the following setup code if you want to follow along."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-the-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-the-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Getting the data",
    "text": "Getting the data\nWe will use the OData API to obtain the data. Using this API is pretty easy in theory; it’s nothing more than constructing a URL and then retrieving the data using that URL. The only tricky bit is how to set it up. In order to know how to do that, we need to understand the API. The OData API links to an information model that shows what kind of data we can request. We can request different entities, such as a Zaak (case), Document, Activiteit (activity), and so on. Going through the documentation I figured out we want to request cases because they have a Besluit (decision) entity, which contain a Stemming (vote) entity. Now that we sort of know what we want, we need to figure out how to actually get it.\nThe documentation of the API is pretty good. They explain how to set up the URL, called a query, and even provide several examples.\nEach query starts with the base URL: https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/. We need to append additional functions to this URL to hone in on the exact data we want.\nThe first thing we’ll specify is that we want a Zaak (case), so we will append Zaak to the end of the base URL.\nNext, we will apply some filter functions. In the documentation they recommend that we always filter on entities that have not been removed. They keep removed entities in the database so they can track changes. In one of the examples we can see how this is done. We have to append the following to the URL: ?$filter=Verwijderd eq false. The (first) filter needs to start with a question mark and a dollar sign, followed by the function name (filter), an equal sign, and a condition. The condition in this case is Verwijderd eq false, in other words: Removed equals false.\nAdditional filters can be added using logical operators such as and, or, or not. We want to request only cases that are motions, so we’ll add and Soort eq 'Motie'. Notice that we use and because we want both conditions to be true. The filter itself means that we want the Soort (type) to be equal to ‘Motie’ (motion). If we were to stop here, we would get a bunch of different motions, many of which have nothing to do with animal welfare. So let’s add another filter: and contains(Titel, 'Dierenwelzijn'). This means we select only the motions whose title contains the word ‘Dierenwelzijn’ (animal welfare). We could run this, but then we will get a total of 250 cases. It turns out that this is the maximum number of entities you can retrieve. That’s not ideal because preferably we get all of the animal welfare-related motions and if we get 250 back it’s not clear whether we got all of them. So let’s add another filter: and year(GestartOp) eq 2021. This means we only want cases when they’ve started in 2021. This probably results in fewer than 250 relevant motions, meaning we obtained them all (of that year).\nThe final function we need to add is an expand function. Right now we’re only requesting the data of motions, but not the data of the decision that was made in the motion, or the voting data. To also include that in the request we need to use the expand function. It’s a bit tricky because we need to run the expand function twice, once to expand on the decision and once on the voting. The part we need to append to the URL is: &$expand=Besluit($expand=Stemming).\nNow our URL is pretty much done. We have to paste all the parts together and request the data. We also need to replace all spaces with %20 so that it becomes a valid URL. You don’t need to do this if you just want to paste the URL in the browser, but if you want to use R code like in the code below, we do need to do this.\nThe data will be returned in a JSON format by the API. In R there’s the jsonlite package to work with JSON data, so we’ll use that package. The following code sets up the URL and retrieves the data.\n\n\nCode\n# Set url components\nbase_url <- \"https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/\"\nentity <- \"Zaak\"\nfilter1 <- \"?$filter=Verwijderd eq false\"\nfilter2 <- \" and Soort eq 'Motie'\"\nfilter3 <- \" and contains(Titel, 'Dierenwelzijn')\"\nfilter4 <- \" and year(GestartOp) eq 2021\"\nexpand <- \"&$expand=Besluit($expand=Stemming)\"\n\n# Construct url\nurl <- paste0(base_url, entity, filter1, filter2, filter3, filter4, expand)\n\n# Escape all spaces by replacing them with %20\nurl <- str_replace_all(url, \" \", \"%20\")\n\n# Get data\ndata <- read_json(url)\n\n\nYou can inspect the retrieved data here."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-the-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-the-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nThe data is structured as a list with various attributes, including additional lists. I personally don’t like working with lists at all in R so I want to convert it to a data frame as soon as possible. My favorite way of converting lists to a data frame is by using map_df(). It’s a function that accepts a list as its first argument and a function as its second argument. The function will be applied to each element in the list and the results of that will automatically be merged into a data frame. So let’s create that function.\nIn the code below we create a function that accepts an element of the value attribute in data, which is a list of cases we requested. The function then creates a data frame with only some of the case attributes: the number, title, subject, and start date. You can figure out which attributes are available by checking the documentation or going through the data we just obtained. After creating this function we run map_df().\n\n\n\n\n\nCode\n# Create a custom function to extract data from each motion\nclean_zaak <- function(zaak) {\n  df <- tibble(\n    number = zaak$Nummer,\n    start_date = as_date(zaak$GestartOp),\n    title = zaak$Titel,\n    subject = zaak$Onderwerp\n  )\n}\n\n# Run the clean_zaak function on each case\ndf <- map_df(data$value, clean_zaak)\n\n\nThe result is the following data frame:\n\n\nCode\ndf\n\n\n\n\n  \n\n\n\nWe can see that all the dates are from 2021 and that the titles contain the word ‘Dierenwelzijn’, just like we filtered on. The subject column is more interesting. It shows us what the case was about (if you don’t see the column, click on the arrow next to the title). After inspecting some of the subjects it becomes obvious that not all cases are about improving animal welfare. One, for example, is about using mobile kill units to kill animals that can’t be transported to a slaughterhouse. Ideally, we should go over all the cases and judge whether the case is about something that improves animal welfare or not.\nAlternatively, we can rely on the heuristic (for now) that in general all the cases on animal welfare are about things that improve animal welfare. Since we’re relying on a heuristic, it would help if we get more data so we can have the exceptions to this heuristic be overruled by many more data points. So let’s retrieve much more data."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-even-more-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-even-more-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Getting even more data",
    "text": "Getting even more data\nBelow I loop over several years and retrieve the data for that year. After retrieving the data, it is saved to a file using the write_json() function. It has an auto_unbox argument so that attributes that only consist of 1 attribute aren’t stored as lists but directly as the type of attribute itself (e.g., a number or string). There’s also the pretty argument which makes sure the file is at least somewhat readable, rather than one single very long line of data.\n\n\nCode\n# Set years we want the data of\nyears <- 2008:2021\n\n# Set url components\nbase_url <- \"https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/\"\nentity <- \"Zaak\"\nfilter1 <- \"?$filter=Verwijderd eq false\"\nfilter2 <- \" and Soort eq 'Motie'\"\nfilter3 <- \" and contains(Titel, 'Dierenwelzijn')\"\nfilter4 <- \" and year(GestartOp) eq \"\nexpand <- \"&$expand=Besluit($expand=Stemming)\"\n\n# Loop over the years\nfor (year in years) {\n  # Construct the url\n  url <- paste0(base_url, entity, filter1, filter2, filter3, filter4,\n    year, expand)\n  \n  # Escape all spaces\n  url <- str_replace_all(url, \" \", \"%20\")\n  \n  # Get data\n  data <- read_json(url)\n  \n  # Write the data to a file\n  write_json(\n    data, \n    path = paste0(\"motions-\", year, \".json\"), \n    auto_unbox = TRUE, \n    pretty = TRUE\n  )\n}"
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-even-more-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-even-more-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Cleaning even more data",
    "text": "Cleaning even more data\nNow that we have a bunch of data files, we need to read them in. A technique to read in multiple files of the same type is to use map_df() again. We can give it a list of files, created with list.files(), and apply a function to each file path. Not only can we use that to simply read in the data, we can immediately parse the data and convert it to a data frame. In the code below I go all inception on this problem and define multiple functions that each convert a list to a data frame. There’s a function for reading in a file, converting a case to a data frame, which calls a function to convert a decision to a data frame, which calls a function to convert a vote to a data frame. It may seem a bit complicated, but once you realize you can call functions within functions, it can actually make some tricky problems easy to solve; at least with relatively little code.\n\n\nCode\nread_file <- function(file) {\n  data <- read_json(file)\n  \n  df <- map_df(data$value, clean_zaak)\n  \n  return(df)\n}\n\nclean_zaak <- function(zaak) {\n  df <- tibble(\n    motion_number = zaak$Nummer,\n    start_date = as_date(zaak$GestartOp),\n  )\n  \n  df <- tibble(\n    df,\n    map_df(zaak$Besluit, clean_besluit)\n  )\n  \n  return(df)\n}\n\nclean_besluit <- function(besluit) {\n  df <- tibble(\n    decision_outcome = besluit$BesluitTekst\n  )\n  \n  if (length(besluit$Stemming) != 0) {\n    df <- tibble(\n      df,\n      map_df(besluit$Stemming, clean_stemming)\n    )\n  }\n  \n  return(df)\n}\n\nclean_stemming <- function(stemming) {\n  df <- tibble(\n    party = stemming$ActorFractie,\n    vote = stemming$Soort,\n    mistake = stemming$Vergissing\n  )\n  \n  return(df)\n}\n\n# Create a list of the files we want to read\nfiles <- list.files(pattern = \"motions-[0-9]+.json\")\n\n# Apply the read_file() function to each file, which calls each other function\ndf <- map_df(files, read_file)\n\n\nLet’s clean up the resulting data frame some more because we kept more information than we actually need. For example, there are different types of decision outcomes, but we only care about the ones where a voting took place. Let’s also translate the votes to English and exclude votes of parties that did not participate (they are still included) and mistaken votes (apparently sometimes they make mistakes when voting).\n\n\nCode\ndf <- df %>%\n  filter(str_detect(decision_outcome, \"Verworpen|Aangenomen\")) %>%\n  filter(vote != \"Niet deelgenomen\") %>%\n  filter(!mistake) %>%\n  mutate(\n    decision_outcome = str_extract(decision_outcome, \"Verworpen|Aangenomen\"),\n    decision_outcome = recode(\n      decision_outcome, \n      \"Verworpen\" = \"rejected\", \n      \"Aangenomen\" = \"accepted\"\n    ),\n    start_date = year(start_date),\n    vote = recode(vote, \"Tegen\" = \"nay\", \"Voor\" = \"aye\"),\n    vote = factor(vote),\n    mistake = NULL\n  )\n\n\nAnnoyingly, I discovered that the decision outcome data changed over the years in a trivial way. Starting in the year 2013, they added a period to the description of the decision outcome (e.g., ‘Verworpen.’). A silly change that actually resulted in me missing data from the years before 2013 while initially writing this post.\nWe now have the following data frame:\n\n\nCode\ndf"
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#analyzing-voting-behavior",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#analyzing-voting-behavior",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Analyzing voting behavior",
    "text": "Analyzing voting behavior\nNow we are ready to inspect the voting behavior of the political parties. For each party we calculate how often they voted ‘aye’ or ‘nay’ and calculate it as a percentage of the times they’ve voted. We then plot the percentage of times they voted ‘aye’.\n\n\nCode\nvoting <- df %>%\n  count(party, vote, .drop = FALSE) %>%\n  pivot_wider(names_from = vote, values_from = n) %>%\n  mutate(\n    votes = aye + nay,\n    aye_pct = aye / votes\n  )\n\nggplot(voting, aes(x = aye_pct, y = reorder(party, aye_pct))) +\n  geom_col(aes(alpha = votes)) +\n  labs(\n    x = \"Times voted 'aye' on an animal welfare motion (in %)\", \n    y = \"\",\n    alpha = \"Times voted\") +\n  scale_x_continuous(limits = c(0, 1), labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\nPercentage of times political parties voted ‘aye’ on motions related to animal welfare\n\n\n\n\nIt seems like the heuristic might be somewhat justified. I don’t know much about Van Kooten-Arissen or Group Krol/vKA, but PvdD stands for Partij voor de Dieren (party for the animals). It makes sense that they are among the top in voting in favor of improving animal welfare. At the same time there’s some evidence that the heuristic is indeed only a heuristic. The PvdD apparently voted ‘aye’ in 84.13% of the motions. That could mean there are some motions where it is in the interest of the animals to vote ‘nay’. I also find it a bit worrying that the PVV, a notorious right-wing party in the Netherlands, is so high on the list of voting ‘aye’ on animal welfare matters. In some manual inspections of the motions I saw they tend to disagree with some obvious animal welfare improvements, although perhaps my sample just happened to find these disagreements and had I inspected more motions I would have found the same results.\nAnother way we can look at this data is by using the PvdD as a benchmark for what the other political parties should vote for. We can assume that this party has the best interest for animals in mind, as that is their most important platform. Of course this would mean we can’t use the result to figure out whether we should vote for PvdD, but it can be useful to figure out which alternative party to vote for.\n\n\nCode\nvote_matches_PvdD <- df %>%\n  filter(party == \"PvdD\") %>%\n  group_by(motion_number) %>%\n  summarize(vote_PvdD = first(vote)) %>%\n  right_join(df, by = \"motion_number\") %>%\n  filter(party != \"PvdD\") %>%\n  mutate(\n    match = if_else(vote == vote_PvdD, \"match\", \"no_match\"),\n    match = factor(match)\n  ) %>%\n  count(party, match, .drop = FALSE) %>%\n  pivot_wider(names_from = match, values_from = n) %>%\n  mutate(\n    votes = match + no_match,\n    match_pct = match / votes\n  )\n\nggplot(vote_matches_PvdD, aes(x = match_pct, y = reorder(party, match_pct))) +\n  geom_col(aes(alpha = votes)) +\n  labs(\n    x = \"Times voted the same as the Party for the Animals (in %)\", \n    y = \"\",\n    alpha = \"Times voted\") +\n  scale_x_continuous(limits = c(0, 1), labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\nPercentage of times a political party voted the same as the Party for the Animals\n\n\n\n\nIt looks like the two graphs are fairly consistent. Van Kooten-Arissen and Groep Krol/vKA are still at the top. The same goes for the bigger parties. In fact, the ranking of the largest parties (who have voted the most times) is the same in the two graphs. That can give us some extra confidence that the heuristic from the first graph works.\nLet’s go back to our heuristic and create another graph that shows the voting behavior across the years. After all, it could very well be that a political party changed their values in the last decade. Let’s only use the 10 biggest parties for this graph because they’re more likely to have enough data for each year.\n\n\nCode\nvoting_years <- df %>%\n  group_by(party) %>%\n  mutate(votes = n()) %>%\n  filter(votes > 300) %>%\n  count(start_date, party, vote, .drop = FALSE) %>%\n  pivot_wider(names_from = vote, values_from = n) %>%\n  mutate(\n    votes = aye + nay,\n    aye_pct = aye / votes\n  ) \n\nggplot(\n    voting_years, \n    aes(x = start_date, y = aye_pct)\n  ) +\n  geom_line(alpha = .25) +\n  geom_point() +\n  facet_wrap(~ party, ncol = 2) +\n  labs(\n    x = \"\", \n    y = \"\"\n  ) +\n  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\nTimes each party voted ‘aye’ on animal welfare motions throughout the years\n\n\n\n\nLooks like most parties are fairly consistent. There’s some variation from year to year, but for most parties you can tell whether they are pro-animal or not. There are some exceptions to this, like the PvdA and D66, which seem to vary quite a bit. We can actually calculate this variation so we don’t have to guess it from the graph.\n\n\nCode\nvoting_years %>%\n  group_by(party) %>%\n  summarize(SD = sd(aye_pct)) %>%\n  arrange(desc(SD))\n\n\n\n\n  \n\n\n\nYup, looks like PvdA and D66 are the least consistent."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#limitations",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#limitations",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Limitations",
    "text": "Limitations\nIt bears repeating that this analysis of the data isn’t perfect. The best way to analyze this data would be to take a look at each individual motion and determine, based on your own values, whether an ‘aye’ vote or a ‘nay’ vote is in the best interest of animals. I hope to do this myself in the future.\nAnother limitation of this analysis is that we have only searched for motions that mention animal welfare in the title. There are many more motions that address animal welfare issues that we’ve missed with our method. The results could therefore be made more reliable by adding more data. In this post we have looked at 5453 votes in 391 motions. I suppose that’s decent, but we could do better."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#conclusion",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#conclusion",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we used publicly available data from the House of Representatives of the Netherlands to inspect the voting behavior of political parties on matters related to animal welfare. This is made possible by the amazing new data portal that makes this data freely and relatively easily available to everyone. Kudos to them for that.\nIf you’re interested in figuring out which party to vote for because you want to support animal welfare, then the largest parties to pay attention to are the PvdD, GroenLinks, and SP. They are large enough to have voted on issues a decent number of times, giving us some confidence that they vote in favor of improving animal welfare. More can be done to improve this interpretation of the data, but looking at actual voting behavior seems like a valuable piece of information when considering which party to vote for."
  },
  {
    "objectID": "content/posts/8-the-right-order-of-method-sections/the-right-order-of-method-sections.html",
    "href": "content/posts/8-the-right-order-of-method-sections/the-right-order-of-method-sections.html",
    "title": "The right order of method sections",
    "section": "",
    "text": "Design\nProcedure\nMaterials\nData Analysis\nParticipants\n\nTwo things are notable here. One, there’s a Data Analysis section. Two, the Participants section is all the way at the end. Here I anticipate that your reaction will be that this is crazy, because that’s not how we do things. But that’s not a good enough reason of course. We should be thinking about whether the order makes sense in terms of whether the content of each section logically follows from each other. For some sections you first need to know information from the other sections in order for your decisions to make sense. Putting the Participants all the way at the beginning doesn’t make sense, and the reason for that is the power analysis.\nNow that power analyses are getting more popular, psychologists have to try and make them fit in their Method section. But rather than thinking about what actually goes into a power analysis, and how to present that information to the reader, they generally stick to the format they’re used to. Or perhaps it’s because they misunderstand how a power analysis works, thinking that you only have 1 power analysis per study, so you should present it together with the Design of the study. Since the Design and Participants are sometimes combined, I can see how this might be the case. That still doesn’t make sense, though, and to understand that, we need to understand power analyses.\nWhat goes into a power analysis? A power analysis consists of setting a few parameters, such as the effect size, alpha, and beta. The alpha and beta parameters are pretty constant across different power analyses, but the effect size isn’t. The effect size depends on the exact analysis you want to power for. A t-test is usually done with a Cohen’s d in mind, while a correlation test is done with a correlation in mind. With more sophisticated analyses, such as repeated measures analyses, you need to set additional parameters (e.g., the correlation between repeated measures). This means that your power analysis is dependent on the exact analysis you will do. Actually, a power analysis is always about a specific analysis, so by that logic alone, you should first present which analysis you will do. Not only that, but you also need to power for all analyses you do, not just 1. In other words, a power analysis is something that is tied to a statistical test, and not to the design of a study (which would mean you only need 1 power analysis per study). The result is obvious: you first need to discuss the analyses you want to run before you can talk about power. This means you need a Data Analysis section in your Method section. Here you can elaborate on the analyses you will run, which analyses are the primary ones that you want to power for, and perhaps elaborate on some secondary or exploratory analyses that you won’t power for. You can also use this section to then present the power analysis. After that you get your needed sample size and you can start to explain how you obtained that sample size (i.e., the Participants section).\nWhat do you need to know in order to understand the Data Analysis section? That would be the Design and Materials. You need to know about the design to know whether it is, for example, a between-subjects design or a within-subjects design. You also need to know what the independent variables and dependent variables are. More specifically, you want to know how they are measured. How many levels are there in the independent variables? Is the outcome measure categorical or continuous? These are some of the properties of the measures that determine the appropriate analysis technique. This, in turn, means the Design section and the Materials section need to come before the Data Analysis section.\nPutting all of this together, I think it makes the most sense to begin with the Design, followed by the Procedure and Materials (possibly combined). This should be followed by a Data Analysis section that includes the analyses and associated power analysis (for all primary analyses, at least). Once these aspects are known, it makes sense to end, rather than start, with the Participants section. So the right order of Method sections is:\n\nDesign\nProcedure\nMaterials\nData Analysis\nParticipants"
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "",
    "text": "Many thanks to the people who replied to my tweet about why you should divide by \\(n - 1\\). Below I try to show the intuition behind why this is necessary. If you want to follow along in R, you can copy the code from each code section; beginning with some setup code."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-formula",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-formula",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The formula",
    "text": "The formula\nThe formula for calculating the variance is:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n}\\]\nThe variance is a measure of the dispersion around the mean, and in that sense this formula makes sense. We calculate all the deviations from the mean (\\(x_i - \\overline{x}\\)), square them (for reasons I might go into in a different post) and sum them. We then divide this sum by the number of observations as a scaling factor. If we ignore this number, we could get a very high variance simply by observing a lot of data. So, to fix that problem, we divide by the total number of observations.\nHowever, this is the formula for the population variance. The formula for calculating the variance of a sample is:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n - 1}\\]\nWhy do we divide by n - 1?\nIf you Google this question, you will get a variety of answers. You might find a mathematical proof of why it needs to be \\(n - 1\\) or something about degrees of freedom. These kinds of answers don’t work for me. I trust them to be correct, but it doesn’t produce any insight. It does not actually help me understand 1) the problem and 2) why the solution is the solution that it is. So, below I am going to try to figure it out in a way that actually makes conceptual and intuitive sense (to me)."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-problem",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-problem",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The problem",
    "text": "The problem\nThe problem with using the population variance formula to calculate the variance of a sample is that it is biased. It is biased in that it produces an underestimation of the true variance. Let’s demonstrate that with some simulated data.\nWe simulate a population of 1000 data points from a uniform distribution with a range from 1 to 10. Below I show the histogram that represents our population.\n\n\nCode\n# Set the seed for reproducibility\nset.seed(1212)\n\n# Create a population consisting of values ranging from 1 to 10\npopulation <- sample(1:10, size = 1000, replace = TRUE)\n\n# Calculate the population variance\nsigma <- my_var(population, population = TRUE)\n\n# Visualize the population\nggplot(tibble(population = population), mapping = aes(x = population)) +\n  geom_bar(alpha = .85) +\n  labs(x = \"x\", y = \"n\") +\n  scale_x_continuous(breaks = 1:10)\n\n\n\n\n\nFigure 1: The population in our example\n\n\n\n\nThe variance is 8.76. Note that this is our population variance (often denoted as \\(\\sigma^2\\)). We want to estimate this value using samples drawn from our population, so let’s do that.\n\n\nCode\n# Draw a single sample from the population\nsample <- sample(population, size = 5)\n\n\nTo start, we can draw a single sample of size 5. Say we do that and get the following values: 7, 6, 3, 5, 5. We can then calculate the variance in two ways, using division by \\(n\\) and division by \\(n - 1\\). In the former case, this will result in 1.76 and in the latter case it results in 2.2.\nNow let’s do that many many times. Below I show the results of draws from our population. I simulated drawing samples of size 2 to 10, each 1000 different times. I then plotted for each sample size the average biased variance (dividing by \\(n\\); Figure 2 (a)) and the average unbiased variance (dividing by \\(n - 1\\); Figure 2 (b)).\n\n\nCode\n# Create an empty data frame with the simulation parameters\nsamples <- crossing(\n    n = 2:20,\n    i = 1:1000\n  )\n\n# Calculate the mean, sample variance, and population variance \n# for each combination of n and i\nsamples <- samples %>%\n  rowwise() %>%\n  mutate(\n    var_unbiased = my_var(sample(population, n), population = FALSE),\n    var_biased = my_var(sample(population, n), population = TRUE)\n  )\n\n# Plot the results in two separate plots\nggplot(samples, aes(x = n, y = var_biased)) +\n  geom_hline(yintercept = sigma, linetype = \"dashed\", alpha = .5) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\") +\n  coord_cartesian(ylim = c(0, sigma + 1)) +\n  scale_x_continuous(breaks = seq(from = 2, to = 20, by = 2))\n\nggplot(samples, aes(x = n, y = var_unbiased)) +\n  geom_hline(yintercept = sigma, linetype = \"dashed\", alpha = .5) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\") +\n  coord_cartesian(ylim = c(0, sigma + 1)) +\n  scale_x_continuous(breaks = seq(from = 2, to = 20, by = 2))\n\n\n\n\n\n\n\n\n(a) Variance with division by n\n\n\n\n\n\n\n\n(b) Variance with division by n - 1\n\n\n\n\nFigure 2: A demonstration that dividing by 1 causes a bias\n\n\n\nWe see that the biased measure of variance is indeed biased. The average variance is lower than the true variance (indicated by the dashed line), for each sample size. We also see that the unbiased variance is indeed unbiased. On average, the sample variance matches that of the population variance.\nThe results of using the biased measure of variance reveals several clues for understanding the solution to the bias. We see that the amount of bias is larger when the sample size of the samples is smaller. So the solution should be a function of sample size, such that the required correction will be smaller as the sample size increases. We also see that that the bias at \\(n = 2\\) is half that of the true variance, \\(\\frac23\\) at \\(n = 3\\), \\(\\frac34\\) at \\(n = 4\\), and so on. Interesting.\nBut before we go into the solution, we still need to figure out what exactly causes the bias.\nIdeally we would estimate the variance of the sample by subtracting each value from the population mean. However, since we don’t know what the population mean is, we use the next best thing—the sample mean. This is where the bias comes in. When you use the sample mean, you’re guaranteed that the mean lies somewhere within the range of your data points. In fact, the mean of a sample minimizes the sum of squared deviations from the mean. This means that the sum of deviations from the sample mean is almost always smaller than the sum of deviations from the population mean. The only exception to that is when the sample mean happens to be the population mean.\nLet’s illustrate this with a few graphs. Below are two graphs in which I show 10 data points that represent our population. I also highlight two data points from this population, which represents our sample. In the left graph I show the deviations from the sample mean and in the right graph the deviations from the population mean.\n\n\nCode\n# Create the population\nx <- c(1, 2, 4, 4, 4, 6, 8, 9, 10, 10)\n\n# Create a sample\nsample1 <- tibble(\n    index = 1:10,\n    value = x,\n    sample = c(0, 0, 0, 0, 0, 0, 1, 0, 0, 1),\n    mean = mean(c(8, 10)),\n    mu = mean(x)\n  ) %>%\n  mutate(\n    mean = ifelse(sample == 1, mean, NA),\n    mu = ifelse(sample == 1, mu, NA)\n  )\n\n# Plot the results\nggplot(sample1, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mu), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mean), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mean, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 5.8, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 9, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n\nggplot(sample1, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mean), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mu), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mu, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 5.8, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 9, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n\n\n\n\n\n\n\n\n(a) Deviations from the sample mean\n\n\n\n\n\n\n\n(b) Deviations from the population mean\n\n\n\n\nFigure 3: An illustration of how using a sample mean introduces bias\n\n\n\nWe see that in the left graph the sum of squared deviations is much smaller than in the right graph. The sum is \\((8 - 9)² + (10 - 9)² = 2\\) in the left graph and in the right graph it’s \\((8 - 5.8)² + (10 - 5.8)² = 22.48\\). The sum is smaller when using the sample mean compared to using the population mean.\nThis is true for any sample you draw from the population (again, except when the sample mean happens to be the same as the population mean). Let’s look at one more draw where the sample mean is closer to the population mean.\n\n\nCode\n# Create a second sample\nsample2 <- tibble(\n    index = 1:10,\n    value = x,\n    sample = c(0, 1, 0, 0, 0, 0, 0, 0, 1, 0),\n    mean = mean(c(2, 10)),\n    mu = mean(x)\n  ) %>%\n  mutate(\n    mean = ifelse(sample == 1, mean, NA),\n    mu = ifelse(sample == 1, mu, NA)\n  )\n\n# Plot the results\nggplot(sample2, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mu), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mean), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mean, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 4.92, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 6.7, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n  \nggplot(sample2, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mean), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mu), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mu, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 4.92, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 6.7, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n\n\n\n\n\n\n\n\n(a) Deviations from the sample mean\n\n\n\n\n\n\n\n(b) Deviations from the population mean\n\n\n\n\nFigure 4: Another illustration of how using a sample mean introduces bias\n\n\n\nHere the sum in the left graph is \\((2 - 6)² + (10 - 6)² = 32\\) and the sum in the right graph is \\((2 - 5.8)² + (10 - 5.8)² = 32.08\\). The difference is small now, but using the sample mean still results in a smaller sum compared to using the population mean.\nIn short, the source of the bias comes from using the sample mean instead of the population mean. The sample mean is always guaranteed to be in the middle of the observed data, thereby reducing the variance, and creating an underestimation."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-solution",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-solution",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The solution",
    "text": "The solution\nNow that we know that the bias is caused by using the sample mean, we can figure out how to solve the problem.\nLooking at the previous graphs, we see that if the sample mean is far from the population mean, the sample variance is smaller and the bias is large. If the sample mean is close to the population mean, the sample variance is larger and the bias is small. So, the more the sample mean moves around the population mean, the greater the bias.\nIn other words, besides the variance of the data points around the sample mean, there is also the variance of the sample mean around the population mean. We need both variances in order to accurately estimate the population variance.\nThe population variance is thus the sum of two variances:\n\\[\\sigma^2_{sample} + \\sigma^2_{\\vphantom{sample}mean} = \\sigma^2_{population}\\] Let’s confirm that this is true. For that we need to know how to calculate the variance of the sample mean around the population mean. This is relatively simple; it’s the variance of the population divided by n (\\(\\frac{\\sigma^2}n\\)). This makes sense because the greater the variance in the population, the more the mean can jump around, but the more data you sample, the closer you get to the population mean.\nNow that we can calculate both the variance of the sample and the variance of the sample mean, we can check whether adding them together results in the population variance.\nBelow I show a graph in which I again sampled from our population with varying sample sizes. For each sample, I calculated the sample variance (the biased one) and the variance of the mean of that sample (\\(\\frac{\\sigma^2}n\\))1. I did this 1000 times per sample size, took the average of each and put them on top of each other. I also added a dashed line to indicate the variance of the population, which is the benchmark we’re trying to reach.\n\n\nCode\n# Calculate the variance sources per sample size\nvariance_sources <- samples %>%\n  mutate(var_mean = var_unbiased / n) %>%\n  group_by(n) %>%\n  summarize(\n    var_biased = mean(var_biased),\n    var_unbiased = mean(var_unbiased),\n    var_mean = mean(var_mean)\n  ) %>%\n  pivot_longer(cols = c(var_biased, var_mean), names_to = \"variance_source\", \n    values_to = \"variance\") %>%\n  mutate(variance_source = recode(variance_source, \"var_biased\" = \n      \"sample\", \"var_mean\" = \"sample mean\"))\n\n# Plot the results\nggplot(variance_sources, aes(x = n, fill = variance_source, y = variance)) +\n  geom_col(alpha = .85) +\n  geom_hline(yintercept = sigma, linetype = \"dashed\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\", fill = \"Variance source:\") +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\n\n\n\nFigure 5: Sources of variance\n\n\n\n\nIndeed, we see that the variance of the sample and the variance of the mean of the sample together form the population variance."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-math",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-math",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The math",
    "text": "The math\nNow that we know that the variance of the population consists of the variance of the sample and the variance of the sample mean, we can figure out the correction factor we need to apply to make the biased variance measure unbiased.\nPreviously, we found an interesting pattern in the simulated samples, which is also visible in the previous figure. We saw that at sample size \\(n=2\\), the (biased) sample variance appears to be half that of the (unbiased) population variance. At sample size \\(n=3\\), it’s \\(\\frac23\\). At sample size \\(n=4\\), it’s \\(\\frac34\\), and so on.\nThis means that we can fix the biased variance measure by multiplying it with \\(\\frac{n}{(n-1)}\\). At \\(n = 2\\), we multiply the biased variance by \\(\\frac21 = 2\\). For sample size \\(n=3\\), we multiply by \\(\\frac32 = 1.5\\). At sample size \\(n=4\\), it’s \\(\\frac43 = 1 \\frac13\\).\nIn other words, to unbias the biased variance measure, we multiply it by a correction factor of \\(\\frac{n}{(n-1)}\\). But where does this correction factor come from?\nWell, because the sample variance misses the variance of the sample mean, we can expect that the variance of the sample is biased by an amount equal to the variance of the population minus the variance of the sample mean. In other words:\n\\[\\sigma^2 - \\frac{\\sigma^2}n\\]\nRewriting this 2, produces:\n\\[\\sigma^2\\cdot\\frac{n - 1}n\\] The variance of a sample will be biased by an amount equal to \\(\\frac{n - 1}n\\). To correct that bias we should multiply the sample variance by the inverse of this bias: \\(\\frac{n}{n-1}\\) 3. This is also called Bessel’s correction.\nSo, an unbiased measure of our sample variance is the biased sample variance times the correction factor:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n}\\cdot{\\frac n{n-1}}\\] Because the n in the denominator of the left term (the biased variance formula) cancels out the n in the numerator of the right term (the bias correction), the formula can be rewritten as:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n-1}\\]"
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#summary",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#summary",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "Summary",
    "text": "Summary\nWe calculate the variance of a sample by summing the squared deviations of each data point from the sample mean and dividing it by \\(n - 1\\). The \\(n - 1\\) actually comes from a correction factor \\(\\frac n{n-1}\\) that is needed to correct for a bias caused by taking the deviations from the sample mean rather than the population mean. Taking the deviations from the sample mean only constitutes the variance around the sample mean, but ignores the variation of the sample mean around the population mean, producing an underestimation equal to the size of the variance of the sample mean: \\(\\frac{\\sigma^2}{n}\\). The correction factor corrects for this underestimation, producing an unbiased estimate of the population variance.\nThis post was last updated on 2022-05-30."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html",
    "title": "Understanding regression (part 1)",
    "section": "",
    "text": "Statistical regression techniques are an important tool in data analysis. As a social scientist, I use it to test hypotheses by comparing differences between groups or testing relationships between variables. While it is easy to run regression analyses in a variety of software packages, like SPSS or R, it often remains a black box that is not well understood. I, in fact, do not believe I actually understand regression. Not fully understanding the mechanics of regression could be okay, though. After all, you also don’t need to know exactly how car engines work in order to drive a car. However, I think many users of regression have isolated themselves too much from the mechanics of regression. This may be the source of some errors, such as applying regression to data that is not suitable for the regression method. If you’re using regression to try and make inferences about the world, it’s probably a good idea to feel like you know what you’re doing.\nSo, there are some reasons to figure out regression. This post is Part 1 of a series of blog posts called ‘Understanding Regression’ in which I try to figure it out.\nFeel free to follow me along by copy-pasting the code from each step."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#setup",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#setup",
    "title": "Understanding regression (part 1)",
    "section": "Setup",
    "text": "Setup\nTo figure out regression, we need data. We could make up some data on the spot, but I’d rather use data that is a bit more meaningful (to me, anyway). Since I’m a big Pokémon fan, I’ll use a data set containing Pokémon statistics.\nIn case you’re following along, start by loading some packages and reading in the data. In the code section below I use the here package to read in the data, but I recommend that you simply specify the path to the file. After that, I subset the data to make the data a bit more manageable and define a custom mode function because R does not have one (and I need it later). Finally, I set some options such as the default ggplot() theme and printing options.\n\n\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(here)\nlibrary(knitr)\n\n# Read in Pokémon data\npokemon <- read_csv(here(\"data\", \"pokemon.csv\"))\n\n# Create a subset with only the first 25 Pokémon\npokemon25 <- filter(pokemon, pokedex <= 25)\n\n# Load a custom function to calculate the mode\nmode <- function(x) {\n  ux <- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n# Set some options\noptions(\n  knitr.kable.NA = \"-\",\n  digits = 2\n)\n\n\nLet’s take a look at several attributes of some Pokémon to see what they’re about:\n\n\nCode\npokemon25 %>%\n  filter(pokedex <= 10) %>%\n  select(name, type_primary, type_secondary, height, weight, \n    evolution) %>%\n  kable(\n    digits = 2, \n    col.names = c(\"Name\", \"Type (primary)\", \"Type (secondary)\", \"Height\", \n      \"Weight\", \"Evolution stage\")\n  )\n\n\n\n\nTable 1: The first 10 Pokémon\n\n\n\n\n\n\n\n\n\n\nName\nType (primary)\nType (secondary)\nHeight\nWeight\nEvolution stage\n\n\n\n\nBulbasaur\nGrass\nPoison\n0.7\n6.9\n0\n\n\nIvysaur\nGrass\nPoison\n1.0\n13.0\n1\n\n\nVenusaur\nGrass\nPoison\n2.0\n100.0\n2\n\n\nCharmander\nFire\n-\n0.6\n8.5\n0\n\n\nCharmeleon\nFire\n-\n1.1\n19.0\n1\n\n\nCharizard\nFire\nFlying\n1.7\n90.5\n2\n\n\nSquirtle\nWater\n-\n0.5\n9.0\n0\n\n\nWartortle\nWater\n-\n1.0\n22.5\n1\n\n\nBlastoise\nWater\n-\n1.6\n85.5\n2\n\n\nCaterpie\nBug\n-\n0.3\n2.9\n0\n\n\n\n\n\n\nPokémon have different types (e.g., grass, fire, water), a height, some weight, and they are of a particular evolutionary stage (0, 1, or 2). This last variable refers to a Pokémon’s ability to evolve and when they do, they tend to become bigger and more powerful.\nLet’s say that we are interested in understanding the weight of different Pokémon. Below I have plotted the weight of the first 25 Pokémon, from Bulbasaur to Pikachu.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .85) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nFigure 1: Weights of the first 25 Pokémon\n\n\n\n\nWe see that the lightest Pokémon is Pidgey, with a weight of 1.8 kg. The heaviest Pokémon is Venusaur, with a weight of 100 kg. The average weight is 26.14 kg."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-simplest-model",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-simplest-model",
    "title": "Understanding regression (part 1)",
    "section": "The simplest model",
    "text": "The simplest model\nIn order to understand the weights of different Pokémon, we need to come up with a statistical model. In a way, this can be considered a description problem. How can we best describe the different weights that we have observed? The simplest description is a single number. We can say that all Pokémon have a weight of say… 6 kg. In other words:\n\nweight = 6 kg\n\nOf course, this is just one among many possible models. Below I plot three different models, including our weight = 6 kg model.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .85) +\n  geom_abline(intercept = 6, slope = 0, linetype = 2) +\n  geom_abline(intercept = 40, slope = 0, linetype = 2) +\n  geom_abline(intercept = 75, slope = 0, linetype = 2) +\n  annotate(\"text\", x = 28, y = 6.5, label = \"weight = 6 kg\", size = 3.5) +\n  annotate(\"text\", x = 28, y = 40.5, label = \"weight = 40 kg\", size = 3.5) +\n  annotate(\"text\", x = 28, y = 75.5, label = \"weight = 75 kg\", size = 3.5) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  coord_cartesian(xlim = c(1, 25), clip = \"off\") +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.margin = unit(c(1, 6, 1, 1), \"lines\")\n  )\n\n\n\n\n\nFigure 2: Three different weight models\n\n\n\n\nWhile a model like weight = 6 kg is a valid model, it is not a very good model. In fact, it only perfectly describes Pikachu’s weight and inaccurately describes the weight of the remaining 24 Pokémon. The other models, such as weight = 40 kg might be even worse; they do not even describe a single Pokémon’s weight correctly, although they do get closer to some of the heavier Pokémon. How do we decide which model is the better model? In order to answer that question, we need to consider the model’s error."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-error-of-models",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-error-of-models",
    "title": "Understanding regression (part 1)",
    "section": "The error of models",
    "text": "The error of models\nThe error of a model is the degree to which the model inaccurately describes the data. There are several ways to calculate that error, depending on how you define error. We will cover three of them.\nThe first definition of error is simply the sum of times that the model inaccurately describes the data. For each observation we check whether the model correctly describes it or not. We then sum the number of misses and consider that the amount of error for that model. With our weight = 6 kg the answer is 24; out of the 25 Pokémon only Pikachu has a weight of 6, which means the model is correct once and wrong 24 times.\nWe can now compare different models to one another by calculating the error for a range of models. Below I plot the number of errors for 100 different models, starting with the model weight = 1 kg, up to weight = 10 kg, in steps of 0.1. Ideally we would test more models (up to the heaviest Pokémon we know of), but for the sake of visualizing the result, I decided to only plot a small subset of models.\n\n\nCode\nerrors_binary <- expand_grid(\n    model = seq(from = 1, to = 10, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %>%\n  mutate(error = if_else(abs(weight - model) == 0, 0, 1)) %>%\n  group_by(model) %>%\n  summarize(error_sum = sum(error))\n\nggplot(errors_binary, aes(x = model, y = error_sum)) +\n  geom_line() + \n  coord_cartesian(ylim = c(0, 25)) +\n  scale_x_continuous(breaks = 1:10) +\n  labs(x = \"Model (weight = x kg)\", y = \"Error (sum of errors)\")\n\n\n\n\n\nFigure 3: Error #1: The sum of (binary) errors\n\n\n\n\nWe see that almost all models perform poorly. The errors range from 23 to 25. Most models seem to have an error of 25, which means they do not accurately describe any of the 25 Pokémon. Some have an error of 24, meaning they describe the weight of 1 Pokémon correctly. There is 1 model with an error of 23: weight = 6.9 kg. Apparently there are 2 Pokémon with a weight of 6.9, which means that this model outperforms the others.\nDespite there being a single model that outperforms the others in this set of models, it’s still a pretty poor model. After all, it is wrong 23 out of 25 times. Perhaps there are some models that outperform this model, but it’s unlikely. That’s because we’re defining error here in a very crude way. The model needs to exactly match the weight of the Pokémon, or else it counts as an error. Saying a weight is 6 kg, while it is in fact 10 kg, is as wrong as saying the weight is 60 kg.\nInstead of defining error in this way, we can redefine it so that it takes into account the degree of error. We can define error as the difference between the actual data point and the model’s value. So, in the case of our weight = 6 kg model, an actual weight of 10 kg would have an error of 10 - 6 = 4. This definition of error is often referred to as the residual.\nBelow I plot the residuals of the first 25 Pokémon for our weight = 6 kg model.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .5) +\n  geom_segment(aes(xend = pokedex, y = 6, yend = weight), linetype = 2) +\n  geom_point() +\n  geom_abline(intercept = 6, slope = 0) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nFigure 4: Residuals of the weight = 6 kg model\n\n\n\n\nWe can add up all of the (absolute) residuals to determine the model’s error. Just like with the binary definition of error, we can then compare multiple models. This is what you see in the graph below. For each model, this time ranging from weight = 1 kg to weight = 100 kg, the absolute residuals were calculated and added together.\n\n\nCode\nerrors_residuals <- expand_grid(\n    model = seq(from = 1, to = 100, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %>%\n  mutate(error = abs(weight - model)) %>%\n  group_by(model) %>%\n  summarize(error_sum = sum(error))\n\nggplot(errors_residuals, aes(x = model, y = error_sum)) +\n  geom_line() +\n  scale_y_continuous(breaks = seq(from = 500, to = 1900, by = 200)) +\n  labs(x = \"Model (weight = x kg)\", y = \"Error (sum of residuals)\")\n\n\n\n\n\nFigure 5: Error #2: The sum of residuals\n\n\n\n\nThis graph looks very different compared to the graph where we calculated the error defined as the sum of misses. Now we see that some kind of minimum appears. Unlike the binary definition of error, it now looks like there are fewer best models. More importantly, though, we have defined error in a less crude manner, meaning that the better models indeed capture the data much better than before.\nBut we might still not be entirely happy with this new definition of error either. Calculating the sum of absolute residuals for each model comes with another conceptual problem.\nWhen you sum the number of absolute errors, four errors of 1 are equal to a single error of 4. In other words, you could have a model that is slightly off multiple times or one that might make fewer, but larger, errors. Both would be counted as equally wrong. What do we think of that? Conceptually speaking, we might find it more problematic when a model is very wrong than when the model is slightly off multiple times. If we think that, we need another definition of error.\nTo address this issue, we can square the residuals before adding them together. That way, larger errors become relatively larger compared to smaller errors. Using our previous example, summing four residuals of 1 remains 4, but a single residual of 4 becomes 4 * 4 = 16. The model now gets punished more severely for making large mistakes.\nUsing this new definition of error, we again plot the error for each model, from 1 to 100.\n\n\nCode\nerrors_squared_residuals <- expand_grid(\n    model = seq(from = 1, to = 100, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %>%\n  mutate(error = abs(weight - model)^2) %>%\n  group_by(model) %>%\n  summarize(error_sum = sum(error))\n\nggplot(errors_squared_residuals, aes(x = model, y = error_sum)) +\n  geom_line() +\n  geom_vline(xintercept = mean(pull(pokemon25, weight)), linetype = 2) +\n  labs(x = \"Model\", y = \"Error (sum of squared residuals)\")\n\n\n\n\n\nFigure 6: Error #3: The sum of squared residuals\n\n\n\n\nWe see a smooth curve, with a clear minimum indicated by the vertical dashed line. This vertical line indicates the model that best describes the data. What is the value of the best model exactly? In this case, the answer is 26.14. And it turns out, there is an easy way to determine this value."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-data-driven-model",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-data-driven-model",
    "title": "Understanding regression (part 1)",
    "section": "The data-driven model",
    "text": "The data-driven model\nRather than setting a specific value and seeing how it fits the data, we can also use the data to determine the value that best fits the data. In the previous graph we saw that the best fitting model is one where the weight is equal to 26.14. This value turns out to be the mean of the different weights we have observed in our sample. Had we defined error as simply the sum of absolute residuals, this would be a different value. In fact, the best fitting value would then be equal to 13, or the median. And had we used the binary definition of error, the best fitting value would be the mode, which in our case is: 6.9.\nNote that there is not always a unique answer to which model is the best fitting model, depending on the error definition. For example, it is possible that there are multiple modes. If you use the binary definition of error, that would mean there are multiple equally plausible models. This can be another argument to not define a model’s error in such a crude way.\nThe table below shows an overview of which technique can be used to find the best fitting value, depending on the error definition.\n\n\nCode\ntibble(\n  error_definition = c(\"sum of errors\", \"sum of absolute residuals\", \n    \"sum of squared residuals\"),\n  estimation_technique = c(\"mode\", \"median\", \"mean\")\n) %>%\n  kable(col.names = c(\"Error definition\", \"Estimation technique\"), digits = 2) \n\n\n\n\n\nError definition\nEstimation technique\n\n\n\n\nsum of errors\nmode\n\n\nsum of absolute residuals\nmedian\n\n\nsum of squared residuals\nmean\n\n\n\n\n\nWe can now update our model to refer to the estimation technique, rather than a fixed value. Given that the third definition of error seems to be most suitable, both pragmatically and conceptually, we’ll use the mean:\n\nweight = mean(weight)\n\nThis is also the value you get when you perform a regression analysis in R:\n\n\nCode\nlm(weight ~ 1, data = pokemon25)\n\n\n\nCall:\nlm(formula = weight ~ 1, data = pokemon25)\n\nCoefficients:\n(Intercept)  \n       26.1  \n\n\nBy regressing weight onto 1 we are telling R to run an intercept-only model. This means that R will estimate which line will best fit all the values in the outcome variable, just like we have done ourselves earlier by testing different models such as weight = 6 kg.\nThe result is an intercept value of 26.14, which matches the mean of the weights.\nSo, we now know where the intercept comes from when we run an intercept-only model. It is the mean of the data we are trying to model. Note that it is the mean because we defined the model’s error as the sum of squared residuals. Had we defined the error differently, such as the sum of absolute residuals or the sum of errors, the intercept would be the median or mode of the data instead. Why did we use the sum of squared residuals? We had a conceptual reason of wanting to punish larger residuals relatively more than several smaller errors. It turns out there is another reason to favor squared residuals, which has to do with a nice property of the mean vs. the median. This will be covered in Part 2 of ‘Understanding Regression’.\nThis post was last updated on 2022-04-29."
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html",
    "href": "content/projects/animal-welfare/animal-welfare.html",
    "title": "Animal welfare",
    "section": "",
    "text": "Now that I work at Rethink Priorities I get to devote a significant chunk of my time on projects related to animal welfare. I’ve only recently joined RP, though, so there is not much yet to show, but below I explain why I want to focus on this more and some steps I’ve taken so far."
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html#why-is-this-important",
    "href": "content/projects/animal-welfare/animal-welfare.html#why-is-this-important",
    "title": "Animal welfare",
    "section": "Why is this important?",
    "text": "Why is this important?\n\n“It may come one day to be recognized, that the number of legs, the villosity of the skin, or the termination of the os sacrum, are reasons equally insufficient for abandoning a sensitive being to the same fate. What else is it that should trace the insuperable line? Is it the faculty of reason, or perhaps, the faculty for discourse?…the question is not, Can they reason? nor, Can they talk? but, Can they suffer?\n\nThis (partial) quote by Jeremy Bentham gets to the heart of the matter. Whether something deserves moral concern is predominantly (if not only) a function of whether that something is capable of suffering. Cheating on a partner is bad not because cheating is inherently bad, but because it likely causes great suffering on the cheated-on partner. If your partner doesn’t care about being cheated on (i.e., being in an open relationship), cheating is no longer a bad thing. This shows morality is about the consequences of one’s actions and whether those consequences cause suffering.\nMany animals can suffer. It is unclear and impossible with our current knowledge about consciousness to assess which animals are capable of suffering, but we know enough to confidently say that some animals can suffer. Large mammals such as cows, sheep, goats, and horses can undoubtedly suffer. Smaller creatures such as chicken and turkeys are similarly unlucky and likely also capable. It is less clear when it comes to fish, but I would put my money them being able to suffer rather than being experience-less creatures.\nThe examples of animals I used above are the kinds of animals we farm. These are the kinds of animals we treat in ways that cause them to suffer, with great intensity and in great numbers. Chicken, for example, live in crowded spaces that cause in-fighting, the spread of diseases, and deaths due to, for instance, pile ups. They are artificially selected to grow at unhealthily fast rates, causing physical abnormalities. They are prevented from displaying their instinctive behaviors, such as establishing pecking orders, dust bathing, building nests, and spreading their wings. Sometimes farmers address these problems, although not always in the animal’s best interest. Injuries due to in-fighting is reduced by cutting or burning off the beaks, thus preventing them from harming each other. Other farm animals face similar situations.\nWhat makes it worse is the scale of factory farming. In the Netherlands alone, over 600 million land animals were killed in 2019. And that’s just in the Netherlands, a pretty tiny country. In the U.S., 9.76 billion land animals were killed in 2020. These numbers are so big they almost lose their meaning. The reality is, however, that factory farming causing suffering in billions and billions of individual animals, every year.\nPeople might retort that killing animals for food is simply the natural order of things. This argument is easy to refute: The natural order also sucks. We should not look at nature to determine what is good or bad (this is called the naturalistic fallacy). In nature, all kinds of suffering takes place. Animals (including humans) die due to various causes including disease, disasters, predation, starvation, and so on. These things are normal in nature. As humans, we have done our best to remove all these natural threats from our lives because that reduces our suffering. If we want to be natural, we should invite all these threats back into our lives. Of course, that’s not what we want to do, because we don’t want to suffer.\nI think we should extent that courtesy also to wild animals. We have succeeded in making our lives a lot better, while ignoring the same problems in other species. If we care about the lives of conscious creatures (such as our fellow humans, our pets, our zoo animals), we should also care about the lives of wild animals.\nIn short, farm animals suffer in horrible ways in great numbers, and the same happens in nature (although perhaps less efficiently than in factory farms). Given that suffering is the main reason to care about something, this logically means that we should figure out ways to alleviate their suffering. I hope to contribute to this."
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html#what-am-i-working-on",
    "href": "content/projects/animal-welfare/animal-welfare.html#what-am-i-working-on",
    "title": "Animal welfare",
    "section": "What am I working on?",
    "text": "What am I working on?\nAt Rethink Priorities I’m working on the development of a scale to assess people’s attitudes toward wild animal suffering. We aim to publish the results of this project in an academic journal with the goal for many other academics to begin studying the topic of wild animal suffering.\nI have also joined the following groups:\n\nSociety for the Psychology of Human-Animal Intergroup Relations (PHAIR)\nResearch to End Consumption of Animal Products (RECAP)\n\nBy joining these groups I hope to learn more about current research directions and to join existing projects. Eventually I also hope to use these platforms to share my own work.\nI’ve joined a project by Mercy for Animals on a multi-country survey to develop insights on people’s knowledge, attitudes, behavioral intentions and behaviors regarding farmed animal issues and key advocacy activities.\nI’ve started my own project that is a meta-analysis on meat intervention studies. There have been several meta-analyses on this topic (see here for a recent one), but I think I can contribute in some unique ways by creating a ‘live meta-analysis’ that can continuously be updated with new studies."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html",
    "href": "content/projects/statcheck/statcheck.html",
    "title": "statcheck",
    "section": "",
    "text": "Together with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer statistics-related typos."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#why-is-this-important",
    "href": "content/projects/statcheck/statcheck.html#why-is-this-important",
    "title": "statcheck",
    "section": "Why is this important?",
    "text": "Why is this important?\nSimilar to my tidystats project, our aim is to address a particular problem in statistics reporting: the reporting of incorrect statistics.\nAs has been shown by Michèle and her colleagues, statistics are often reported incorrectly (Nuijten et al., 2016). This is likely due to the fact that researchers do not have the necessary software tools to reliably take the output of statistics from their data analysis software and enter it into their text editor. Instead, researchers are likely to copy statistics from the output by hand or by copy-pasting the output. Both techniques are error-prone, resulting in many papers containing statistical typos. This is a problem because statistical output is used in meta-analyses to aggregate the evidence for particular theories, which sometimes also inform policy. In some cases, the errors may even be so large that it affects the conclusion drawn from the statistical test."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#what-am-i-working-on",
    "href": "content/projects/statcheck/statcheck.html#what-am-i-working-on",
    "title": "statcheck",
    "section": "What am I working on?",
    "text": "What am I working on?\nAdmittedly, I am simply joining Michèle and her efforts to help researchers make fewer typos. She and her colleagues have already done a lot of the work—we’re now just trying to make it even better. For example, Sacha Epskamp and Michèle developed statcheck. statcheck is an R package designed to catch statistical reporting mistakes. It works by first extracting statistics from a paper (e.g., t values, degrees of freedom, p-values). It then uses the test statistic and degrees of freedom to re-calculate the p-value and compare it to the reported p-value. If the two don’t match, there is probably a reporting mistake.\nYou can use the statcheck package in R to check your paper or you can use the web app. Using the web app consists of simply uploading your paper and checking the results. You can then go back to the paper and correct the mistakes.\nWith my experience creating tidystats, and particularly the tidystats Word add-in, we’ve started to create a Word add-in for statcheck. This add-in allows researchers to scan their document for statistical inconsistencies, find them, and fix them. This add-in is currently in beta and we hope to release it soon.\nWe are also working on improving statcheck together with the eScience Center. Together with their help we hope to expand statcheck so it can catch a greater variety of statistical inconsistencies. We have had some preparatory meetings with them and plan to fully begin this project soon."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#links",
    "href": "content/projects/statcheck/statcheck.html#links",
    "title": "statcheck",
    "section": "Links",
    "text": "Links\n\nThe web app\nThe R package on CRAN\nThe GitHub page of statcheck\nThe GitHub page of the statcheck Word add-in."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html",
    "href": "content/projects/tidystats/tidystats.html",
    "title": "tidystats",
    "section": "",
    "text": "tidystats is a project centered around creating software to improve how statistics are reported and shared in the field of (social) psychology."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#why-is-this-important",
    "href": "content/projects/tidystats/tidystats.html#why-is-this-important",
    "title": "tidystats",
    "section": "Why is this important?",
    "text": "Why is this important?\nWith this project, I hope to address two problems in statistics reporting: Incorrect and incomplete statistics reporting.\nStatistics are often reported incorrectly (Nuijten et al., 2016). I think this is because researchers do not have the necessary software tools to reliably take the output of statistics from their data analysis software and enter it into their text editor. Instead, researchers are likely to copy statistics from the output by hand or by copy-pasting the output. Both techniques are error-prone, resulting in many papers containing statistical typos. This is a problem because statistical output is used in meta-analyses to aggregate the statistical evidence for theories, which in turn may affect policy. In some cases, the errors may even be so large that it affects the conclusion drawn from the statistical test.\nThere is also a more fundamental issue. Researchers usually only report the statistics in their manuscript and nowhere else. As a result, researchers face trade-offs between reporting all statistics, writing a legible text, and journal guidelines. Reporting all statistics makes results sections difficult (and boring) to read and it also takes up valuable space. Consequently, researchers are likely to only report the statistics that they deem to be relevant, rather than reporting all the statistics. While this is fine for someone who wants to simply read the paper and get the main takeaway, this is not desirable from a cumulative science perspective. All statistics should be easily available so they can be build on in future research."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#what-am-i-working-on",
    "href": "content/projects/tidystats/tidystats.html#what-am-i-working-on",
    "title": "tidystats",
    "section": "What am I working on?",
    "text": "What am I working on?\nI have designed an R package called tidystats that enables researchers to export the statistics from their analyses into a single file. It works by adding your analyses to a list (in R) and then exporting this list to a JSON file. This file will contain all the statistics from the analyses, in an organized format, ready to be used in other software.\nBy storing the output of statistical tests into a separate file, rather than only in one’s manuscript, the researcher no longer needs to worry about which analyses to report in the space-limited manuscript. They can simply share the file together with the manuscript, on OSF or as supplemental material.\nAn additional benefit is that because JSON files are easy to read for computers, it is (relatively) easy to write software that does cool things with these files.\nAn example of software that can read the JSON file is the tidystats Microsoft Word add-in. This add-in can be installed via the Add-in Store from inside Word. With this add-in, researchers can upload the JSON file to reveal a user-friendly list of their analyses. Clicking on an analysis reveals the associated statistics and clicking on a statistic inserts it into the document. This add-in also comes with several time-saving features, such as inserting multiple statistics at once (immediately in APA style) and automatic updating of statistics.\nRecently, I’ve also obtained a grant to work on tidystats. Thanks to this grant, the functionality of tidystats will be expanded, both in terms of adding support for more analyses and by expanding to other platforms, such as Python and Google Docs.\nBesides working on the software itself, I also spent some time on making it accessible. I have given talks introducing tidystats and I’ve created a website to help people become familiar with tidystats."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#links",
    "href": "content/projects/tidystats/tidystats.html#links",
    "title": "tidystats",
    "section": "Links",
    "text": "Links\n\nThe tidystats website\nR package on CRAN\nR package GitHub repository\nThe tidystats Word add-in in AppSource (the Office add-in store)\nWord add-in GitHub repository\nA blog post describing an example of using tidystats"
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html",
    "title": "Cognitive dissonance",
    "section": "",
    "text": "Cognitive dissonance refers to a state of aversive arousal that is experienced when people realize they possess mutually inconsistent cognitions. This state is the foundation of cognitive dissonance theory (CDT)—a theory developed by Leon Festinger in 1957. Several of my projects are aimed at assessing the evidence for this theory and at applying this theory to other issues (although now that I’ve left academia, these projects have been deprioritized)."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#why-is-this-important",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#why-is-this-important",
    "title": "Cognitive dissonance",
    "section": "Why is this important?",
    "text": "Why is this important?\nThe theory of cognitive dissonance can explain many different phenomena that we should understand so that we may intervene and improve the lives of others. For example, cognitive dissonance theory has been used to explain religious beliefs, unhealthy behaviors, and people’s attitude towards animals.\nBefore the theory can be applied, however, it needs to be verified. We need to have sufficient evidence to believe in the theory. The social psychological evidence we have for the theory is, however, quite weak. The research stems from old research, mostly conducted in the 50s, 60s, and 70s. While this would not necessarily be a problem, it is a problem in the case of social psychology. The original studies were conducted with extremely low sample sizes and without pre-registration, or other tools that limit p-hacking. This means that many past findings may be false positives, which is supported by recent findings that show many findings in psychology do not replicate."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#what-am-i-working-on",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#what-am-i-working-on",
    "title": "Cognitive dissonance",
    "section": "What am I working on?",
    "text": "What am I working on?\nI am one of the lead investigators of a large-scaled replication project. In this project, we will try and replicate a seminal finding in the cognitive dissonance literature. Specifically, our aim is to replicate the classic finding that people who write a counterattitudinal essay (e.g., students arguing in favor of a tuition increase) become more in favor of the position they argued for. We have submitted this project as a registered report to Advanced in Methods and Practices in Psychological Science (AMPPS). There it has received an in-principle acceptance. Data collection is currently underway.\nI also hope to start up a meta-analysis project to produce live reviews of studies from the cognitive dissonance literature."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#links",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#links",
    "title": "Cognitive dissonance",
    "section": "Links",
    "text": "Links\n\nThe landing page of our large-scaled replication project\nThe stage-1 accepted manuscript"
  },
  {
    "objectID": "content/projects.html",
    "href": "content/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Farm animals and wild animals suffer in horrible ways in great numbers. At Rethink Priorities, I contribute to various projects aimed at addressing this important problem.\n\n\n\n\n\n\n\n\n\n\n\nIn this project I aim to assess the evidence for cognitive dissonance theory using a large-scaled replication study of a seminal cognitive dissonance study.\n\n\n\n\n\n\n\n\n\n\n\nTogether with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer statistics-related typos.\n\n\n\n\n\n\n\n\n\n\n\ntidystats refers to a collection of software solutions to improve how statistics are reported and shared in the field of (social) psychology.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "About",
    "section": "",
    "text": "Before joining Rethink Priorities, I was an assistant professor in the Department of Social Psychology at Tilburg University.\nOn this website you can find information about some of the projects I’m involved in. Some notable projects I’m working on are tidystats and a large-scaled replication study of cognitive dissonance. Besides writing about these projects, I also blog posts about various topics, including tutorials or opinion pieces.\nOne of my main research interests concern animal welfare. I think animal welfare, and their lack thereof, is one of the most pressing issues in the world at this moment and as a fruitful area of research where influential theories in social psychology (such as cognitive dissonance) can be applied and tested.\nI’m also interested in the methodology of psychological research and ways to improve how we conduct science. A notable project I’m working on is tidystats. This is a software solution to help researchers more easily and more reproducibly report statistics in scientific manuscripts. It’s main goal is to get researchers to report more statistics with fewer errors. I’m pretty proud of this project, so please check it out on the tidystats project or the tidystats website.\nI also have teaching experience thanks to my time as an assistant professor. I’m quite experienced in teaching undergraduate courses, in both small and large groups of students. Besides course work I have also provided many R workshops (although I have less time for that now).\nThis website is created using Quarto."
  },
  {
    "objectID": "content/cv/cv.html#employment",
    "href": "content/cv/cv.html#employment",
    "title": "Willem Sleegers",
    "section": "Employment",
    "text": "Employment\n\n\n\n\n\n\n\n\n\n2021-current\nSenior Behavioral Scientist at Rethink Priorities\n\n\n2018-2021\nTenure track Assistant Professor in social psychology at Tilburg University\n\n\n2016-2018\nFixed term Assistant Professor in social psychology at Tilburg University\n\n\n2012-2016\nGraduate student on the topic of physiological arousal in meaning maintenance at Tilburg University\n\n\n2011-2012\nStudent assistant during my Research Master: Behavioral Science at Nijmegen University\n\n\n2012-2013\nProgrammer of experimental psychology tasks for FrieslandCampina\n\n\n2007-2010\nMedia analyst for Report International"
  },
  {
    "objectID": "content/cv/cv.html#education",
    "href": "content/cv/cv.html#education",
    "title": "Willem Sleegers",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\n\n\n2012-2016\nGraduate student at Tilburg University supervised by prof. dr. Ilja van Beest and dr. Travis Proulx\n\n\n2012-2016\nStudent member of the Kurt Lewin Institute (KLI)\n\n\n2010-2012\nResearch Master Behavioural Science at Nijmegen University\n\n\n2010-2012\nExpert track in data-analysis\n\n\n2007-2010\nPsychology BSc. at Nijmegen University\n\n\n2007-2010\nHonours Program of Psychology at Nijmegen University"
  },
  {
    "objectID": "content/cv/cv.html#publications",
    "href": "content/cv/cv.html#publications",
    "title": "Willem Sleegers",
    "section": "Publications",
    "text": "Publications\n\nIn preparation\n\n\n\n\n\n\n\n\nvan Leeuwen, F., Jaeger, B., Sleegers, W. W. A., & Petersen, M. B. (in prep.) Pathogen avoidance and conformity: Salient infectious disease does not increase conformity.\n\n\nJaeger, B., Sleegers, W. W. A., Stern, J., Penke, L., & Jones, A. (in prep.) The accuracy and meta-accuracy of personality impressions from faces.\n\n\nSleegers, W. W. A. & Jaeger, B. (in prep.) The social cost of correcting others.\n\n\n\n\n\n\n\nPreprints\n\n\n\n\n\n\n\n\nJaeger, B., Sleegers, W. W. A. (2020) Racial discrimination in the sharing economy: Evidence from Airbnb markets across the world. https://psyarxiv.com/qusxf\n\n\nBreznau, N., et al. (2021) The Crowdsourced Replication Initiative: Investigating Immigration and Social Policy Preferences. Executive Report. https://osf.io/preprints/socarxiv/6j9qb/\n\n\nBreznau, N., et al. (2021) How many replicators does it take to achieve reliability? Investigating researcher variability in a crowdsourced replication. https://osf.io/preprints/socarxiv/j7qta/\n\n\n\n\n\n\n\nStage-1 accepted manuscripts\n\n\n\n\n\n\n\n\n\n2021\nSleegers, W. W. A., Vaidis, D. (shared first author) et al. (2021). A multi-lab replication of the induced compliance paradigm of cognitive dissonance. Advances in Methods and Practices in Psychological Science. https://osf.io/52wpj\n\n\n\n\n\n\n\nPeer-reviewed journals\n\n\n\n\n\n\n\n\n\n2021\nPronk, T. M., Bogaers, R. I., Verheijen, M. S., Sleegers, W. W. A. (in press). Pupil size predicts partner choices in online dating. Social Cognition.\n\n\n2021\nEvans, A. M., Kogler, C., & Sleegers, W. W. A. (in press). No effect of synchronicity in online social dilemma experiments: A registered report. Judgment and Decision Making.\n\n\n2021\nVan Osch, Y., & Sleegers, W. W. A. (2021). Replicating and reversing the group attractiveness effect: Relatively unattractive groups are perceived as less attractive than the average attractiveness of their members. Acta Psychologica, 217, 103331. https://.doi.org/10.1016/j.actpsy.2021.103331\n\n\n2021\nBrandt, M., Sleegers, W. W. A. (2021) Evaluating belief system networks as a theory of political belief system dynamics, 25(2), 159-185. https://doi.org/10.1177/1088868321993751\n\n\n2021\nJones, B. C., DeBruine, L. M., Flake, J. K., et al. (2021) To which world regions does the valence–dominance model of social perception apply? Nature Human Behavior, 5, 159–169. https://doi.org/10.1038/s41562-020-01007-2\n\n\n2020\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2020) Pupillometry and hindsight bias: Physiological arousal predicts compensatory behavior. Social Psychological and Personality Science. https://doi.org/10.1177/1948550620966153\n\n\n2020\nEvans, A., Sleegers, W. W. A., & Mlakar, Ž. (2020). Individual differences in receptivity to scientific bullshit. Judgment and Decision Making, 15(3), 401-412.\n\n\n2020\nJaeger, B., Sleegers, W. W. A., & Evans, A. M. (2020). Automated classification of demographics from face images: A tutorial and validation. Social and Personality Psychology Compass, 14(3). https://doi.org/10.1111/spc3.12520\n\n\n2019\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2019). Confirmation bias and misconceptions: Pupillometric evidence for a confirmation bias in misconceptions feedback. Biological Psychology, 145, 76–83. https://doi.org/10.1016/j.biopsycho.2019.03.018\n\n\n2019\nBender, M., van Osch, Y., Sleegers, W. W. A., & Ye, M. (2019). Social support benefits psychological adjustment of international students: Evidence from a meta-analysis. Journal of Cross-Cultural Psychology, 50(7), 827–847. https://doi.org/10.1177/0022022119861151\n\n\n2019\nVan ’t Veer, A. E., & Sleegers, W. W. A. (2019). Psychology data from an exploration of the effect of anticipatory stress on disgust vs. Non-disgust related moral judgments. Journal of Open Psychology Data, 7(1), 1. https://doi.org/10.5334/jopd.43\n\n\n2018\nJaeger, B., Sleegers, W. W. A., Evans, A. M., Stel, M., & van Beest, I. (2018). The effects of facial attractiveness and trustworthiness in online peer-to-peer markets. Journal of Economic Psychology. https://doi.org/https://doi.org/10.1016/j.joep.2018.11.004\n\n\n2017\nProulx, T., Sleegers, W. W. A., & Tritt, S. (2017). The expectancy bias: Expectancy-violating faces evoke earlier pupillary dilation than neutral or negative faces. Journal of Experimental Social Psychology, 70, 69-79. https://doi.org/10.1016/j.jesp.2016.12.003\n\n\n2017\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2017). The social pain of Cyberball: Decreased pupillary reactivity to exclusion cues. Journal of Experimental Social Psychology, 69, 187–200. https://doi.org/10.1016/j.jesp.2016.08.004\n\n\n2015\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2015). Extremism reduces conflict arousal and increases values affirmation in response to meaning violations. Biological Psychology, 108, 126–131. https://doi.org/10.1016/j.biopsycho.2015.03.012\n\n\n2015\nSleegers, W. W. A., & Proulx, T. (2015). The comfort of approach: Self-soothing effects of behavioral approach in response to meaning violations. Frontiers in Psychology, 5, 1–10. https://doi.org/10.3389/fpsyg.2014.01568\n\n\n\n\n\n\n\nBook chapters\n\n\n\n\n\n\n\n\n\n2019\nvan Beest, I., & Sleegers, W.W.A (2019). Physiostracism: A case for non-invasive measures of arousal in ostracism research. In S. C. Rudert, R. Greifeneder, & K. D. Williams (Eds.), Current directions in ostracism, social exclusion and rejectionresearch. Routledge. https://doi.org/10.4324/9781351255912\n\n\n\n\n\n\n\nDissertation\n\n\n\n\n\n\n\n\n\n2017\nSleegers, W. W. A. (2017). Meaning and pupillometry: The role of physiological arousal in meaning maintenance (Doctoral dissertation). Retrieved from https://pure.uvt.nl/portal/en/publications/meaning-and-pupillometry(20680e63-e785-43d0-a3ae-e97b26de5f05).html\n\n\n\n\n\n\n\nSoftware\n\n\n\n\n\n\n\n\n\n2020\nSleegers, W. W. A. (2020). tidystats: Save output of statistical tests (Version 0.5) [Computer software]. https://doi.org/10.5281/zenodo.4041859\n\n\n2020\nSleegers, W. W. A. (2020). tidystats (Version 1) [Computer software]. https://doi.org/10.5281/zenodo.4434634\n\n\n\n\n\n\n\nWebsites\n\n\n\n\n\n\n\n\nMy personal website where I blog about (some of) my research.https://www.willemsleegers.com\n\n\nThe tidystats website, a support website for my tidystats software.https://www.tidystats.io"
  },
  {
    "objectID": "content/cv/cv.html#presentations",
    "href": "content/cv/cv.html#presentations",
    "title": "Willem Sleegers",
    "section": "Presentations",
    "text": "Presentations\n\nInvited talks\n\n\n\n\n\n\n\n\n\n2021\nSleegers, W. W. A. (2021, March). tidystats. Talk for a research group at the Ministry of Defence.\n\n\n2021\nSleegers, W. W. A. (2021, February). tidystats. Talk for the BSI at Nijmegen University.\n\n\n2021\nSleegers, W. W. A. (2021, February). Cognitive dissonance RRR. Lab meeting at Cardiff University.\n\n\n2018\nSleegers, W. W. A. (2018, December). Pupillometry and psychology. Colloquium presentation for the Laboratoire de Psychologie Sociale department at Paris Descartes University.\n\n\n2018\nSleegers, W. W. A. (2018, October) tidystats. Colloquium presentation for the Methodology and Statistics department at Leiden University.\n\n\n2018\nSleegers, W. W. A. (2018, March) tidystats. Colloquium presentation for the MTO department at Tilburg University.\n\n\n2017\nSleegers, W. W. A. (2017, December). Meaning and pupillometry: The role of physiological arousal in meaning maintenance. Presentation at the ASPO conference as part of receiving the best ASPO dissertation award.\n\n\n2017\nSleegers, W. W. A. (2017, March). Pupillometry and psychological processes. Colloquium presentation at Cardiff University.\n\n\n2015\nSleegers, W. W. A., Proulx, T. & Van Beest, I. (2015, October). Capturing the physiological response to meaning violations: An eye tracker approach. Colloquium presentation at Tilburg University.\n\n\n\n\n\n\n\nConference presentations\n\n\n\n\n\n\n\n\n\n2019\nSleegers, W. W. A. (2019, July) tidystats. Lightning talk at the SIPS conference, Rotterdam, the Netherlands.\n\n\n2019\nSleegers, W. W. A. & Jaeger, B. (2019, December) The Social Cost of Correcting Others. Talk at the ASPO conference, Wageningen, the Netherlands.\n\n\n2018\nSleegers, W. W. A. (2018, June) tidystats. Lightning talk at the SIPS conference, Grand Rapids, MI.\n\n\n2017\nSleegers, W. W. A. (2017, August). oTree for social scientists. Presentation at the TIBER conference, Tilburg, the Netherlands.\n\n\n2016\nSleegers, W. W. A., Proulx, T. & Van Beest (2016, December). Evidence of aversive arousal motivating compensatory behavior. Presentation at ASPO conference, Leiden, the Netherlands.\n\n\n2014\nProulx, T. & Sleegers, W. W. A. (2014, May). Meaning Maintenance Model: Towards a unified account of threat-compensation behaviors. Presentation at KLI conference, Zeist, the Netherlands.\n\n\n2014\nSleegers, W. W. A., Proulx, T., & Van Beest (2014, December). Cyberball and eye tracking: Support for the numbing hypothesis of social exclusion. Presentation at ASPO 2014, Groningen, the Netherlands.\n\n\n2014\nSleegers, W. W. A., Proulx, T., & Van Beest (2014, July). Ostracism and eye tracking. Presentation at EASP preconference on threat regulation, Amsterdam, the Netherlands.\n\n\n\n\n\n\n\nSmall meetings\n\n\n\n\n\n\n\n\n\n2019\nSleegers, W. W. A. (2019, November) Addressing Incorrect and Incomplete Statistics Reporting (with tidystats). Talk at the meta-research day at Tilburg University, the Netherlands.\n\n\n\n\n\n\n\nPoster presentations\n\n\n\n\n\n\n\n\n\n2017\nSleegers, W. W. A. (2017, July). Pupillometry and psychology: Pupillometry as an experimental tool for psychologists. Poster session presented at the Ostracism, Social Exclusion, and Rejection conference, Vitznau, Switzerland.\n\n\n2016\nSleegers, W. W. A., Proulx, T., & Van Beest (2016, January). Ostracism and eye tracking: Decreased pupillary reactivity to exclusion cues. Poster session presented at the SPSP conference, San Diego.\n\n\n2015\nSleegers, W. W. A., Proulx, T., & Van Beest (2015, December). Meaning and misconceptions: The effect of error feedback and commitment towards misconceptions on pupil size. Poster session presented at ASPO conference, Amsterdam.\n\n\n2015\nSleegers, W. W. A., Proulx, T. & Van Beest (2015, March). Cyberball and eye tracking: Support for the numbing hypothesis of social exclusion. Poster session presented at ICPS conference, Amsterdam, the Netherlands.\n\n\n2014\nSleegers, W. W. A., Proulx, T, & Van Beest, I. (2014, May). Extremism and the response to meaning threats: Extremism reduces pupillary response to threat and increases affirmation of values. Poster session presented at the KLI conference, Zeist, the Netherlands.\n\n\n\n\n\n\n\nValorization presentations\n\n\n\n\n\n\n\n\n\n2017\nSleegers, W. W. A., & Wagemans, F. M. A. (2017, November). The psychology behind eye tracking. Presentation organized by the Academic Forum of Tilburg University.\n\n\n2017\nWagemans, F. M. A., & Sleegers, W. W. A. (2017, June). Waar walg jij van? Presentation at Mundial festival on attentional processes and disgust sensitivity using eye tracking."
  },
  {
    "objectID": "content/cv/cv.html#journals",
    "href": "content/cv/cv.html#journals",
    "title": "Willem Sleegers",
    "section": "Journals",
    "text": "Journals\n\n\n\nI reviewed for Behavioural Processes, Biological Psychology, British Journal of Psychology, British Journal of Social Psychology, Collabra, European Journal of Social Psychology, Group Processes & Intergroup Relations, International Journal of Psychology, International Review of Social Psychology, Journal of Consumer Behaviour, Journal of Experimental Social Psychology, Journal of Social and Personal Relationships, Personality and Social Psychology Bulletin, PLOS ONE, Self and Identity, Social Cognition, Social Influence, Social Psychology, Current Psychology, Journal of Cognitive Psychology."
  },
  {
    "objectID": "content/cv/cv.html#teaching",
    "href": "content/cv/cv.html#teaching",
    "title": "Willem Sleegers",
    "section": "Teaching",
    "text": "Teaching\n\nCourses\n\n\n\n\n\n\n\n\n\n2017-2021\nSocial Psychology\n\n\n2016-2021\nAttitudes and Advertising\n\n\n2019-2021\nUnderstanding Data with R\n\n\n2015/2017/2019\nResearch Master: Experimental Research and Meta-Analysis\n\n\n2016-2021\nCourse in R software\n\n\n2019-2021\nUnderstanding Data with R\n\n\n\n\n\n\n\nSeminars\n\n\n\n\n\n\n\n\n\n2012-2017\nSocial Psychology\n\n\n2015-2016\nIntroduction and History of Psychology\n\n\n2014-2015\nCultural Psychology\n\n\n2013-2015\nAcademic Skills\n\n\n2012-2013\nGroup Skills\n\n\n\n\n\n\n\nIndividual lectures\n\n\n\n\n\n\n\n\n\n2016\nSocial Psychology\n\n\n2014\nIntroduction and History of Psychology on intrapersonal conflict\n\n\n2013\nIntroduction to Social Psychology for prospective students\n\n\n\n\n\n\n\nSupervision\n\n\n\n\n\n\n\n\n\n2021\nResearch Master in Psychology theses\n\n\n2016-2021\nMaster in Psychology theses\n\n\n2013-2021\nBachelor in Psychology theses\n\n\n2012-2018\nResearch Skills in Psychology\n\n\n\n\n\n\n\nCoordination\n\n\n\n\n\n\n\n\n\n2014-2021\nSocial Psychology\n\n\n2016-2021\nAttitudes and Advertising\n\n\n\n\n\n\n\nOther\n\n\n\n\n\n\n\n\n\n2013-2021\nAn introduction to R; part of the Kurt Lewin Institute course program"
  },
  {
    "objectID": "content/cv/cv.html#technical-skills",
    "href": "content/cv/cv.html#technical-skills",
    "title": "Willem Sleegers",
    "section": "Technical skills",
    "text": "Technical skills\n\nStatistics\n\n\n\n\n\nR: A free software environment for statistical computing and graphics\n\n\nSPSS: A proprietary data analysis program"
  },
  {
    "objectID": "content/cv/cv.html#programming",
    "href": "content/cv/cv.html#programming",
    "title": "Willem Sleegers",
    "section": "Programming",
    "text": "Programming\n\n\n\n\n\nPython: A cross-platform procedural programming language\n\n\nHTML: Markup language for creating web pages and web applications\n\n\nCSS: Markup language for styling web pages and web applications\n\n\nJavaScript: A programming language for creating web applications\n\n\nDjango: A high-level Python Web framework\n\n\nHugo: A static HTML and CSS website generator\n\n\n\n\n\n\nExperimental design\n\n\n\n\n\n\n\n\nMillisecond’s Inquisit: Stimulus delivery and experimental design software\n\n\noTree: Framework based on Python and Django to create standard and interactive online psychological experiments\n\n\nTobii Studio and Tobii Studio Extensions for E-prime: software to run eye tracker experiments using Tobii eye trackers\n\n\nPsychology Software Tool’s E-Prime: Stimulus delivery and experimental design software\n\n\nAdobe’s Authorware: Stimulus delivery and experimental design software. This has been discontinued, please do not make me use it\n\n\nNeurobehavioural Systems’ Presentation®: A stimulus delivery and experimental control program for neuroscience"
  },
  {
    "objectID": "content/blog.html",
    "href": "content/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\n\nBayesian statistics seems pretty cool, but I don’t really know how to apply it yet. In this blog post, I try to setup a Bayesian workflow that teaches both you and me how to do it.\n\n\n\n\n\n\nNov 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nanimal welfare\n\n\ndata cleaning\n\n\nAPIs\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nanimal welfare\n\n\ndata cleaning\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npower analysis\n\n\nsimulation\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\npower analysis\n\n\nsimulation\n\n\n\n\nSimulation-based power analyses make it easy to understand what power is: Power is simply counting how often you find the results you expect to find. Running simulation-based power analyses might be new for some, so in this blog post I present code to simulate data for a range of different scenarios.\n\n\n\n\n\n\nOct 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwriting\n\n\nstatistics\n\n\npower analysis\n\n\n\n\nMethod sections in academic (psychology) papers usually consist of the following sections: Participants, Design, Procedure, and Materials. They also tend to be presented in this order. But is this, generally speaking, the right order? I don’t think so.\n\n\n\n\n\n\nJul 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidystats\n\n\nstatistics\n\n\ntutorial\n\n\n\n\nI illustrate how to use my tidystats software to analyze and report the results of a replication study that was part of the Many Labs 1 project.\n\n\n\n\n\n\nApr 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\n\n\nIn a recent tweet I asked the question why we use \\(n - 1\\) to calculate the variance of a sample. Many people contributed an answer, but many of them were of the type I feared. Most consisted of some statistical jargon that confuses me more, rather than less. Other responses were very useful, though, so I recommend checking out the replies to the tweet. In this post, I will try to describe my favorite way of looking at the issue.\n\n\n\n\n\n\nAug 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\npower analysis\n\n\n\n\nA curious thing happened in the field of social psychology: Social psychologists finally realized that statistical power is important. Unfortunately, they then skipped the step of figuring out how to do them correctly. Here I list some papers on power analyses that I hope help in improving the way we do them.\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nregression\n\n\n\n\nThis is Part 1 of a series of blog posts on how to understand regression. The goal is to develop an intuitive understanding of the different components of regression. In this first post, we figure out where the estimate of an intercept-only regression model comes from.\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willem Sleegers",
    "section": "",
    "text": "Senior Behavioral Scientist at Rethink Priorities\n \n  \n   \n  \n    \n     E-mail\n  \n  \n    \n     Twitter\n  \n  \n    \n     Google Scholar\n  \n  \n    \n     GitHub"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Willem Sleegers",
    "section": "About",
    "text": "About\nI’m a Senior Behavioral Scientist at Rethink Priorities. I am part of the survey team, which means I conduct research on attitude assessments and attitude change, using surveys and experimental designs. Before joining Rethink Priorities, I was an assistant professor in the Department of Social Psychology at Tilburg University. On this website you can find information about some of the projects I’m involved in. I also blog about various topics related to my work.\nLearn more"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Willem Sleegers",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\nAnimal welfare\n\n\nFarm animals and wild animals suffer in horrible ways in great numbers. At Rethink Priorities, I contribute to various projects aimed at addressing this important problem.\n\n\n\n\n\n\n\n\n\n\nCognitive dissonance\n\n\nIn this project I aim to assess the evidence for cognitive dissonance theory using a large-scaled replication study of a seminal cognitive dissonance study.\n\n\n\n\n\n\n\n\n\n\nstatcheck\n\n\nTogether with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer statistics-related typos.\n\n\n\n\n\n\n\n\n\n\ntidystats\n\n\ntidystats refers to a collection of software solutions to improve how statistics are reported and shared in the field of (social) psychology.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Willem Sleegers",
    "section": "Blog posts",
    "text": "Blog posts\n\n\n\n\n\n\nFiguring out Bayesian statistics\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\n\nBayesian statistics seems pretty cool, but I don’t really know how to apply it yet. In this blog post, I try to setup a Bayesian workflow that teaches both you and me how to do it.\n\n\n\n\n\n\nNov 24, 2022\n\n\n\n\n\n\n\n\nVoting behavior of Dutch political parties on animal welfare motions\n\n\n\n\n\n\n\nanimal welfare\n\n\ndata cleaning\n\n\nAPIs\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\n\n\n\n\n\n\nAnimals slaughtered in the Netherlands\n\n\n\n\n\n\n\nanimal welfare\n\n\ndata cleaning\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n\n\n\n\nNo matching items\n\n\nMore posts"
  },
  {
    "objectID": "index.html#cv",
    "href": "index.html#cv",
    "title": "Willem Sleegers",
    "section": "CV",
    "text": "CV\nI’m an Senior Behavioral Scientist at Rethink Priorities. I have recently left academia after having built up 10 years of research experience, with publications of both scientific papers in peer-reviewed journals, as well as software publications. It should be no surprise then that my skill set consists of research skills (e.g., experimental design, data analysis, writing) and technical skills (e.g., programming). I now look forward to applying my skills to topics of great impact together with my colleagues at Rethink Priorities.\nLearn more"
  },
  {
    "objectID": "content/posts/5-my-bayesian-workflow-and-tutorial/my-bayesian-workflow-and-tutorial.html",
    "href": "content/posts/5-my-bayesian-workflow-and-tutorial/my-bayesian-workflow-and-tutorial.html",
    "title": "Figuring out Bayesian statistics",
    "section": "",
    "text": "This post is about figuring out how Bayesian statistics works and about developing a workflow to conduct Bayesian analyses. Specifically, I want to go through the process of running a Bayesian analysis and visualizing the different steps of that analysis in order to make sure I know what I’m doing. With a bit of luck this is educational and we’ll both end up learning how this works!\nIf you want to follow a long, run the following setup code.\nThe data we will use to play with is the same data Richard McElreath uses in Chapter 4 of his amazing book called Statistical Rethinking. The data consists of partial census data of the !Kung San, compiled from interviews conducted by Nancy Howell in the late 1960s. Just like in the book, we will focus only on people 18 years or older, so in the code below we create a subset of the data and store the result in a data frame called data.\nThe general idea behind Bayesian statistics is that you start with some prior beliefs about the parameters of interest and then update those beliefs with the data. Note that this doesn’t mean that you have to personally accept those beliefs. You could simply postulate a belief to serve a particular purpose, such as assuming that a null effect is most likely even though you personally believe that there should be an effect. It does mean that when we want to analyze the data, we should start with defining our beliefs, rather than immediately jumping into running an analysis.\nLet’s focus our first question on the heights in the data. We should begin by defining a belief that describes the different heights, which is based on our a priori knowledge of the heights of the Dobe area !Kung San. In other words, we have to describe what we believe their heights to be. This is unlike what you have to do with frequentist statistics, so this part might be a bit tricky.\nTo make it easier, we will use the amazing brms package to both define and inspect our beliefs, as well as use the data to update those beliefs."
  },
  {
    "objectID": "content/posts/5-my-bayesian-workflow-and-tutorial/my-bayesian-workflow-and-tutorial.html#an-intercept-only-model",
    "href": "content/posts/5-my-bayesian-workflow-and-tutorial/my-bayesian-workflow-and-tutorial.html#an-intercept-only-model",
    "title": "Figuring out Bayesian statistics",
    "section": "An intercept-only model",
    "text": "An intercept-only model\nIf we start with analyzing only the heights data then we’ll be constructing an intercept-only model. You may be familiar with the R formula for this type of model: height ~ 1.\nWith this formula and the data we can actually use brms to figure out which priors we need to set by running the get_prior() function. This is probably the easiest way to figure which priors you need when you’re just starting out using brms.\n\n\nCode\nget_prior(height ~ 1, data = data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\nstudent_t(3, 154.3, 8.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 8.5)\nsigma\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\nThe output shows us that we need to set two priors, one for the Intercept and one for sigma. brms also already determined a default prior for each, but we’ll ignore that for now.\nThis is not the best way to think about which priors we need, though. Using the function will give you the answer, but it doesn’t really improve our understanding of why we need these two priors. In this case we also omitted an important specification of the heights, which is that we think they are normally distributed (the default assumption in get_prior()). So let’s instead write down our model in a different way, which explicitly specifies how we think the heights are distributed and which parameters we need to set priors on. If we think the heights are normally distributed, we define our model like this:\n\\[heights_i ∼ Normal(\\mu, \\sigma)\\]\nWe explicitly note that the heights come from a normal distribution, which is determined by the parameters \\(\\mu\\) and \\(\\sigma\\). This then also immediately tells us that we need to set two priors, one on \\(\\mu\\) and one on \\(\\sigma\\).\nIn our intercept-only model, the \\(\\mu\\) parameter refers to our intercept and the \\(\\sigma\\) parameter refers to, well, sigma. It’s not often discussed in the literature I’m familiar with, but we’ll figure it out below. In fact, let’s discuss each of these parameters in turn and figure out what kind of prior makes sense.\n\nThe Intercept prior (\\(\\mu\\))\nThe prior for the intercept indicates what we believe the average height of the !Kung San to be.\nbrms has set the default Intercept prior as a Student t-distribution with 3 degrees of freedom, a mean of 154.3 and a standard deviation of 8.5. That means brms starts off with a ‘belief’ that the average of the heights is 154.3, but with quite some uncertainty reflected in the standard deviation of 8.5 and the fact that the distribution is a Student t-distribution. A Student t-distribution has thicker tails compared to a normal distribution, meaning that lower and higher numbers are considered more likely compared to a normal distribution, at least when the degrees of freedom are low. At higher degrees of freedom, the t-distribution becomes more and more like the normal distribution. So, the thicker tails of the t-distributions means smaller and taller average heights are relatively more plausible.\nBut this is the default prior. brms determines this automatic prior by peeking at the data, which is not what we want to do. Instead, we should create our own.\nSo what do I believe the average height to be? As a Dutch person, I might be under the impression that the average height is around 175 centimeters. This is probably too tall to use as an average for the !Kung San because we’re known for being quite tall. So I think the average should be lower than 175, perhaps 170. I am not very sure, though. After all, I am far from an expert on people’s heights; I am only using my layman knowledge here. An average of 165 seems possible to me too. So let’s describe my belief in the form of a distribution in which multiple averages are possible, to varying extents. We should use a Student t-distribution with small degrees of freedom if we want to allow for the possibility of being very wrong (remember, it has thicker tails, so it assigns more probability to a wider range of average heights). We’re not super uncertain about people’s heights, though, so let’s use a normal distribution.\nAs we saw in defining our height model, a normal distribution requires that we set the \\(\\mu\\) and the \\(\\sigma\\). The \\(\\mu\\) we already covered (i.e., 170), so that leaves \\(\\sigma\\). Let’s set this to 10 and see what happens by visualizing this prior. Below I plot both the default brms prior and our own with \\(\\mu\\) = 170 and \\(\\sigma\\) = 10.\n\n\nCode\nheight_prior_intercept <- tibble(\n  height_mean = seq(from = 100, to = 250, by = 0.1),\n  ours = dnorm(height_mean, mean = 170, sd = 10),\n  default = dstudent_t(height_mean, df = 30, mu = 154.3, sigma = 8.5),\n)\n\nheight_prior_intercept <- pivot_longer(\n  height_prior_intercept, \n  cols = -height_mean, \n  names_to = \"prior\"\n) \n\nggplot(\n    height_prior_intercept, \n    aes(x = height_mean, y = value, linetype = fct_rev(prior))\n  ) +\n  geom_line() +\n  labs(x = \"Average height\", y = \"\", linetype = \"Prior\") +\n  scale_x_continuous(breaks = seq(100, 250, 20))\n\n\n\n\n\nTwo priors for \\(\\mu\\)\n\n\n\n\nOur prior indicates that we believe the average height to be higher than the default prior. In terms of the standard deviation, we both seem to be about equally uncertain about this average. To be fair, I think this prior of ours is not very plausible. Apparently we assign quite a chunk of plausibility to an average of 180 cm, or even 190 cm, which is very unlikely. An average of 160 cm is more plausible to me than an average of 180, so I should probably lower the mu, or use more of a skewed distribution. This is one of the benefits of visualizing the prior, it lets you think again about your prior so that you may improve on it. Regardless, we can keep the prior like this for now. We’ll see later that our data easily overshadows our prior.\n\n\nThe sigma prior (\\(\\sigma\\))\nWhat about the standard deviation? I find setting the standard deviation of the distribution of heights (not the mean of the heights) quite difficult. There are parts that are easy, such as the fact that the standard deviation has to be 0 or larger (it can’t be negative), but exactly how large it should be, I don’t know.\nI do know it is unlikely to be close to 0, and unlikely to be very large. That’s because I know people’s heights do vary, so I know the sigma can’t be 0. I also know it’s not super large because we don’t see people who are taller than 2 meters very often. This means the peak should be somewhere above 0, with a tail to allow higher values but not too high. We can use a normal distribution for this with a mean above 0 and a particular standard deviation, and ignore everything that’s smaller than 0 (brms automatically ignores negative values for \\(\\sigma\\)).\nAs I mentioned before, there is a downside of using a normal distribution, though. Normal distributions have long tails, but there is actually very little density in those tails. If we are quite uncertain about our belief about sigma, we should use a t-distribution, or perhaps even a cauchy distribution (actually, the cauchy distribution is a special case of the Student t-distribution; they are equivalent if the degree of freedom is 1). The lower the degrees of freedom, the more probability we assign to higher and lower values.\nSo, a t-distribution requires three parameters: \\(\\mu\\), \\(\\sigma\\), and the degrees of freedom. I set \\(\\mu\\) to 5, \\(\\sigma\\) to 5, and the degrees of freedom to 1. Below I plot this prior and brms’s default prior.\n\n\nCode\nheight_prior_sigma <- tibble(\n  height_sigma = seq(from = 0, to = 50, by = .1),\n  default = dstudent_t(height_sigma, df = 3, mu = 0, sigma = 8.5),\n  ours = dstudent_t(height_sigma, df = 1, mu = 5, sigma = 5) \n)\n\nheight_prior_sigma <- pivot_longer(\n  height_prior_sigma, \n  cols = -height_sigma, \n  names_to = \"prior\"\n)\n\nggplot(\n    height_prior_sigma, \n    aes(x = height_sigma, y = value, linetype = fct_rev(prior))\n  ) +\n  geom_line() +\n  labs(x = \"Standard deviation of heights\", y = \"\", linetype = \"Prior\")\n\n\n\n\n\nTwo priors for \\(\\sigma\\)\n\n\n\n\nAs you can see, both distributions have longish tails, allowing for the possibility of high standard deviations. There are some notable differences between the two priors, though. Our prior puts more weight on a standard deviation larger than 0, while the default prior reflects a belief in which a standard deviation of 0 is most likely. However, both priors are quite weak. We’ll see that the data easily overshadows these priors.\nBefore we run the analysis, we can also check the results of both our priors on the distribution of heights.\n\n\nA prior predictive check\nBefore we run our model, we should check what the effect is of both priors combined. Because we have set the priors we can simulate what we believe the data to be. This is one way to see whether our priors actually make sense. It is called a prior predictive check.\nWe can use brms to do this by running the brm() function. However, instead of running the actual model, we tell it to only sample from the prior.\n\n\nCode\nmodel_height_prior <- brm(\n  height ~ 1,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(normal(170, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\")\n    ), \n  cores = 4,\n  seed = 4, \n  sample_prior = \"only\",\n  file = \"models/model_height_prior.rds\"\n)\n\n\nWe then use the tidybayes package to draw samples from the prior and plot these draws.\n\n\nCode\npredictions_prior <- tibble(distribution = \"prior\")\n\npredictions_prior <- add_predicted_draws(\n  newdata = predictions_prior, \n  object = model_height_prior, \n  value = \"predicted_height\"\n)\n\nggplot(predictions_prior, aes(x = predicted_height)) +\n  geom_histogram(binwidth = 1, alpha = .85) +\n  xlim(100, 250) +\n  labs(x = \"Height\", y = \"\")\n\n\n\n\n\nPrior predictive check\n\n\n\n\nSo, our priors result in a normal distribution of heights ranging from about 125 cm to 225 cm. That is too wide, but let’s run the model to see what happens.\n\n\nRunning the model\nWe run the model with the code below. Notice that we sample from the prior so we can not only visualize our posterior later, but also the priors we have just defined.\n\n\nCode\nmodel_height <- brm(data = data, \n  family = gaussian,\n  height ~ 1,\n  prior = c(\n    prior(normal(170, 10), class = \"Intercept\"),\n    prior(cauchy(5, 5), class = \"sigma\")\n  ),\n  cores = 4,\n  seed = 4,\n  sample_prior = TRUE,\n  file = \"models/model_height.rds\"\n)\n\n\nAfter running the model, we first check whether the chains look good.\n\n\nCode\nplot(model_height)\n\n\n\n\n\nIt seems like they do. The distributions look normal and the chains look like caterpillars, which means they’re sampling from the distribution space and that’s what we want.\nWe can call up the estimates and the 95% confidence intervals by printing the model object.\n\n\nCode\nsummary(model_height)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.63      0.41   153.80   155.42 1.00     3674     2697\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.77      0.30     7.19     8.39 1.00     3720     2921\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we see the Intercept and sigma estimates. Apparently our posterior estimate for the Intercept is 154.63 and the estimate for \\(\\sigma\\) is 7.77. We also see the 95% CIs, but let’s visualize these results instead.\n\n\nComparing the prior and posterior distributions\nInspecting the chains also showed us the posterior distributions of the two parameters, but let’s create our own graphs that compare both the prior and posterior distributions.\n\n\nCode\nresults <- model_height %>%\n  gather_draws(b_Intercept, sigma, prior_Intercept, prior_sigma) %>%\n  mutate(\n    parameter = if_else(str_detect(.variable, \"sigma\"), \"sigma\", \"intercept\"),\n    distribution = if_else(str_detect(.variable, \"prior\"), \"prior\", \"posterior\")\n  )\n\nresults_intercept <- filter(results, parameter == \"intercept\")\nresults_sigma <- filter(results, parameter == \"sigma\")\n\nggplot(results_intercept, aes(x = .value, fill = fct_rev(distribution))) +\n  geom_histogram(binwidth = 1, position = \"identity\", alpha = .85) +\n  xlim(145, 195) +\n  labs(x = \"Average height\", y = \"\", fill = \"Distribution\") +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\n\n\n\nHere we see that the posterior distribution of average heights is now much more narrow and centered around 155 cm. So not only should we switch from thinking the average is a lot lower than 170, we can also be much more confident about the mean.\nHow about sigma?\n\n\nCode\nggplot(results_sigma, aes(x = .value, fill = fct_rev(distribution))) +\n  geom_histogram(binwidth = 0.25, position = \"identity\", alpha = .85) + \n  xlim(0, 25) +\n  labs(x = \"Height standard deviation\", y = \"\", fill = \"Distribution\") +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\n\n\n\nSimilarly, we see that the posterior for sigma is also much more narrow and around 8.\nA final step is to visualize the posterior distribution of all heights (posterior predictive check) and compare it the distribution of heights based on our priors (the prior predictive check).\n\n\nCode\npredictions_posterior <- tibble(distribution = \"posterior\")\n\npredictions_posterior <- add_predicted_draws(\n  newdata = predictions_posterior,\n  object = model_height, \n  value = \"predicted_height\"\n)\n\npredictions <- bind_rows(predictions_prior, predictions_posterior)\n\nggplot(predictions, aes(x = predicted_height, fill = distribution)) +\n  geom_histogram(binwidth = 1, alpha = .85, position = \"identity\") +\n  xlim(100, 250) +\n  labs(x = \"Height\", y = \"\", fill = \"Distribution\") +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\n\n\n\nPrior and posterior predictive check"
  },
  {
    "objectID": "content/posts/5-my-bayesian-workflow-and-tutorial/my-bayesian-workflow-and-tutorial.html#the-intercept-prior-mu",
    "href": "content/posts/5-my-bayesian-workflow-and-tutorial/my-bayesian-workflow-and-tutorial.html#the-intercept-prior-mu",
    "title": "Figuring out Bayesian statistics (Part 1)",
    "section": "The Intercept prior (\\(\\mu\\))",
    "text": "The Intercept prior (\\(\\mu\\))\nThe prior for the intercept indicates what we believe the average height of the !Kung San to be.\nbrms has set the default Intercept prior as a Student t-distribution with 3 degrees of freedom, a mean of 154.3 and a standard deviation of 8.5. That means brms starts off with a ‘belief’ that the average of the heights is roughly normally distributed, with the most common average height being 154.3, but with quite some uncertainty. In fact, a Student t-distribution has thicker tails compared to a normal distribution, which means more uncertainty, at least at low sample sizes. With these values you think the average height is most likely to be 154.3, but you also think that quite some other values are possible, even much smaller or much taller average heights.\nBut this is the default prior. brms determines this automatic prior by peeking at the data, which is not what we want to do. Instead, we should create our own.\nSo what do I believe the average height to be? As a Dutch person, I might be under the impression that the average height is around 175 centimeters. This is probably too tall to use as an average for the !Kung San because we’re known for being quite tall. So I think the average should be lower than 175, perhaps 170. I am not very sure, though. After all, I am far from an expert on people’s heights; I am only using my layman knowledge here. As a result, an average of 165 seems possible to me too. So let’s describe my belief in the form of a distribution in which multiple averages are possible, to varying extents. We could use different types of distributions for this purpose. We could use a Student t-distribution, but we can also use a normal distribution. We should use a Student t-distribution with small degrees of freedom if we want to allow for the possibility of being very wrong (remember, it has thicker tails, so it covers a wider range of average heights). We’re not super uncertain about people’s heights, though, so let’s use a normal distribution.\nAs we saw in defining our height model, a normal distribution requires that we set the \\(\\mu\\) and the \\(\\sigma\\). The \\(\\mu\\) we already covered (i.e., 170), so that leaves \\(\\sigma\\). Let’s set this to 10 and see what happens by visualizing this prior. Below I plot both the default brms prior and our own.\n\n\nCode\nheight_prior_intercept <- tibble(\n  height_mean = seq(from = 100, to = 250, by = 0.1),\n  ours = dnorm(height_mean, mean = 170, sd = 10),\n  default = dstudent_t(height_mean, df = 30, mu = 154.3, sigma = 8.5),\n)\n\nheight_prior_intercept <- pivot_longer(\n  height_prior_intercept, \n  cols = -height_mean, \n  names_to = \"prior\"\n) \n\nggplot(\n    height_prior_intercept, \n    aes(x = height_mean, y = value, linetype = fct_rev(prior))\n  ) +\n  geom_line() +\n  labs(x = \"Average height\", y = \"\", linetype = \"Prior\") +\n  scale_x_continuous(breaks = seq(100, 250, 20))\n\n\n\n\n\nTwo priors for \\(\\mu\\)\n\n\n\n\nOur prior indicates that we believe the average height to be higher than the default prior. In terms of the standard deviation, we both seem to be about equally uncertain about this average. To be fair, I think this prior of ours is not very plausible. Apparently we assign quite a chunk of plausibility to an average of 180 cm, or even 190 cm, which is very unlikely. An average of 160 cm is more plausible to me than an average of 180, so I should probably lower the mu, or use more of a skewed distribution. This is one of the benefits of visualizing the prior, it lets you think again about your prior so that you may improve on it. Regardless, we can keep the prior like this for now. We’ll see later that our data easily overshadows our prior.\n\nThe sigma prior\nWhat about the standard deviation? I find setting the standard deviation of the distribution of heights (not the mean of the heights) quite difficult. There are parts that are easy, such as the fact that the standard deviation has to be 0 or larger (it can’t be negative), but exactly how large it should be, I don’t know.\nI do know it is unlikely to be close to 0, and unlikely to be very large. That’s because I know people’s heights do vary, so I know the sigma can’t be 0. I also know it’s not super large because we don’t see people who are taller than 2 meters very often. This means the peak should be somewhere above 0, with a tail to allow higher values but not too high. We can use a normal distribution for this with a mean above 0 and a particular standard deviation, and ignore everything that’s smaller than 0.\nAs I mentioned before, there is a downside of using a normal distribution, though. Normal distributions have long tails, but there is actually very little density in those tails. If we are quite uncertain about our belief about sigma, we should have thicker tails to indicate we believe those values are more plausible. One way to do this is by using a Student t-distribution, perhaps even a cauchy distribution. Actually, the cauchy distribution is a special case of the Student t distribution; they are equivalent if the degree of freedom is 1. By increasing the degrees of freedom, you make the t-distribution more similar to a normal distribution.\nSo, a t-distribution requires three parameters: \\(\\mu\\), \\(\\sigma\\), and the degrees of freedom. I set \\(\\mu\\) to 5,\\(\\sigma\\) to 5, and the degrees of freedom to 1. Below I plot this prior and brms’s default prior.\n\n\nCode\nheight_prior_sigma <- tibble(\n  height_sigma = seq(from = 0, to = 50, by = .1),\n  default = dstudent_t(height_sigma, df = 3, mu = 0, sigma = 8.5) * 2,\n  ours = dstudent_t(height_sigma, df = 1, mu = 5, sigma = 5) * 2\n)\n\nheight_prior_sigma <- pivot_longer(\n  height_prior_sigma, \n  cols = -height_sigma, \n  names_to = \"prior\"\n)\n\nggplot(\n    height_prior_sigma, \n    aes(x = height_sigma, y = value, linetype = fct_rev(prior))\n  ) +\n  geom_line() +\n  labs(x = \"Standard deviation of heights\", y = \"\", linetype = \"Prior\")\n\n\n\n\n\nTwo priors for \\(\\sigma\\) - brms’ default and my own\n\n\n\n\nAs you can see, both distributions have longish tails, allowing for the possibility of high standard deviations. There are some notable differences between the two priors, though. Our prior puts more weight on a standard deviation larger than 0, while the default prior reflects a belief in which a standard deviation of 0 is most likely. However, both priors are quite weak. We’ll see that the data easily overshadows these priors.\nBefore we run the analysis, we can also check the results of both our priors on the distribution of heights.\n\n\nA prior predictive check\nBefore we run our model, we should check what the effect is of both priors combined. Because we have set the priors we can simulate what we believe the data to be. This is one way to see whether our priors actually make sense. It is called a prior predictive check.\nWe can use brms to do this by running the brm() function. However, instead of running the actual model, we tell it to only sample from the prior.\n\n\nCode\nmodel_height_prior <- brm(\n  height ~ 1,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(normal(170, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\")\n    ), \n  cores = 4,\n  seed = 4, \n  sample_prior = \"only\",\n  file = \"models/model_height_prior.rds\"\n)\n\ndata <- mutate(data, height_z = (height - mean(height)) / sd(height))\n\nmodel_height_prior <- brm(\n  height ~ 1,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(normal(170, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\")\n    ), \n  cores = 4,\n  seed = 4, \n  sample_prior = \"only\",\n  file = \"models/model_height_prior.rds\"\n)\n\n\nWe then use the tidybayes package to draw samples from the prior and plot these draws.\n\n\nCode\npredictions_prior <- tibble(distribution = \"prior\")\n\npredictions_prior <- add_predicted_draws(\n  newdata = predictions_prior, \n  object = model_height_prior, \n  value = \"predicted_height\"\n)\n\nggplot(predictions_prior, aes(x = predicted_height)) +\n  geom_histogram(binwidth = 1, alpha = .85) +\n  xlim(100, 250) +\n  labs(x = \"Height\", y = \"\")\n\n\nWarning: Removed 85 rows containing non-finite values (stat_bin).\n\n\nWarning: Removed 2 rows containing missing values (geom_bar).\n\n\n\n\n\nPrior predictive check\n\n\n\n\nSo, our priors result in a normal distribution of heights ranging from about 125 cm to 225 cm. That is probably a bit too wide, but let’s go with this. Let’s run the model for real now.\n\n\nCode\nmodel_height <- brm(data = data, \n  family = gaussian,\n  height ~ 1,\n  prior = c(\n    prior(normal(170, 10), class = \"Intercept\"),\n    prior(cauchy(5, 5), class = \"sigma\")\n  ),\n  cores = 4,\n  seed = 4,\n  sample_prior = TRUE,\n  file = \"models/model_height.rds\"\n)\n\n\nBefore we check the results, let’s first check whether the chains look good.\n\n\nCode\nplot(model_height)\n\n\n\n\n\nIt seems like they do. The distributions look normal and the chains look like caterpillars, which means they’re sampling from the distribution space and that’s what we want. This also shows us the posterior distributions of the two parameters, but let’s create our own graphs that compare both the prior and posterior distributions.\n\n\nCode\nresults <- model_height %>%\n  gather_draws(b_Intercept, sigma, prior_Intercept, prior_sigma) %>%\n  ungroup() %>%\n  mutate(\n    parameter = if_else(str_detect(.variable, \"sigma\"), \"sigma\", \"intercept\"),\n    distribution = if_else(str_detect(.variable, \"prior\"), \"prior\", \"posterior\")\n  )\n\nresults_intercept <- filter(results, parameter == \"intercept\")\nresults_sigma <- filter(results, parameter == \"sigma\")\n\nggplot(results_intercept, aes(x = .value, fill = fct_rev(distribution))) +\n  geom_histogram(binwidth = 1, position = \"identity\", alpha = .85) +\n  xlim(145, 195) +\n  labs(x = \"Average height\", y = \"\", fill = \"Distribution\") +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\nWarning: Removed 49 rows containing non-finite values (stat_bin).\n\n\nWarning: Removed 4 rows containing missing values (geom_bar).\n\n\n\n\n\nHere we see that the posterior distribution of average heights is now much more narrow and centered around 156 cm. So not only should we switch from thinking the average is a lot lower than 170, we can also be much more confident about the mean.\nHow about sigma?\n\n\nCode\nggplot(results_sigma, aes(x = .value, fill = fct_rev(distribution))) +\n  geom_histogram(binwidth = 0.25, position = \"identity\", alpha = .85) + \n  xlim(0, 25) +\n  labs(x = \"Height standard deviation\", y = \"\", fill = \"Distribution\") +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\nWarning: Removed 372 rows containing non-finite values (stat_bin).\n\n\nWarning: Removed 4 rows containing missing values (geom_bar).\n\n\n\n\n\nSimilarly, we see that the posterior for sigma is also much more narrow and around 7.75.\nWe can call up the estimates and the 95% confidence intervals by printing the model object.\n\n\nCode\nmodel_height\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.63      0.41   153.80   155.42 1.00     3674     2697\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.77      0.30     7.19     8.39 1.00     3720     2921\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we see the Intercept and sigma estimates, as well as their 95% CIs to summarize their posterior distribution.\nA final step is to visualize the posterior distribution of all heights (posterior predictive check) and compare it the distribution of heights based on our priors (the prior predictive check).\n\n\nCode\npredictions_posterior <- tibble(distribution = \"posterior\")\n\npredictions_posterior <- add_predicted_draws(\n  newdata = predictions_posterior,\n  object = model_height, \n  value = \"predicted_height\"\n)\n\npredictions <- bind_rows(predictions_prior, predictions_posterior)\n\nggplot(predictions, aes(x = predicted_height, fill = distribution)) +\n  geom_histogram(binwidth = 1, alpha = .85, position = \"identity\") +\n  xlim(100, 250) +\n  labs(x = \"Height\", y = \"\", fill = \"Distribution\") +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\nWarning: Removed 85 rows containing non-finite values (stat_bin).\n\n\nWarning: Removed 4 rows containing missing values (geom_bar).\n\n\n\n\n\nPrior and posterior predictive check\n\n\n\n\nBased on the data, we should believe, after seeing the data, that people’s heights are lower than expected and that there is less variation."
  },
  {
    "objectID": "content/posts/5-my-bayesian-workflow-and-tutorial/my-bayesian-workflow-and-tutorial.html#adding-a-predictor",
    "href": "content/posts/5-my-bayesian-workflow-and-tutorial/my-bayesian-workflow-and-tutorial.html#adding-a-predictor",
    "title": "Figuring out Bayesian statistics",
    "section": "Adding a predictor",
    "text": "Adding a predictor\nNow let’s add a predictor to our model. Besides heights, the data set also contains their weights. We can create a model in which we regress heights onto weights. The formula syntax for a model like that in R is height ~ weight. We can use this formula again in get_prior() to see which priors we need to specify.\n\n\nCode\nget_prior(height ~ weight, data = data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\nweight\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 154.3, 8.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 8.5)\nsigma\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\nThe output is a bit trickier this time. We see the Intercept and sigma priors from our previous model, as well as two extra rows referring to a class called b. These two rows actually refer to the same prior, one refers specifically to the weight predictor and one refers to all predictors. If you run a model with many more predictors, you could set one prior that applies to all predictors. In this case though, we only have 1 predictor so it actually doesn’t matter, both refer to the same prior.\nGiven that this is a bit trickier, and given that I said writing down your model explicitly is better, we should go ahead and do that.\n\\[\nheights_i ∼ Normal(\\mu_i, \\sigma)\\\\\n\\mu_i = \\alpha + \\beta x_i\n\\]\nWe again specify that the heights are normally distributed, so we still have a \\(\\mu\\) and \\(\\sigma\\), but this time the \\(\\mu\\) is no longer a parameter we will estimate. Instead, it’s constructed from other parameters, \\(\\alpha\\), \\(\\beta\\), and an observed variable \\(x_i\\) (the weight observations).\nIf you’re used to linear regression equations, this notation should not surprise you. \\(\\alpha\\) refers to the intercept and \\(\\beta\\) to the slope.\nWe need to set priors on these parameters. We previously already discussed the intercept prior so we could reuse that prior, although that means we need to center the data so the intercept refers to the average height of someone with an average weight rather than someone with 0 weight. So let’s first mean center the weight observations.\n\n\nCode\ndata <- mutate(data, weight_mc = weight - mean(weight))\n\n\nNow we can use the same prior as before, which was a normal distribution with a mean of 170 and a standard deviation of 10.\nNext is the prior for the slope. This represents the relationship between weights and heights. For every 1 increase in weight, how much do we think that the height will increase or decrease? We could begin with an agnostic prior in which we do not specify the direction and instead just add some uncertainty so the slope can go in either direction. For example, let’s put a normal distribution on the slope with a mean of 0 and a standard deviation of 10.\n\n\nCode\nmodel_height_weight_prior <- brm(\n  height ~ weight_mc,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(normal(170, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\"),\n      prior(normal(0, 10), class = \"b\")\n    ), \n  cores = 4,\n  seed = 4, \n  sample_prior = \"only\",\n  file = \"models/model_height_weight_prior.rds\"\n)\n\n\nWe can again create a prior predictive check to see whether our priors actually make sense. However, instead of plotting the predicted distribution of heights, we’re mostly interested in the relationship between weight and height, so we should plot a check of that relationship instead. This can be done by drawing sets of intercepts and slopes from the model results.\nBelow we draw intercepts and slopes from the model result and plot 100 of them. To help make sense of the sensibility of the slopes I’ve added the average weight to the weights so we’re back on the normal scale and not the mean centered scale and I’ve added two dashed lines to indicate the minimum and maximum height we can expect.\n\n\nCode\ndraws <- spread_draws(\n  model_height_weight_prior, b_Intercept, b_weight_mc\n)\n\nweight_mean <- data %>%\n  pull(weight) %>%\n  mean()\n\nggplot(data, aes(x = weight_mc, y = height)) +\n  geom_blank() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 272, linetype = \"dashed\") +\n  geom_abline(\n    data = filter(draws, .draw <= 100),\n    mapping = aes(intercept = b_Intercept, slope = b_weight_mc),\n    alpha = .25\n  ) +\n  geom_label(x = 15, y = 260, label = \"Tallest person ever\") +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n\n\n\n\n\nA prior predictive check of the relationship between weight and height\n\n\n\n\nThe plot shows a wide range of possible slopes, some of which are definitely unlikely. We should lower our uncertainty by reducing the standard deviation on the prior. In the next model I lower it to 3. Additionally, the negative slopes are all pretty unlikely because we should expect a positive relationship between weight and height (taller people tend to be heavier). We could therefore also change our prior to force it to be positive using the lb argument in our prior for b.\n\n\nCode\nmodel_height_weight_prior <- brm(\n  height ~ weight_mc,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(normal(170, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\"),\n      prior(normal(0, 3), class = \"b\", lb = 0)\n    ), \n  cores = 4,\n  seed = 4, \n  sample_prior = \"only\", \n  file = \"models/model_height_weight_prior_lb.rds\",\n  control = list(adapt_delta = 0.9)\n)\n\n\nWhen I first ran this model I received the warning that there was 1 divergent transition after warmup. The Rhat values did not show this was problematic but I wanted to get rid of the warning anyway so I increased the adapt_delta, as suggested in the documentation, from .8 to .9.\n\n\nCode\ndraws <- spread_draws(\n  model_height_weight_prior, b_Intercept, b_weight_mc\n)\n\nggplot(data, aes(x = weight_mc, y = height)) +\n  geom_blank() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 272, linetype = \"dashed\") +\n  geom_abline(\n    data = filter(draws, .draw <= 100),\n    mapping = aes(intercept = b_Intercept, slope = b_weight_mc),\n    alpha = .25\n  ) +\n  geom_label(x = -10, y = 260, label = \"Tallest person ever\") +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n\n\n\n\n\nA prior predictive check of the relationship between weight and height\n\n\n\n\nThis looks a lot better, so let’s run the model for real now.\n\n\nCode\nmodel_height_weight <- brm(data = data, \n  height ~ weight,\n  family = gaussian,\n  prior = c(\n      prior(normal(170, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\"),\n      prior(normal(0, 3), class = \"b\", lb = 0)\n    ), \n  cores = 4,\n  seed = 4,\n  sample_prior = TRUE,\n  file = \"models/model_height_weight.rds\",\n  control = list(adapt_delta = 0.9)\n)\n\nmodel_height_weight\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ weight \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   113.91      1.93   110.02   117.68 1.00     4029     2527\nweight        0.90      0.04     0.82     0.99 1.00     4054     2520\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.11      0.20     4.75     5.51 1.00     3814     2859\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe see that the estimate for the weight predictor is 0.9. Let’s plot the entire posterior for the estimate and also compare it to the prior we set for it.\n\n\nCode\nresults <- model_height_weight %>%\n  gather_draws(prior_b, b_weight) %>%\n  mutate(\n    distribution = if_else(\n      str_detect(.variable, \"prior\"), \"prior\", \"posterior\"\n    )\n  )\n\nggplot(results, aes(x = .value, fill = fct_rev(distribution))) +\n  geom_histogram(binwidth = 0.05, position = \"identity\", alpha = .85) +\n  xlim(0, 5) +\n  labs(x = \"Slope\", y = \"\", fill = \"Distribution\") +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\n\n\n\nApparently our prior was still very uninformed because the posterior shows we can be a confident in a much narrower range of slopes!\n\nThinking correlations instead\nMaybe one reason our prior was so uninformed was because it’s harder to think of the right prior for a content-specific topic such as weights and heights of the !Kung San. Maybe we can instead standardize both the heights and weights in order to turn the regression model into a simple correlation analysis. That way we can specify a prior on what we think the correlation should be, which may be easier to do because we then think in terms of whether we think the relationship is small or medium or large, or something along those lines.\nSo, let’s standardize the heights and weights.\n\n\nCode\ndata <- mutate(\n  data, \n  height_z = (height - mean(height)) / sd(height),\n  weight_z = (weight - mean(weight)) / sd(weight)\n)\n\n\nThe formula for our correlation analysis is height_z ~ weight_z. Which priors we have to specify remains the same, but what these priors should be did change. For instance, we know that the Intercept has to be 0 now because the heights have been standardized. This means the mean will be 0. In brms, we can specify a constant as a prior using constant().\nWhat should the prior for \\(\\sigma\\) be? Let’s keep that one the same for now and see what the posterior results will be.\nThe prior for the slope is a lot easier now. We can simply specify a normal distribution with a mean of 0 and a standard deviation equal to the size of the effect we deem likely, together with a lower bound of 0 and upper bound of 1.\n\n\nCode\nmodel_height_weight_z <- brm(\n  height_z ~ weight_z,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(constant(0), class = \"Intercept\"),\n      prior(cauchy(0, 2.5), class = \"sigma\"),\n      prior(normal(0, 1), class = \"b\", lb = 0, ub = 1)\n    ), \n  cores = 4,\n  seed = 4, \n  sample_prior = TRUE,\n  file = \"models/model_height_weight_prior_z.rds\"\n)\n\nmodel_height_weight_z\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height_z ~ weight_z \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.00    -0.00    -0.00 1.00     3371       NA\nweight_z      0.75      0.03     0.68     0.82 1.00     3371     2588\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.66      0.03     0.61     0.71 1.00     3024     2617\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  }
]