[
  {
    "objectID": "content/blog.html",
    "href": "content/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nBayesian tutorial: Two groups\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\n\nThe fourth of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to model the difference between two groups.\n\n\n\n\n\n\nApr 24, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBayesian tutorial: Correlation\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\n\nThe third of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to model a correlation.\n\n\n\n\n\n\nFeb 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBayesian tutorial: Single predictor regression\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\n\nThe second of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to run a regression with a single predictor.\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBayesian tutorial: Intercept-only model\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\n\nThe first of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to run an intercept-only regression model.\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nVoting behavior of Dutch political parties on animal welfare motions\n\n\n\n\n\n\n\nanimal welfare\n\n\ndata cleaning\n\n\nAPIs\n\n\npolitics\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\nAnimals slaughtered in the Netherlands\n\n\n\n\n\n\n\nanimal welfare\n\n\ndata cleaning\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSimulation-based power curves\n\n\n\n\n\n\n\npower analysis\n\n\nsimulation\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\nSimulation-based power analyses\n\n\n\n\n\n\n\nstatistics\n\n\npower analysis\n\n\nsimulation\n\n\n\n\nSimulation-based power analyses make it easy to understand what power is: Power is simply counting how often you find the results you expect to find. Running simulation-based power analyses might be new for some, so in this blog post I present code to simulate data for a range of different scenarios.\n\n\n\n\n\n\nOct 23, 2021\n\n\n\n\n\n\n  \n\n\n\n\nThe right order of method sections\n\n\n\n\n\n\n\nwriting\n\n\nstatistics\n\n\npower analysis\n\n\n\n\nMethod sections in academic (psychology) papers usually consist of the following sections: Participants, Design, Procedure, and Materials. They also tend to be presented in this order. But is this, generally speaking, the right order? I don’t think so.\n\n\n\n\n\n\nJul 8, 2021\n\n\n\n\n\n\n  \n\n\n\n\nA tidystats example\n\n\n\n\n\n\n\ntidystats\n\n\nstatistics\n\n\ntutorial\n\n\n\n\nI illustrate how to use my tidystats software to analyze and report the results of a replication study that was part of the Many Labs 1 project.\n\n\n\n\n\n\nApr 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\nWhy divide by \\(n - 1\\) to calculate the variance of a sample?“\n\n\n\n\n\n\n\nstatistics\n\n\n\n\nIn a recent tweet I asked the question why we use \\(n - 1\\) to calculate the variance of a sample. Many people contributed an answer, but many of them were of the type I feared. Most consisted of some statistical jargon that confuses me more, rather than less. Other responses were very useful, though, so I recommend checking out the replies to the tweet. In this post, I will try to describe my favorite way of looking at the issue.\n\n\n\n\n\n\nAug 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\nUseful power analysis papers\n\n\n\n\n\n\n\nstatistics\n\n\npower analysis\n\n\n\n\nA curious thing happened in the field of social psychology: Social psychologists finally realized that statistical power is important. Unfortunately, they then skipped the step of figuring out how to do them correctly. Here I list some papers on power analyses that I hope help in improving the way we do them.\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding regression (part 1)\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nregression\n\n\n\n\nThis is Part 1 of a series of blog posts on how to understand regression. The goal is to develop an intuitive understanding of the different components of regression. In this first post, we figure out where the estimate of an intercept-only regression model comes from.\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "About",
    "section": "",
    "text": "I’m a Senior Behavioral Scientist at Rethink Priorities. Rethink Priorities is a research organization that houses a bunch of cool people who support and conduct research to inform policymakers and major foundations about how to best help people and nonhuman animals, in both the present and the long-term future. I am part of the survey team, which means I conduct research on attitude assessments and attitude change, using surveys and experimental studies.\nBefore joining Rethink Priorities, I was an assistant professor in the Department of Social Psychology at Tilburg University.\nOn this website you can find information about some of the projects I’m involved in. Some notable projects I’m working on are tidystats and a large-scaled replication study of cognitive dissonance. Besides writing about these projects, I also blog posts about various topics, including tutorials or opinion pieces.\nOne of my main research interests concern animal welfare. I think animal welfare, and their lack thereof, is one of the most pressing issues in the world at this moment and as a fruitful area of research where influential theories in social psychology (such as cognitive dissonance) can be applied and tested.\nI’m also interested in the methodology of psychological research and ways to improve how we conduct science. A notable project I’m working on is tidystats. This is a software solution to help researchers more easily and more reproducibly report statistics in scientific manuscripts. It’s main goal is to get researchers to report more statistics with fewer errors. I’m pretty proud of this project, so please check it out on the tidystats project or the tidystats website.\nI also have teaching experience thanks to my time as an assistant professor. I’m quite experienced in teaching undergraduate courses, in both small and large groups of students. Besides course work I have also provided many R workshops (although I have less time for that now).\nThis website is created using Quarto."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html",
    "title": "Cognitive dissonance",
    "section": "",
    "text": "Cognitive dissonance refers to a state of aversive arousal that is experienced when people realize they possess mutually inconsistent cognitions. This state is the foundation of cognitive dissonance theory (CDT)—a theory developed by Leon Festinger in 1957. Several of my projects are aimed at assessing the evidence for this theory and at applying this theory to other issues (although now that I’ve left academia, these projects have been deprioritized)."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#why-is-this-important",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#why-is-this-important",
    "title": "Cognitive dissonance",
    "section": "Why is this important?",
    "text": "Why is this important?\nThe theory of cognitive dissonance can explain many different phenomena that we should understand so that we may intervene and improve the lives of others. For example, cognitive dissonance theory has been used to explain religious beliefs, unhealthy behaviors, and people’s attitude towards animals.\nBefore the theory can be applied, however, it needs to be verified. We need to have sufficient evidence to believe in the theory. The social psychological evidence we have for the theory is, however, quite weak. The research stems from old research, mostly conducted in the 50s, 60s, and 70s. While this would not necessarily be a problem, it is a problem in the case of social psychology. The original studies were conducted with extremely low sample sizes and without pre-registration, or other tools that limit p-hacking. This means that many past findings may be false positives, which is supported by recent findings that show many findings in psychology do not replicate."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#what-am-i-working-on",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#what-am-i-working-on",
    "title": "Cognitive dissonance",
    "section": "What am I working on?",
    "text": "What am I working on?\nI am one of the lead investigators of a large-scaled replication project. In this project, we will try and replicate a seminal finding in the cognitive dissonance literature. Specifically, our aim is to replicate the classic finding that people who write a counterattitudinal essay (e.g., students arguing in favor of a tuition increase) become more in favor of the position they argued for. We have submitted this project as a registered report to Advanced in Methods and Practices in Psychological Science (AMPPS). There it has received an in-principle acceptance. Data collection is currently underway.\nI also hope to start up a meta-analysis project to produce live reviews of studies from the cognitive dissonance literature."
  },
  {
    "objectID": "content/projects/cognitive-dissonance/cognitive-dissonance.html#links",
    "href": "content/projects/cognitive-dissonance/cognitive-dissonance.html#links",
    "title": "Cognitive dissonance",
    "section": "Links",
    "text": "Links\n\nThe landing page of our large-scaled replication project\nThe stage-1 accepted manuscript"
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html",
    "href": "content/projects/statcheck/statcheck.html",
    "title": "statcheck",
    "section": "",
    "text": "Together with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer statistics-related typos."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#why-is-this-important",
    "href": "content/projects/statcheck/statcheck.html#why-is-this-important",
    "title": "statcheck",
    "section": "Why is this important?",
    "text": "Why is this important?\nSimilar to my tidystats project, our aim is to address a particular problem in statistics reporting: the reporting of incorrect statistics.\nAs has been shown by Michèle and her colleagues, statistics are often reported incorrectly (Nuijten et al., 2016). This is likely due to the fact that researchers do not have the necessary software tools to reliably take the output of statistics from their data analysis software and enter it into their text editor. Instead, researchers are likely to copy statistics from the output by hand or by copy-pasting the output. Both techniques are error-prone, resulting in many papers containing statistical typos. This is a problem because statistical output is used in meta-analyses to aggregate the evidence for particular theories, which sometimes also inform policy. In some cases, the errors may even be so large that it affects the conclusion drawn from the statistical test."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#what-am-i-working-on",
    "href": "content/projects/statcheck/statcheck.html#what-am-i-working-on",
    "title": "statcheck",
    "section": "What am I working on?",
    "text": "What am I working on?\nAdmittedly, I am simply joining Michèle and her efforts to help researchers make fewer typos. She and her colleagues have already done a lot of the work—we’re now just trying to make it even better. For example, Sacha Epskamp and Michèle developed statcheck. statcheck is an R package designed to catch statistical reporting mistakes. It works by first extracting statistics from a paper (e.g., t values, degrees of freedom, p-values). It then uses the test statistic and degrees of freedom to re-calculate the p-value and compare it to the reported p-value. If the two don’t match, there is probably a reporting mistake.\nYou can use the statcheck package in R to check your paper or you can use the web app. Using the web app consists of simply uploading your paper and checking the results. You can then go back to the paper and correct the mistakes.\nWith my experience creating tidystats, and particularly the tidystats Word add-in, we’ve started to create a Word add-in for statcheck. This add-in allows researchers to scan their document for statistical inconsistencies, find them, and fix them. This add-in is currently in beta and we hope to release it soon.\nWe are also working on improving statcheck together with the eScience Center. Together with their help we hope to expand statcheck so it can catch a greater variety of statistical inconsistencies. We have had some preparatory meetings with them and plan to fully begin this project soon."
  },
  {
    "objectID": "content/projects/statcheck/statcheck.html#links",
    "href": "content/projects/statcheck/statcheck.html#links",
    "title": "statcheck",
    "section": "Links",
    "text": "Links\n\nThe web app\nThe R package on CRAN\nThe GitHub page of statcheck\nThe GitHub page of the statcheck Word add-in."
  },
  {
    "objectID": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html",
    "href": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html",
    "title": "Bayesian tutorial: Single predictor regression",
    "section": "",
    "text": "In my previous blog post I showed how to use brms and tidybayes to run an intercept-only model. Now let’s extend that model by adding a predictor.\nThe data is the same as in the previous post (including the filter that we only focus on people 18 years or older). This data contains weight data as well as height data, so that means we can run a model in which we regress heights onto weights, i.e., a regression with a single predictor.\nIf you want to follow along, run the following setup code.\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\n\ntheme_set(theme_minimal())\nblue_1 &lt;- \"#d1e1ec\"\nblue_2 &lt;- \"#b3cde0\"\nblue_3 &lt;- \"#6497b1\"\nblue_4 &lt;- \"#005b96\"\nblue_5 &lt;- \"#03396c\"\nblue_6 &lt;- \"#011f4b\"\n\noptions(\n  mc.cores = 4,\n  brms.threads = 4,\n  brms.backend = \"cmdstanr\",\n  brms.file_refit = \"on_change\"\n)\n\ndata &lt;- read_csv(\"Howell1.csv\")\ndata &lt;- filter(data, age &gt;= 18)"
  },
  {
    "objectID": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#adding-a-single-predictor",
    "href": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#adding-a-single-predictor",
    "title": "Bayesian tutorial: Single predictor regression",
    "section": "Adding a single predictor",
    "text": "Adding a single predictor\nThe formula syntax for a model in which we regress heights onto weights is height ~ weight. We can use this formula in get_prior() to see which priors we need to specify.\n\n\nCode\nget_prior(height ~ weight, data = data)\n\n\n                    prior     class   coef group resp dpar nlpar lb ub\n                   (flat)         b                                   \n                   (flat)         b weight                            \n student_t(3, 154.3, 8.5) Intercept                                   \n     student_t(3, 0, 8.5)     sigma                               0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nThe output is a bit trickier compared to the intercept-only model output. There’s the Intercept and sigma priors again, as well as two extra rows referring to a class called b. These two rows actually refer to the same prior, one refers specifically to the weight predictor and one refers to all predictors. If you run a model with many more predictors, you could set one prior that applies to all predictors. In this case though, we only have 1 predictor so it actually doesn’t matter, both refer to the same prior.\nRecall from the previous post that I said writing down your model explicitly is a better way to understand what you’re doing, so let’s go ahead and do that.\n\\[\\displaylines{heights_i ∼ Normal(\\mu_i, \\sigma) \\\\ \\mu_i = \\alpha + \\beta x_i}\\]\nWe again specify that the heights are normally distributed, so we still have a \\(\\mu\\) and \\(\\sigma\\), but this time the \\(\\mu\\) is no longer a parameter to estimate. Instead, it’s constructed from other parameters, \\(\\alpha\\), \\(\\beta\\), and an observed variable \\(x_i\\) (the weight observations).\nIf you’re used to linear regression equations, this notation should not surprise you. \\(\\alpha\\) refers to the intercept and \\(\\beta\\) to the slope.\nWe need to set priors on these parameters. The prior for \\(\\alpha\\) can be the same as the prior for \\(\\mu\\) from the previous intercept-only model if we center the data so the intercept refers to the average height of someone with an average weight, rather than someone with 0 weight (the default, which makes no sense). So let’s first mean center the weight observations.\n\n\nCode\ndata &lt;- mutate(data, weight_mc = weight - mean(weight))\n\n\nNow we can use the same prior as before, which was a normal distribution with a mean of 160 and a standard deviation of 10 (assuming we did not update this as a result of the previous analysis).\nNext is the prior for the slope. This represents the relationship between weights and heights. For every 1 increase in weight, how much do we think that the height will increase or decrease? We could begin with an agnostic prior in which we do not specify the direction and instead just add some uncertainty so the slope can go in either direction. For example, let’s put a normal distribution on the slope with a mean of 0 and a standard deviation of 10.\nFinally, we have the prior for sigma (\\(\\sigma\\)). To remind you, sigma refers to the standard deviation of the errors or the residual standard deviation. Now that we have a predictor that means the sigma can be less than what it was in the intercept-only model because some of the variance in heights might be explained by the weights, decreasing the size of the residuals and therefore sigma. So, if we believe in a relationship between heights and weights, we should change our prior for sigma so that it’s lower. Given that we used a prior for the slope that is agnostic (there could be a positive, negative, or no relationship), our prior for sigma could be left unchanged because it was broad enough to allow for these possibilities."
  },
  {
    "objectID": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#prior-predictive-check",
    "href": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#prior-predictive-check",
    "title": "Bayesian tutorial: Single predictor regression",
    "section": "Prior predictive check",
    "text": "Prior predictive check\nWe can again create a prior predictive check to see whether our priors actually make sense. However, instead of plotting the predicted distribution of heights, we’re mostly interested in the relationship between weight and height, so we should plot a check of that relationship instead. We could simulate our own data like I did in the previous post or we can just run the Bayesian model and only draw from the prior, which I also did in the previous post and will do so again here.\n\n\nCode\nmodel_height_weight_prior &lt;- brm(\n  height ~ weight_mc,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(normal(160, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\"),\n      prior(normal(0, 10), class = \"b\")\n    ), \n  sample_prior = \"only\",\n  seed = 4,\n  file = \"models/model_height_weight_prior.rds\"\n)\n\n\nWe can use the spread_draws() function to get draws from the posterior distribution of the intercept and slope parameters. With an intercept and slope we can visualize the relationship we’re interested in. Remember, though, that brms will give you 4000 draws by default from the posteriors. In other words, you get 4000 intercepts and slopes. That’s a bit much to visualize, so let’s only draw 100 intercepts and slopes.\nTo help make sense of the sensibility of the slopes I’ve added the average weight to the weights so we’re back on the normal scale and not the mean centered scale and I’ve added two dashed lines to indicate a very obvious minimum and a possible maximum height to help figure out whether the priors produce sensible results.\n\n\nCode\ndraws &lt;- spread_draws(\n  model_height_weight_prior, b_Intercept, b_weight_mc,\n  ndraws = 100\n)\n\nweight_mean &lt;- data %&gt;%\n  pull(weight) %&gt;%\n  mean()\n\nggplot(data, aes(x = weight_mc, y = height)) +\n  geom_blank() +\n  geom_abline(\n    data = draws,\n    mapping = aes(intercept = b_Intercept, slope = b_weight_mc),\n    alpha = .25\n  ) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 272, linetype = \"dashed\") +\n  geom_label(x = 0, y = 260, label = \"Tallest person ever\") +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n\n\n\n\n\nA prior predictive check of the relationship between weight and height\n\n\n\n\nThe plot shows a wide range of possible slopes, some of which are definitely unlikely because they lead to heights that are smaller than 0 or higher than the tallest person who ever lived. We should lower our uncertainty by reducing the standard deviation on the prior. In the next model I lower it to 3.\nAdditionally, the negative slopes are actually also pretty unlikely because we should expect a positive relationship between weight and height (taller people tend to be heavier). We could therefore also change our prior to force it to be positive using the lb argument in our prior for b or use a distribution that doesn’t allow for any negative values. Let’s not do this though. Let’s assume we have no idea whether the relationship will be positive or negative and instead focus on the standard deviation instead so that we don’t obtain relationships we definitely know are unlikely.\n\n\nCode\nmodel_height_weight_prior_2 &lt;- brm(\n  height ~ weight_mc,  \n  data = data, \n  family = gaussian,\n  prior = c(\n      prior(normal(160, 10), class = \"Intercept\"),\n      prior(cauchy(5, 5), class = \"sigma\"),\n      prior(normal(0, 3), class = \"b\")\n    ), \n  sample_prior = \"only\",\n  seed = 4,\n  file = \"models/model_height_weight_prior_2.rds\"\n)\n\n\nLet’s inspect the lines again.\n\n\nCode\ndraws &lt;- spread_draws(\n  model_height_weight_prior_2, b_Intercept, b_weight_mc,\n  ndraws = 100\n)\n\nggplot(data, aes(x = weight_mc, y = height)) +\n  geom_blank() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 272, linetype = \"dashed\") +\n  geom_abline(\n    data = draws,\n    mapping = aes(intercept = b_Intercept, slope = b_weight_mc),\n    alpha = .25\n  ) +\n  geom_label(x = 0, y = 260, label = \"Tallest person ever\") +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n\n\n\n\n\nA prior predictive check of the relationship between weight and height\n\n\n\n\nThis looks a lot better, so let’s run the model for real now.\n\n\nCode\nmodel_height_weight &lt;- brm(\n  data = data,\n  height ~ weight_mc,\n  family = gaussian,\n  prior = c(\n    prior(normal(160, 10), class = \"Intercept\"),\n    prior(cauchy(5, 5), class = \"sigma\"),\n    prior(normal(0, 3), class = \"b\")\n  ),\n  sample_prior = TRUE,\n  seed = 4,\n  file = \"models/model_height_weight.rds\"\n)\n\nmodel_height_weight\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ weight_mc \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.60      0.28   154.07   155.14 1.00     3979     2809\nweight_mc     0.90      0.04     0.82     0.99 1.00     4563     3129\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.11      0.19     4.74     5.51 1.00     4362     2791\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe see that the estimate for the weight predictor is 0.9. For every increase of weight by 1 we can expect height to increase by this number. We can also be fairly confident in this kind of relationship because the lower and upper bound of the 95% CI ranges from 0.82 to 0.99. These numbers are what we are usually interested in, but let’s also plot the the entire posterior for the slope estimate so can see the entire distribution and not just this summary. Let’s also add the prior so we can see how much that changed as a result of the data. This time we use gather_draws() to create a long data frame, instead of a wide data frame that you get with spread_draws().\n\n\nCode\ndraws &lt;- model_height_weight %&gt;%\n  gather_draws(prior_b, b_weight_mc) %&gt;%\n  mutate(\n    distribution = if_else(\n      str_detect(.variable, \"prior\"), \"prior\", \"posterior\"\n    )\n  )\n\nggplot(draws, aes(x = .value, fill = fct_rev(distribution))) +\n  geom_histogram(binwidth = 0.05, position = \"identity\") +\n  labs(x = \"Slope\", y = \"\", fill = \"Distribution\") +\n  scale_fill_manual(values = c(blue_2, blue_4)) +\n  coord_cartesian(xlim = c(-5, 5))\n\n\n\n\n\nApparently our prior was still very uninformed because the posterior shows we can be confident in a much narrower range of slopes!\nLet’s also create another plot in which we plot the slope and its posterior against the observed data. The way to do this is by first creating a data frame containing weights that we want to predict the heights for. The (mean-centered) weights in the data range from -13.92 to 18, so we can roughly use that same range.\nThen we use add_epred_draws() to predict the expected height for each of the weights we stored in the data frame. This is not a single value. Instead, we get a distribution of possible heights for each weight value. We could plot all of these distributions, for example by creating a shaded region at each weight representing how likely the height is, or we can summarize that distribution of heights for each weight. The tidybayes package has the median_qi() function to summarize a distribution to a point and interval. By default it uses the median for the point summary and a 5% and 95% quartile range for the interval; the same summary we saw in the output from brm.\n\n\nCode\nslopes_qi &lt;- tibble(\n  weight_mc = seq(from = -15, to = 20, by = 1)\n) %&gt;%\n  add_epred_draws(model_height_weight) %&gt;%\n  median_qi()\n\nggplot() +\n  geom_ribbon(\n    mapping = aes(ymin = .lower, ymax = .upper, x = weight_mc),\n    data = slopes_qi,\n    alpha = .25\n  ) +\n  geom_line(\n    mapping = aes(x = weight_mc, y = .epred),\n    data = slopes_qi\n  ) +\n  geom_point(\n    mapping = aes(x = weight_mc, y = height),\n    data = data,\n    alpha = .25\n  ) +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n\n\n\n\n\nThis graph is great because it shows us how confident we can be in the regression line. It does omit one source of uncertainty, though. The previous plot only shows the uncertainty about the regression line (the intercept and slope). We can also make a plot with predicted values of individual heights, which also incorporates the uncertainty from the \\(\\sigma\\) parameter. To get these values, we use add_predicted_draws().\n\n\nCode\npredicted_slopes_qi &lt;- tibble(\n  weight_mc = seq(from = -20, to = 20, by = 1)\n) %&gt;%\n  add_predicted_draws(model_height_weight) %&gt;%\n  median_qi()\n\nggplot() +\n  geom_ribbon(\n    aes(ymin = .lower, ymax = .upper, x = weight_mc),\n    data = predicted_slopes_qi,\n    alpha = .25\n  ) +\n  geom_line(\n    aes(x = weight_mc, y = .prediction),\n    data = predicted_slopes_qi\n  ) +\n  geom_point(\n    aes(x = weight_mc, y = height),\n    data = data,\n    alpha = .25\n  ) +\n  labs(x = \"Weight\", y = \"Height\") +\n  scale_x_continuous(labels = function(x) round(x + weight_mean))\n\n\n\n\n\nWhile this graph is pretty cool, I haven’t ever seen one in a social psychology paper, probably because academic psychologists are mostly interested in the parameters (e.g., means, correlations) rather than predicting individual observations."
  },
  {
    "objectID": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#summary",
    "href": "content/posts/18-bayesian-tutorial-simple-regression/bayesian-tutorial-simple-regression.html#summary",
    "title": "Bayesian tutorial: Single predictor regression",
    "section": "Summary",
    "text": "Summary\nIn this post I showed how to run a single predictor model in brms. The addition of a predictor meant that the previous intercept-only model had to be updated by turning the \\(\\mu\\) parameter into a regression equation. This then required an additional prior for the slope. To help set a prior on the slope, I created a prior predictive check of the slope. Running the model itself was straightforward and I provided several visualizations to help understand the results, including visualizing the posteriors of the slope parameter, the slope across the range of weights, and individual predicted heights.\nIn the next post I’ll show how to use brms to analyze correlations.\nThis post was last updated on 2023-08-07."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html",
    "title": "Understanding regression (part 1)",
    "section": "",
    "text": "Statistical regression techniques are an important tool in data analysis. As a social scientist, I use it to test hypotheses by comparing differences between groups or testing relationships between variables. While it is easy to run regression analyses in a variety of software packages, like SPSS or R, it often remains a black box that is not well understood. I, in fact, do not believe I actually understand regression. Not fully understanding the mechanics of regression could be okay, though. After all, you also don’t need to know exactly how car engines work in order to drive a car. However, I think many users of regression have isolated themselves too much from the mechanics of regression. This may be the source of some errors, such as applying regression to data that is not suitable for the regression method. If you’re using regression to try and make inferences about the world, it’s probably a good idea to feel like you know what you’re doing.\nSo, there are some reasons to figure out regression. This post is Part 1 of a series of blog posts called ‘Understanding Regression’ in which I try to figure it out.\nFeel free to follow me along by copy-pasting the code from each step."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#setup",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#setup",
    "title": "Understanding regression (part 1)",
    "section": "Setup",
    "text": "Setup\nTo figure out regression, we need data. We could make up some data on the spot, but I’d rather use data that is a bit more meaningful (to me, anyway). Since I’m a big Pokémon fan, I’ll use a data set containing Pokémon statistics.\nIn case you’re following along, start by loading some packages and reading in the data. In the code section below I use the here package to read in the data, but I recommend that you simply specify the path to the file. After that, I subset the data to make the data a bit more manageable and define a custom mode function because R does not have one (and I need it later). Finally, I set some options such as the default ggplot() theme and printing options.\n\n\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(here)\nlibrary(knitr)\n\n# Read in Pokémon data\npokemon &lt;- read_csv(here(\"data\", \"pokemon.csv\"))\n\n# Create a subset with only the first 25 Pokémon\npokemon25 &lt;- filter(pokemon, pokedex &lt;= 25)\n\n# Load a custom function to calculate the mode\nmode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n# Set some options\noptions(\n  knitr.kable.NA = \"-\",\n  digits = 2\n)\n\n\nLet’s take a look at several attributes of some Pokémon to see what they’re about:\n\n\nCode\npokemon25 %&gt;%\n  filter(pokedex &lt;= 10) %&gt;%\n  select(name, type_primary, type_secondary, height, weight, \n    evolution) %&gt;%\n  kable(\n    digits = 2, \n    col.names = c(\"Name\", \"Type (primary)\", \"Type (secondary)\", \"Height\", \n      \"Weight\", \"Evolution stage\")\n  )\n\n\n\n\nTable 1: The first 10 Pokémon\n\n\n\n\n\n\n\n\n\n\nName\nType (primary)\nType (secondary)\nHeight\nWeight\nEvolution stage\n\n\n\n\nBulbasaur\nGrass\nPoison\n0.7\n6.9\n0\n\n\nIvysaur\nGrass\nPoison\n1.0\n13.0\n1\n\n\nVenusaur\nGrass\nPoison\n2.0\n100.0\n2\n\n\nCharmander\nFire\n-\n0.6\n8.5\n0\n\n\nCharmeleon\nFire\n-\n1.1\n19.0\n1\n\n\nCharizard\nFire\nFlying\n1.7\n90.5\n2\n\n\nSquirtle\nWater\n-\n0.5\n9.0\n0\n\n\nWartortle\nWater\n-\n1.0\n22.5\n1\n\n\nBlastoise\nWater\n-\n1.6\n85.5\n2\n\n\nCaterpie\nBug\n-\n0.3\n2.9\n0\n\n\n\n\n\n\nPokémon have different types (e.g., grass, fire, water), a height, some weight, and they are of a particular evolutionary stage (0, 1, or 2). This last variable refers to a Pokémon’s ability to evolve and when they do, they tend to become bigger and more powerful.\nLet’s say that we are interested in understanding the weight of different Pokémon. Below I have plotted the weight of the first 25 Pokémon, from Bulbasaur to Pikachu.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .85) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nFigure 1: Weights of the first 25 Pokémon\n\n\n\n\nWe see that the lightest Pokémon is Pidgey, with a weight of 1.8 kg. The heaviest Pokémon is Venusaur, with a weight of 100 kg. The average weight is 26.14 kg."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-simplest-model",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-simplest-model",
    "title": "Understanding regression (part 1)",
    "section": "The simplest model",
    "text": "The simplest model\nIn order to understand the weights of different Pokémon, we need to come up with a statistical model. In a way, this can be considered a description problem. How can we best describe the different weights that we have observed? The simplest description is a single number. We can say that all Pokémon have a weight of say… 6 kg. In other words:\n\nweight = 6 kg\n\nOf course, this is just one among many possible models. Below I plot three different models, including our weight = 6 kg model.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .85) +\n  geom_abline(intercept = 6, slope = 0, linetype = 2) +\n  geom_abline(intercept = 40, slope = 0, linetype = 2) +\n  geom_abline(intercept = 75, slope = 0, linetype = 2) +\n  annotate(\"text\", x = 28, y = 6.5, label = \"weight = 6 kg\", size = 3.5) +\n  annotate(\"text\", x = 28, y = 40.5, label = \"weight = 40 kg\", size = 3.5) +\n  annotate(\"text\", x = 28, y = 75.5, label = \"weight = 75 kg\", size = 3.5) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  coord_cartesian(xlim = c(1, 25), clip = \"off\") +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.margin = unit(c(1, 6, 1, 1), \"lines\")\n  )\n\n\n\n\n\nFigure 2: Three different weight models\n\n\n\n\nWhile a model like weight = 6 kg is a valid model, it is not a very good model. In fact, it only perfectly describes Pikachu’s weight and inaccurately describes the weight of the remaining 24 Pokémon. The other models, such as weight = 40 kg might be even worse; they do not even describe a single Pokémon’s weight correctly, although they do get closer to some of the heavier Pokémon. How do we decide which model is the better model? In order to answer that question, we need to consider the model’s error."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-error-of-models",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-error-of-models",
    "title": "Understanding regression (part 1)",
    "section": "The error of models",
    "text": "The error of models\nThe error of a model is the degree to which the model inaccurately describes the data. There are several ways to calculate that error, depending on how you define error. We will cover three of them.\nThe first definition of error is simply the sum of times that the model inaccurately describes the data. For each observation we check whether the model correctly describes it or not. We then sum the number of misses and consider that the amount of error for that model. With our weight = 6 kg the answer is 24; out of the 25 Pokémon only Pikachu has a weight of 6, which means the model is correct once and wrong 24 times.\nWe can now compare different models to one another by calculating the error for a range of models. Below I plot the number of errors for 100 different models, starting with the model weight = 1 kg, up to weight = 10 kg, in steps of 0.1. Ideally we would test more models (up to the heaviest Pokémon we know of), but for the sake of visualizing the result, I decided to only plot a small subset of models.\n\n\nCode\nerrors_binary &lt;- expand_grid(\n    model = seq(from = 1, to = 10, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %&gt;%\n  mutate(error = if_else(abs(weight - model) == 0, 0, 1)) %&gt;%\n  group_by(model) %&gt;%\n  summarize(error_sum = sum(error))\n\nggplot(errors_binary, aes(x = model, y = error_sum)) +\n  geom_line() + \n  coord_cartesian(ylim = c(0, 25)) +\n  scale_x_continuous(breaks = 1:10) +\n  labs(x = \"Model (weight = x kg)\", y = \"Error (sum of errors)\")\n\n\n\n\n\nFigure 3: Error #1: The sum of (binary) errors\n\n\n\n\nWe see that almost all models perform poorly. The errors range from 23 to 25. Most models seem to have an error of 25, which means they do not accurately describe any of the 25 Pokémon. Some have an error of 24, meaning they describe the weight of 1 Pokémon correctly. There is 1 model with an error of 23: weight = 6.9 kg. Apparently there are 2 Pokémon with a weight of 6.9, which means that this model outperforms the others.\nDespite there being a single model that outperforms the others in this set of models, it’s still a pretty poor model. After all, it is wrong 23 out of 25 times. Perhaps there are some models that outperform this model, but it’s unlikely. That’s because we’re defining error here in a very crude way. The model needs to exactly match the weight of the Pokémon, or else it counts as an error. Saying a weight is 6 kg, while it is in fact 10 kg, is as wrong as saying the weight is 60 kg.\nInstead of defining error in this way, we can redefine it so that it takes into account the degree of error. We can define error as the difference between the actual data point and the model’s value. So, in the case of our weight = 6 kg model, an actual weight of 10 kg would have an error of 10 - 6 = 4. This definition of error is often referred to as the residual.\nBelow I plot the residuals of the first 25 Pokémon for our weight = 6 kg model.\n\n\nCode\nggplot(pokemon25, aes(x = reorder(name, pokedex), y = weight)) +\n  geom_bar(stat = \"identity\", alpha = .5) +\n  geom_segment(aes(xend = pokedex, y = 6, yend = weight), linetype = 2) +\n  geom_point() +\n  geom_abline(intercept = 6, slope = 0) +\n  labs(x = \"\", y = \"Weight (kg)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nFigure 4: Residuals of the weight = 6 kg model\n\n\n\n\nWe can add up all of the (absolute) residuals to determine the model’s error. Just like with the binary definition of error, we can then compare multiple models. This is what you see in the graph below. For each model, this time ranging from weight = 1 kg to weight = 100 kg, the absolute residuals were calculated and added together.\n\n\nCode\nerrors_residuals &lt;- expand_grid(\n    model = seq(from = 1, to = 100, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %&gt;%\n  mutate(error = abs(weight - model)) %&gt;%\n  group_by(model) %&gt;%\n  summarize(error_sum = sum(error))\n\nggplot(errors_residuals, aes(x = model, y = error_sum)) +\n  geom_line() +\n  scale_y_continuous(breaks = seq(from = 500, to = 1900, by = 200)) +\n  labs(x = \"Model (weight = x kg)\", y = \"Error (sum of residuals)\")\n\n\n\n\n\nFigure 5: Error #2: The sum of residuals\n\n\n\n\nThis graph looks very different compared to the graph where we calculated the error defined as the sum of misses. Now we see that some kind of minimum appears. Unlike the binary definition of error, it now looks like there are fewer best models. More importantly, though, we have defined error in a less crude manner, meaning that the better models indeed capture the data much better than before.\nBut we might still not be entirely happy with this new definition of error either. Calculating the sum of absolute residuals for each model comes with another conceptual problem.\nWhen you sum the number of absolute errors, four errors of 1 are equal to a single error of 4. In other words, you could have a model that is slightly off multiple times or one that might make fewer, but larger, errors. Both would be counted as equally wrong. What do we think of that? Conceptually speaking, we might find it more problematic when a model is very wrong than when the model is slightly off multiple times. If we think that, we need another definition of error.\nTo address this issue, we can square the residuals before adding them together. That way, larger errors become relatively larger compared to smaller errors. Using our previous example, summing four residuals of 1 remains 4, but a single residual of 4 becomes 4 * 4 = 16. The model now gets punished more severely for making large mistakes.\nUsing this new definition of error, we again plot the error for each model, from 1 to 100.\n\n\nCode\nerrors_squared_residuals &lt;- expand_grid(\n    model = seq(from = 1, to = 100, by = 0.1),\n    weight = pull(pokemon25, weight)\n  ) %&gt;%\n  mutate(error = abs(weight - model)^2) %&gt;%\n  group_by(model) %&gt;%\n  summarize(error_sum = sum(error))\n\nggplot(errors_squared_residuals, aes(x = model, y = error_sum)) +\n  geom_line() +\n  geom_vline(xintercept = mean(pull(pokemon25, weight)), linetype = 2) +\n  labs(x = \"Model\", y = \"Error (sum of squared residuals)\")\n\n\n\n\n\nFigure 6: Error #3: The sum of squared residuals\n\n\n\n\nWe see a smooth curve, with a clear minimum indicated by the vertical dashed line. This vertical line indicates the model that best describes the data. What is the value of the best model exactly? In this case, the answer is 26.14. And it turns out, there is an easy way to determine this value."
  },
  {
    "objectID": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-data-driven-model",
    "href": "content/posts/1-understanding-regression-part-1/understanding-regression-part-1.html#the-data-driven-model",
    "title": "Understanding regression (part 1)",
    "section": "The data-driven model",
    "text": "The data-driven model\nRather than setting a specific value and seeing how it fits the data, we can also use the data to determine the value that best fits the data. In the previous graph we saw that the best fitting model is one where the weight is equal to 26.14. This value turns out to be the mean of the different weights we have observed in our sample. Had we defined error as simply the sum of absolute residuals, this would be a different value. In fact, the best fitting value would then be equal to 13, or the median. And had we used the binary definition of error, the best fitting value would be the mode, which in our case is: 6.9.\nNote that there is not always a unique answer to which model is the best fitting model, depending on the error definition. For example, it is possible that there are multiple modes. If you use the binary definition of error, that would mean there are multiple equally plausible models. This can be another argument to not define a model’s error in such a crude way.\nThe table below shows an overview of which technique can be used to find the best fitting value, depending on the error definition.\n\n\nCode\ntibble(\n  error_definition = c(\"sum of errors\", \"sum of absolute residuals\", \n    \"sum of squared residuals\"),\n  estimation_technique = c(\"mode\", \"median\", \"mean\")\n) %&gt;%\n  kable(col.names = c(\"Error definition\", \"Estimation technique\"), digits = 2) \n\n\n\n\n\nError definition\nEstimation technique\n\n\n\n\nsum of errors\nmode\n\n\nsum of absolute residuals\nmedian\n\n\nsum of squared residuals\nmean\n\n\n\n\n\nWe can now update our model to refer to the estimation technique, rather than a fixed value. Given that the third definition of error seems to be most suitable, both pragmatically and conceptually, we’ll use the mean:\n\nweight = mean(weight)\n\nThis is also the value you get when you perform a regression analysis in R:\n\n\nCode\nlm(weight ~ 1, data = pokemon25)\n\n\n\nCall:\nlm(formula = weight ~ 1, data = pokemon25)\n\nCoefficients:\n(Intercept)  \n       26.1  \n\n\nBy regressing weight onto 1 we are telling R to run an intercept-only model. This means that R will estimate which line will best fit all the values in the outcome variable, just like we have done ourselves earlier by testing different models such as weight = 6 kg.\nThe result is an intercept value of 26.14, which matches the mean of the weights.\nSo, we now know where the intercept comes from when we run an intercept-only model. It is the mean of the data we are trying to model. Note that it is the mean because we defined the model’s error as the sum of squared residuals. Had we defined the error differently, such as the sum of absolute residuals or the sum of errors, the intercept would be the median or mode of the data instead. Why did we use the sum of squared residuals? We had a conceptual reason of wanting to punish larger residuals relatively more than several smaller errors. It turns out there is another reason to favor squared residuals, which has to do with a nice property of the mean vs. the median. This will be covered in Part 2 of ‘Understanding Regression’.\nThis post was last updated on 2022-04-29."
  },
  {
    "objectID": "content/posts/8-the-right-order-of-method-sections/the-right-order-of-method-sections.html",
    "href": "content/posts/8-the-right-order-of-method-sections/the-right-order-of-method-sections.html",
    "title": "The right order of method sections",
    "section": "",
    "text": "I think the proper order of Method sections is:\n\nDesign\nProcedure\nMaterials\nData Analysis\nParticipants\n\nTwo things are notable here. One, there’s a Data Analysis section. Two, the Participants section is all the way at the end. Here I anticipate that your reaction will be that this is crazy, because that’s not how we do things. But that’s not a good enough reason of course. We should be thinking about whether the order makes sense in terms of whether the content of each section logically follows from each other. For some sections you first need to know information from the other sections in order for your decisions to make sense. Putting the Participants all the way at the beginning doesn’t make sense, and the reason for that is the power analysis.\nNow that power analyses are getting more popular, psychologists have to try and make them fit in their Method section. But rather than thinking about what actually goes into a power analysis, and how to present that information to the reader, they generally stick to the format they’re used to. Or perhaps it’s because they misunderstand how a power analysis works, thinking that you only have 1 power analysis per study, so you should present it together with the Design of the study. Since the Design and Participants are sometimes combined, I can see how this might be the case. That still doesn’t make sense, though, and to understand that, we need to understand power analyses.\nWhat goes into a power analysis? A power analysis consists of setting a few parameters, such as the effect size, alpha, and beta. The alpha and beta parameters are pretty constant across different power analyses, but the effect size isn’t. The effect size depends on the exact analysis you want to power for. A t-test is usually done with a Cohen’s d in mind, while a correlation test is done with a correlation in mind. With more sophisticated analyses, such as repeated measures analyses, you need to set additional parameters (e.g., the correlation between repeated measures). This means that your power analysis is dependent on the exact analysis you will do. Actually, a power analysis is always about a specific analysis, so by that logic alone, you should first present which analysis you will do. Not only that, but you also need to power for all analyses you do, not just 1. In other words, a power analysis is something that is tied to a statistical test, and not to the design of a study (which would mean you only need 1 power analysis per study). The result is obvious: you first need to discuss the analyses you want to run before you can talk about power. This means you need a Data Analysis section in your Method section. Here you can elaborate on the analyses you will run, which analyses are the primary ones that you want to power for, and perhaps elaborate on some secondary or exploratory analyses that you won’t power for. You can also use this section to then present the power analysis. After that you get your needed sample size and you can start to explain how you obtained that sample size (i.e., the Participants section).\nWhat do you need to know in order to understand the Data Analysis section? That would be the Design and Materials. You need to know about the design to know whether it is, for example, a between-subjects design or a within-subjects design. You also need to know what the independent variables and dependent variables are. More specifically, you want to know how they are measured. How many levels are there in the independent variables? Is the outcome measure categorical or continuous? These are some of the properties of the measures that determine the appropriate analysis technique. This, in turn, means the Design section and the Materials section need to come before the Data Analysis section.\nPutting all of this together, I think it makes the most sense to begin with the Design, followed by the Procedure and Materials (possibly combined). This should be followed by a Data Analysis section that includes the analyses and associated power analysis (for all primary analyses, at least). Once these aspects are known, it makes sense to end, rather than start, with the Participants section. So the right order of Method sections is:\n\nDesign\nProcedure\nMaterials\nData Analysis\nParticipants"
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "",
    "text": "Not too long ago the House of Representatives of The Netherlands released a public portal to a lot of their data. The portal contains data on law proposals, motions, rapports, etc. I’ve been interested in this kind of data for a while now because I want to know more about the voting behavior of political parties. Specifically, I want to know which parties consistently vote in favor of improving animal rights. It’s relatively easy for a political party to say that they care about animal rights, but that doesn’t mean they consistently vote in favor of motions that improve animal rights. So let’s figure out how the open data portal works and which party to vote for.\nRun the following setup code if you want to follow along.\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(jsonlite)"
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-the-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-the-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Getting the data",
    "text": "Getting the data\nWe will use the OData API to obtain the data. Using this API is pretty easy in theory; it’s nothing more than constructing a URL and then retrieving the data using that URL. The only tricky bit is how to set it up. In order to know how to do that, we need to understand the API. The OData API links to an information model that shows what kind of data we can request. We can request different entities, such as a Zaak (case), Document, Activiteit (activity), and so on. Going through the documentation I figured out we want to request cases because they have a Besluit (decision) entity, which contain a Stemming (vote) entity. Now that we sort of know what we want, we need to figure out how to actually get it.\nThe documentation of the API is pretty good. They explain how to set up the URL, call a query, and even provide several examples.\nEach query starts with the base URL: https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/. We need to append additional functions to this URL to hone in on the exact data we want.\nThe first thing we’ll specify is that we want a Zaak (case), so we will append Zaak to the end of the base URL.\nNext, we will apply some filter functions. In the documentation they recommend that we always filter on entities that have not been removed. They keep removed entities in the database so they can track changes. In one of the examples we can see how this is done. We have to append the following to the URL: ?$filter=Verwijderd eq false. The (first) filter needs to start with a question mark and a dollar sign, followed by the function name (filter), an equal sign, and a condition. The condition in this case is Verwijderd eq false, in other words: Removed equals false.\nAdditional filters can be added using logical operators such as and, or, or not. We want to request only cases that are motions, so we’ll add and Soort eq 'Motie'. Notice that we use and because we want both conditions to be true. The filter itself means that we want the Soort (type) to be equal to ‘Motie’ (motion). If we were to stop here, we would get a bunch of different motions, many of which have nothing to do with animal welfare. So let’s add another filter: and contains(Titel, 'Dierenwelzijn'). This means we select only the motions whose title contains the word ‘Dierenwelzijn’ (animal welfare). We could run this, but then we will get a total of 250 cases. It turns out that this is the maximum number of entities you can retrieve. That’s not ideal because preferably we get all of the animal welfare-related motions and if we get 250 back it’s not clear whether we got all of them. So let’s add another filter: and year(GestartOp) eq 2021. This means we only want cases when they’ve started in 2021. This probably results in fewer than 250 relevant motions, meaning we obtained them all (of that year).\nThe final function we need to add is an expand function. Right now we’re only requesting the data of motions, but not the data of the decision that was made in the motion, or the voting data. To also include that in the request we need to use the expand function. It’s a bit tricky because we need to run the expand function twice, once to expand on the decision and once on the voting. The part we need to append to the URL is: &$expand=Besluit($expand=Stemming).\nNow our URL is pretty much done. We have to paste all the parts together and request the data. We also need to replace all spaces with %20 so that it becomes a valid URL. You don’t need to do this if you just want to paste the URL in the browser, but if you want to use R code like in the code below, we do need to do this.\nThe data will be returned in a JSON format by the API. In R there’s the jsonlite package to work with JSON data, so we’ll use that package. The following code sets up the URL and retrieves the data.\n\n\nCode\n# Set url components\nbase_url &lt;- \"https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/\"\nentity &lt;- \"Zaak\"\nfilter1 &lt;- \"?$filter=Verwijderd eq false\"\nfilter2 &lt;- \" and Soort eq 'Motie'\"\nfilter3 &lt;- \" and contains(Titel, 'Dierenwelzijn')\"\nfilter4 &lt;- \" and year(GestartOp) eq 2021\"\nexpand &lt;- \"&$expand=Besluit($expand=Stemming)\"\n\n# Construct url\nurl &lt;- paste0(base_url, entity, filter1, filter2, filter3, filter4, expand)\n\n# Escape all spaces by replacing them with %20\nurl &lt;- str_replace_all(url, \" \", \"%20\")\n\n# Get data\ndata &lt;- read_json(url)\n\n\nYou can inspect the retrieved data here."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-the-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-the-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nThe data is structured as a list with various attributes, including additional lists. I personally don’t like working with lists at all in R so I want to convert it to a data frame as soon as possible. My favorite way of converting lists to a data frame is by using map_df(). It’s a function that accepts a list as its first argument and a function as its second argument. The function will be applied to each element in the list and the results of that will automatically be merged into a data frame. So let’s create that function.\nIn the code below we create a function that accepts an element of the value attribute in data, which is a list of cases we requested. The function then creates a data frame with only some of the case attributes: the number, title, subject, and start date. You can figure out which attributes are available by checking the documentation or going through the data we just obtained. After creating this function we run map_df().\n\n\nCode\n# Create a custom function to extract data from each motion\nclean_zaak &lt;- function(zaak) {\n  df &lt;- tibble(\n    number = zaak$Nummer,\n    start_date = as_date(zaak$GestartOp),\n    title = zaak$Titel,\n    subject = zaak$Onderwerp\n  )\n}\n\n# Run the clean_zaak function on each case\ndf &lt;- map_df(data$value, clean_zaak)\n\n\nThe result is the following data frame:\n\n\nCode\ndf\n\n\n\n Subset of cases data\n  \n\n\n\nWe can see that all the dates are from 2021 and that the titles contain the word ‘Dierenwelzijn’, just like we filtered on. The subject column is more interesting. It shows us what the case was about (if you don’t see the column, click on the arrow next to the title). After inspecting some of the subjects it becomes obvious that not all cases are about improving animal welfare. One, for example, is about using mobile kill units to kill animals that can’t be transported to a slaughterhouse. Ideally, we should go over all the cases and judge whether the case is about something that improves animal welfare or not.\nAlternatively, we can rely on the heuristic (for now) that in general all the cases on animal welfare are about things that improve animal welfare. Since we’re relying on a heuristic, it would help if we get more data so we can have the exceptions to this heuristic be overruled by many more data points. So let’s retrieve much more data."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-even-more-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#getting-even-more-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Getting even more data",
    "text": "Getting even more data\nBelow I loop over several years and retrieve the data for that year. After retrieving the data, it is saved to a file using the write_json() function. It has an auto_unbox argument so that attributes that only consist of 1 attribute aren’t stored as lists but directly as the type of attribute itself (e.g., a number or string). There’s also the pretty argument which makes sure the file is at least somewhat readable, rather than one single very long line of data.\n\n\nCode\n# Set years we want the data of\nyears &lt;- 2008:2021\n\n# Set url components\nbase_url &lt;- \"https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/\"\nentity &lt;- \"Zaak\"\nfilter1 &lt;- \"?$filter=Verwijderd eq false\"\nfilter2 &lt;- \" and Soort eq 'Motie'\"\nfilter3 &lt;- \" and contains(Titel, 'Dierenwelzijn')\"\nfilter4 &lt;- \" and year(GestartOp) eq \"\nexpand &lt;- \"&$expand=Besluit($expand=Stemming)\"\n\n# Loop over the years\nfor (year in years) {\n  # Construct the url\n  url &lt;- paste0(base_url, entity, filter1, filter2, filter3, filter4,\n    year, expand)\n  \n  # Escape all spaces\n  url &lt;- str_replace_all(url, \" \", \"%20\")\n  \n  # Get data\n  data &lt;- read_json(url)\n  \n  # Write the data to a file\n  write_json(\n    data, \n    path = paste0(\"motions-\", year, \".json\"), \n    auto_unbox = TRUE, \n    pretty = TRUE\n  )\n}"
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-even-more-data",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#cleaning-even-more-data",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Cleaning even more data",
    "text": "Cleaning even more data\nNow that we have a bunch of data files, we need to read them in. A technique to read in multiple files of the same type is to use map_df() again. We can give it a list of files, created with list.files(), and apply a function to each file path. Not only can we use that to simply read in the data, we can immediately parse the data and convert it to a data frame. In the code below I go all inception on this problem and define multiple functions that each convert a list to a data frame. There’s a function for reading in a file, converting a case to a data frame, which calls a function to convert a decision to a data frame, which calls a function to convert a vote to a data frame. It may seem a bit complicated, but once you realize you can call functions within functions, it can actually make some tricky problems easy to solve; at least with relatively little code.\n\n\nCode\nread_file &lt;- function(file) {\n  data &lt;- read_json(file)\n  \n  df &lt;- map_df(data$value, clean_zaak)\n  \n  return(df)\n}\n\nclean_zaak &lt;- function(zaak) {\n  df &lt;- tibble(\n    motion_number = zaak$Nummer,\n    start_date = as_date(zaak$GestartOp),\n  )\n  \n  df &lt;- tibble(\n    df,\n    map_df(zaak$Besluit, clean_besluit)\n  )\n  \n  return(df)\n}\n\nclean_besluit &lt;- function(besluit) {\n  df &lt;- tibble(\n    decision_outcome = besluit$BesluitTekst\n  )\n  \n  if (length(besluit$Stemming) != 0) {\n    df &lt;- tibble(\n      df,\n      map_df(besluit$Stemming, clean_stemming)\n    )\n  }\n  \n  return(df)\n}\n\nclean_stemming &lt;- function(stemming) {\n  df &lt;- tibble(\n    party = stemming$ActorFractie,\n    vote = stemming$Soort,\n    mistake = stemming$Vergissing\n  )\n  \n  return(df)\n}\n\n# Create a list of the files we want to read\nfiles &lt;- list.files(pattern = \"motions-[0-9]+.json\")\n\n# Apply the read_file() function to each file, which calls each other function\ndf &lt;- map_df(files, read_file)\n\n\nLet’s clean up the resulting data frame some more because we kept more information than we actually need. For example, there are different types of decision outcomes, but we only care about the ones where a voting took place. Let’s also translate the votes to English and exclude votes of parties that did not participate (they are still included) and mistaken votes (apparently sometimes they make mistakes when voting).\n\n\nCode\ndf &lt;- df %&gt;%\n  filter(str_detect(decision_outcome, \"Verworpen|Aangenomen\")) %&gt;%\n  filter(vote != \"Niet deelgenomen\") %&gt;%\n  filter(!mistake) %&gt;%\n  mutate(\n    decision_outcome = str_extract(decision_outcome, \"Verworpen|Aangenomen\"),\n    decision_outcome = recode(\n      decision_outcome, \n      \"Verworpen\" = \"rejected\", \n      \"Aangenomen\" = \"accepted\"\n    ),\n    start_date = year(start_date),\n    vote = recode(vote, \"Tegen\" = \"nay\", \"Voor\" = \"aye\"),\n    vote = factor(vote),\n    mistake = NULL\n  )\n\n\nAnnoyingly, I discovered that the decision outcome data changed over the years in a trivial way. Starting in the year 2013, they added a period to the description of the decision outcome (e.g., ‘Verworpen.’). A silly change that actually resulted in me missing data from the years before 2013 while initially writing this post.\nWe now have the following data frame:\n\n\nCode\ndf"
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#analyzing-voting-behavior",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#analyzing-voting-behavior",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Analyzing voting behavior",
    "text": "Analyzing voting behavior\nNow we are ready to inspect the voting behavior of the political parties. For each party we calculate how often they voted ‘aye’ or ‘nay’ and calculate it as a percentage of the times they’ve voted. We then plot the percentage of times they voted ‘aye’.\n\n\nCode\nvoting &lt;- df %&gt;%\n  count(party, vote, .drop = FALSE) %&gt;%\n  pivot_wider(names_from = vote, values_from = n) %&gt;%\n  mutate(\n    votes = aye + nay,\n    aye_pct = aye / votes\n  )\n\nggplot(voting, aes(x = aye_pct, y = reorder(party, aye_pct))) +\n  geom_col(aes(alpha = votes)) +\n  labs(\n    x = \"Times voted 'aye' on an animal welfare motion (in %)\", \n    y = \"\",\n    alpha = \"Times voted\") +\n  scale_x_continuous(limits = c(0, 1), labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\nPercentage of times political parties voted ‘aye’ on motions related to animal welfare\n\n\n\n\nIt seems like the heuristic might be somewhat justified. I don’t know much about Van Kooten-Arissen or Group Krol/vKA, but PvdD stands for Partij voor de Dieren (party for the animals). It makes sense that they are among the top in voting in favor of improving animal welfare. At the same time there’s some evidence that the heuristic is indeed only a heuristic. The PvdD apparently voted ‘aye’ in 84.13% of the motions. That could mean there are some motions where it is in the interest of the animals to vote ‘nay’. I also find it a bit worrying that the PVV, a notorious right-wing party in the Netherlands, is so high on the list of voting ‘aye’ on animal welfare matters. In some manual inspections of the motions I saw they tend to disagree with some obvious animal welfare improvements, although perhaps my sample just happened to find these disagreements and had I inspected more motions I would have found the same results.\nAnother way we can look at this data is by using the PvdD as a benchmark for what the other political parties should vote for. We can assume that this party has the best interest for animals in mind, as that is their most important platform. Of course this would mean we can’t use the result to figure out whether we should vote for PvdD, but it can be useful to figure out which alternative party to vote for.\n\n\nCode\nvote_matches_PvdD &lt;- df %&gt;%\n  filter(party == \"PvdD\") %&gt;%\n  group_by(motion_number) %&gt;%\n  summarize(vote_PvdD = first(vote)) %&gt;%\n  right_join(df, by = \"motion_number\") %&gt;%\n  filter(party != \"PvdD\") %&gt;%\n  mutate(\n    match = if_else(vote == vote_PvdD, \"match\", \"no_match\"),\n    match = factor(match)\n  ) %&gt;%\n  count(party, match, .drop = FALSE) %&gt;%\n  pivot_wider(names_from = match, values_from = n) %&gt;%\n  mutate(\n    votes = match + no_match,\n    match_pct = match / votes\n  )\n\nggplot(vote_matches_PvdD, aes(x = match_pct, y = reorder(party, match_pct))) +\n  geom_col(aes(alpha = votes)) +\n  labs(\n    x = \"Times voted the same as the Party for the Animals (in %)\", \n    y = \"\",\n    alpha = \"Times voted\") +\n  scale_x_continuous(limits = c(0, 1), labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\nPercentage of times a political party voted the same as the Party for the Animals\n\n\n\n\nIt looks like the two graphs are fairly consistent. Van Kooten-Arissen and Groep Krol/vKA are still at the top. The same goes for the bigger parties. In fact, the ranking of the largest parties (who have voted the most times) is the same in the two graphs. That can give us some extra confidence that the heuristic from the first graph works.\nLet’s go back to our heuristic and create another graph that shows the voting behavior across the years. After all, it could very well be that a political party changed their values in the last decade. Let’s only use the 10 biggest parties for this graph because they’re more likely to have enough data for each year.\n\n\nCode\nvoting_years &lt;- df %&gt;%\n  group_by(party) %&gt;%\n  mutate(votes = n()) %&gt;%\n  filter(votes &gt; 300) %&gt;%\n  count(start_date, party, vote, .drop = FALSE) %&gt;%\n  pivot_wider(names_from = vote, values_from = n) %&gt;%\n  mutate(\n    votes = aye + nay,\n    aye_pct = aye / votes\n  ) \n\nggplot(\n    voting_years, \n    aes(x = start_date, y = aye_pct)\n  ) +\n  geom_line(alpha = .25) +\n  geom_point() +\n  facet_wrap(~ party, ncol = 2) +\n  labs(\n    x = \"\", \n    y = \"\"\n  ) +\n  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\nTimes each party voted ‘aye’ on animal welfare motions throughout the years\n\n\n\n\nLooks like most parties are fairly consistent. There’s some variation from year to year, but for most parties you can tell whether they are pro-animal or not. There are some exceptions to this, like the PvdA and D66, which seem to vary quite a bit. We can actually calculate this variation so we don’t have to guess it from the graph.\n\n\nCode\nvoting_years %&gt;%\n  group_by(party) %&gt;%\n  summarize(SD = sd(aye_pct)) %&gt;%\n  arrange(desc(SD))\n\n\n\n Party consistency across years\n  \n\n\n\nYup, looks like PvdA and D66 are the least consistent."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#limitations",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#limitations",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Limitations",
    "text": "Limitations\nIt bears repeating that this analysis of the data isn’t perfect. The best way to analyze this data would be to take a look at each individual motion and determine, based on your own values, whether an ‘aye’ vote or a ‘nay’ vote is in the best interest of animals. I hope to do this myself in the future.\nAnother limitation of this analysis is that we have only searched for motions that mention animal welfare in the title. There are many more motions that address animal welfare issues that we’ve missed with our method. The results could therefore be made more reliable by adding more data. In this post we have looked at 5453 votes in 391 motions. I suppose that’s decent, but we could do better."
  },
  {
    "objectID": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#conclusion",
    "href": "content/posts/17-politics-animal-voting-behavior-netherlands/politics-animal-voting-behavior-netherlands.html#conclusion",
    "title": "Voting behavior of Dutch political parties on animal welfare motions",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we used publicly available data from the House of Representatives of the Netherlands to inspect the voting behavior of political parties on matters related to animal welfare. This is made possible by the amazing new data portal that makes this data freely and relatively easily available to everyone. Kudos to them for that.\nIf you’re interested in figuring out which party to vote for because you want to support animal welfare, then the largest parties to pay attention to are the PvdD, GroenLinks, and SP. They are large enough to have voted on issues a decent number of times, giving us some confidence that they vote in favor of improving animal welfare. More can be done to improve this interpretation of the data, but looking at actual voting behavior seems like a valuable piece of information when considering which party to vote for."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "",
    "text": "Many thanks to the people who replied to my tweet about why you should divide by \\(n - 1\\). Below I try to show the intuition behind why this is necessary. If you want to follow along in R, you can copy the code from each code section; beginning with some setup code.\nCode\n# Load packages\nlibrary(tidyverse)\nlibrary(viridis)\n\n# Create our own variance function that returns the population or sample \n# variance\nmy_var &lt;- function(x, population = FALSE) {\n  if (population) {\n    sum((x - mean(x))^2)/length(x)\n  } else {\n    sum((x - mean(x))^2)/(length(x) - 1)\n  }\n}\n\n# Set the default ggplot theme\ntheme_set(theme_minimal())\n\n# Set options\noptions(\n  knitr.kable.NA = \"\",\n  digits = 2\n)"
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-formula",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-formula",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The formula",
    "text": "The formula\nThe formula for calculating the variance is:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n}\\]\nThe variance is a measure of the dispersion around the mean, and in that sense this formula makes sense. We calculate all the deviations from the mean (\\(x_i - \\overline{x}\\)), square them (for reasons I might go into in a different post) and sum them. We then divide this sum by the number of observations as a scaling factor. If we ignore this number, we could get a very high variance simply by observing a lot of data. So, to fix that problem, we divide by the total number of observations.\nHowever, this is the formula for the population variance. The formula for calculating the variance of a sample is:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n - 1}\\]\nWhy do we divide by n - 1?\nIf you Google this question, you will get a variety of answers. You might find a mathematical proof of why it needs to be \\(n - 1\\) or something about degrees of freedom. These kinds of answers don’t work for me. I trust them to be correct, but it doesn’t produce any insight. It does not actually help me understand 1) the problem and 2) why the solution is the solution that it is. So, below I am going to try to figure it out in a way that actually makes conceptual and intuitive sense (to me)."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-problem",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-problem",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The problem",
    "text": "The problem\nThe problem with using the population variance formula to calculate the variance of a sample is that it is biased. It is biased in that it produces an underestimation of the true variance. Let’s demonstrate that with some simulated data.\nWe simulate a population of 1000 data points from a uniform distribution with a range from 1 to 10. Below I show the histogram that represents our population.\n\n\nCode\n# Set the seed for reproducibility\nset.seed(1212)\n\n# Create a population consisting of values ranging from 1 to 10\npopulation &lt;- sample(1:10, size = 1000, replace = TRUE)\n\n# Calculate the population variance\nsigma &lt;- my_var(population, population = TRUE)\n\n# Visualize the population\nggplot(tibble(population = population), mapping = aes(x = population)) +\n  geom_bar(alpha = .85) +\n  labs(x = \"x\", y = \"n\") +\n  scale_x_continuous(breaks = 1:10)\n\n\n\n\n\nFigure 1: The population in our example\n\n\n\n\nThe variance is 8.76. Note that this is our population variance (often denoted as \\(\\sigma^2\\)). We want to estimate this value using samples drawn from our population, so let’s do that.\n\n\nCode\n# Draw a single sample from the population\nsample &lt;- sample(population, size = 5)\n\n\nTo start, we can draw a single sample of size 5. Say we do that and get the following values: 7, 6, 3, 5, 5. We can then calculate the variance in two ways, using division by \\(n\\) and division by \\(n - 1\\). In the former case, this will result in 1.76 and in the latter case it results in 2.2.\nNow let’s do that many many times. Below I show the results of draws from our population. I simulated drawing samples of size 2 to 10, each 1000 different times. I then plotted for each sample size the average biased variance (dividing by \\(n\\); Figure 2 (a)) and the average unbiased variance (dividing by \\(n - 1\\); Figure 2 (b)).\n\n\nCode\n# Create an empty data frame with the simulation parameters\nsamples &lt;- crossing(\n    n = 2:20,\n    i = 1:1000\n  )\n\n# Calculate the mean, sample variance, and population variance \n# for each combination of n and i\nsamples &lt;- samples %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    var_unbiased = my_var(sample(population, n), population = FALSE),\n    var_biased = my_var(sample(population, n), population = TRUE)\n  )\n\n# Plot the results in two separate plots\nggplot(samples, aes(x = n, y = var_biased)) +\n  geom_hline(yintercept = sigma, linetype = \"dashed\", alpha = .5) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\") +\n  coord_cartesian(ylim = c(0, sigma + 1)) +\n  scale_x_continuous(breaks = seq(from = 2, to = 20, by = 2))\n\nggplot(samples, aes(x = n, y = var_unbiased)) +\n  geom_hline(yintercept = sigma, linetype = \"dashed\", alpha = .5) +\n  stat_summary(fun = \"mean\", geom = \"point\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\") +\n  coord_cartesian(ylim = c(0, sigma + 1)) +\n  scale_x_continuous(breaks = seq(from = 2, to = 20, by = 2))\n\n\n\n\n\n\n\n\n(a) Variance with division by n\n\n\n\n\n\n\n\n(b) Variance with division by n - 1\n\n\n\n\nFigure 2: A demonstration that dividing by 1 causes a bias\n\n\n\nWe see that the biased measure of variance is indeed biased. The average variance is lower than the true variance (indicated by the dashed line), for each sample size. We also see that the unbiased variance is indeed unbiased. On average, the sample variance matches that of the population variance.\nThe results of using the biased measure of variance reveals several clues for understanding the solution to the bias. We see that the amount of bias is larger when the sample size of the samples is smaller. So the solution should be a function of sample size, such that the required correction will be smaller as the sample size increases. We also see that that the bias at \\(n = 2\\) is half that of the true variance, \\(\\frac23\\) at \\(n = 3\\), \\(\\frac34\\) at \\(n = 4\\), and so on. Interesting.\nBut before we go into the solution, we still need to figure out what exactly causes the bias.\nIdeally we would estimate the variance of the sample by subtracting each value from the population mean. However, since we don’t know what the population mean is, we use the next best thing—the sample mean. This is where the bias comes in. When you use the sample mean, you’re guaranteed that the mean lies somewhere within the range of your data points. In fact, the mean of a sample minimizes the sum of squared deviations from the mean. This means that the sum of deviations from the sample mean is almost always smaller than the sum of deviations from the population mean. The only exception to that is when the sample mean happens to be the population mean.\nLet’s illustrate this with a few graphs. Below are two graphs in which I show 10 data points that represent our population. I also highlight two data points from this population, which represents our sample. In the left graph I show the deviations from the sample mean and in the right graph the deviations from the population mean.\n\n\nCode\n# Create the population\nx &lt;- c(1, 2, 4, 4, 4, 6, 8, 9, 10, 10)\n\n# Create a sample\nsample1 &lt;- tibble(\n    index = 1:10,\n    value = x,\n    sample = c(0, 0, 0, 0, 0, 0, 1, 0, 0, 1),\n    mean = mean(c(8, 10)),\n    mu = mean(x)\n  ) %&gt;%\n  mutate(\n    mean = ifelse(sample == 1, mean, NA),\n    mu = ifelse(sample == 1, mu, NA)\n  )\n\n# Plot the results\nggplot(sample1, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mu), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mean), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mean, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 5.8, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 9, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n\nggplot(sample1, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mean), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mu), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mu, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 5.8, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 9, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n\n\n\n\n\n\n\n\n(a) Deviations from the sample mean\n\n\n\n\n\n\n\n(b) Deviations from the population mean\n\n\n\n\nFigure 3: An illustration of how using a sample mean introduces bias\n\n\n\nWe see that in the left graph the sum of squared deviations is much smaller than in the right graph. The sum is \\((8 - 9)² + (10 - 9)² = 2\\) in the left graph and in the right graph it’s \\((8 - 5.8)² + (10 - 5.8)² = 22.48\\). The sum is smaller when using the sample mean compared to using the population mean.\nThis is true for any sample you draw from the population (again, except when the sample mean happens to be the same as the population mean). Let’s look at one more draw where the sample mean is closer to the population mean.\n\n\nCode\n# Create a second sample\nsample2 &lt;- tibble(\n    index = 1:10,\n    value = x,\n    sample = c(0, 1, 0, 0, 0, 0, 0, 0, 1, 0),\n    mean = mean(c(2, 10)),\n    mu = mean(x)\n  ) %&gt;%\n  mutate(\n    mean = ifelse(sample == 1, mean, NA),\n    mu = ifelse(sample == 1, mu, NA)\n  )\n\n# Plot the results\nggplot(sample2, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mu), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mean), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mean, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 4.92, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 6.7, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n  \nggplot(sample2, aes(x = index, y = value, color = factor(sample))) +\n  geom_hline(aes(yintercept = mean), linetype = \"dashed\") +\n  geom_point(size = 2.5) +\n  geom_hline(aes(yintercept = mu), linetype = \"solid\") +\n  geom_segment(aes(xend = index, y = mu, yend = value), linetype = \"solid\") +\n  annotate(\"text\", x = 11, y = 4.92, label = \"population mean\") +\n  annotate(\"text\", x = 11, y = 6.7, label = \"sample mean\") +\n  labs(x = \"\", y = \"\") +\n  scale_color_viridis(\n    option = \"mako\", \n    discrete = TRUE, \n    begin = .25, \n    end = .75\n  ) +\n  coord_flip(clip = \"off\", xlim = c(0, 10)) +\n  scale_y_continuous(breaks = 1:10) +\n  guides(color = \"none\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(2, 1, 1, 1), \"lines\")\n  )\n\n\n\n\n\n\n\n\n(a) Deviations from the sample mean\n\n\n\n\n\n\n\n(b) Deviations from the population mean\n\n\n\n\nFigure 4: Another illustration of how using a sample mean introduces bias\n\n\n\nHere the sum in the left graph is \\((2 - 6)² + (10 - 6)² = 32\\) and the sum in the right graph is \\((2 - 5.8)² + (10 - 5.8)² = 32.08\\). The difference is small now, but using the sample mean still results in a smaller sum compared to using the population mean.\nIn short, the source of the bias comes from using the sample mean instead of the population mean. The sample mean is always guaranteed to be in the middle of the observed data, thereby reducing the variance, and creating an underestimation."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-solution",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-solution",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The solution",
    "text": "The solution\nNow that we know that the bias is caused by using the sample mean, we can figure out how to solve the problem.\nLooking at the previous graphs, we see that if the sample mean is far from the population mean, the sample variance is smaller and the bias is large. If the sample mean is close to the population mean, the sample variance is larger and the bias is small. So, the more the sample mean moves around the population mean, the greater the bias.\nIn other words, besides the variance of the data points around the sample mean, there is also the variance of the sample mean around the population mean. We need both variances in order to accurately estimate the population variance.\nThe population variance is thus the sum of two variances:\n\\[\\sigma^2_{sample} + \\sigma^2_{\\vphantom{sample}mean} = \\sigma^2_{population}\\] Let’s confirm that this is true. For that we need to know how to calculate the variance of the sample mean around the population mean. This is relatively simple; it’s the variance of the population divided by n (\\(\\frac{\\sigma^2}n\\)). This makes sense because the greater the variance in the population, the more the mean can jump around, but the more data you sample, the closer you get to the population mean.\nNow that we can calculate both the variance of the sample and the variance of the sample mean, we can check whether adding them together results in the population variance.\nBelow I show a graph in which I again sampled from our population with varying sample sizes. For each sample, I calculated the sample variance (the biased one) and the variance of the mean of that sample (\\(\\frac{\\sigma^2}n\\))1. I did this 1000 times per sample size, took the average of each and put them on top of each other. I also added a dashed line to indicate the variance of the population, which is the benchmark we’re trying to reach.\n\n\nCode\n# Calculate the variance sources per sample size\nvariance_sources &lt;- samples %&gt;%\n  mutate(var_mean = var_unbiased / n) %&gt;%\n  group_by(n) %&gt;%\n  summarize(\n    var_biased = mean(var_biased),\n    var_unbiased = mean(var_unbiased),\n    var_mean = mean(var_mean)\n  ) %&gt;%\n  pivot_longer(cols = c(var_biased, var_mean), names_to = \"variance_source\", \n    values_to = \"variance\") %&gt;%\n  mutate(variance_source = recode(variance_source, \"var_biased\" = \n      \"sample\", \"var_mean\" = \"sample mean\"))\n\n# Plot the results\nggplot(variance_sources, aes(x = n, fill = variance_source, y = variance)) +\n  geom_col(alpha = .85) +\n  geom_hline(yintercept = sigma, linetype = \"dashed\") +\n  labs(x = \"Sample size (n)\", y = \"Variance\", fill = \"Variance source:\") +\n  scale_fill_viridis(option = \"mako\", discrete = TRUE, begin = .25, end = .75)\n\n\n\n\n\nFigure 5: Sources of variance\n\n\n\n\nIndeed, we see that the variance of the sample and the variance of the mean of the sample together form the population variance."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-math",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#the-math",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "The math",
    "text": "The math\nNow that we know that the variance of the population consists of the variance of the sample and the variance of the sample mean, we can figure out the correction factor we need to apply to make the biased variance measure unbiased.\nPreviously, we found an interesting pattern in the simulated samples, which is also visible in the previous figure. We saw that at sample size \\(n=2\\), the (biased) sample variance appears to be half that of the (unbiased) population variance. At sample size \\(n=3\\), it’s \\(\\frac23\\). At sample size \\(n=4\\), it’s \\(\\frac34\\), and so on.\nThis means that we can fix the biased variance measure by multiplying it with \\(\\frac{n}{(n-1)}\\). At \\(n = 2\\), we multiply the biased variance by \\(\\frac21 = 2\\). For sample size \\(n=3\\), we multiply by \\(\\frac32 = 1.5\\). At sample size \\(n=4\\), it’s \\(\\frac43 = 1 \\frac13\\).\nIn other words, to unbias the biased variance measure, we multiply it by a correction factor of \\(\\frac{n}{(n-1)}\\). But where does this correction factor come from?\nWell, because the sample variance misses the variance of the sample mean, we can expect that the variance of the sample is biased by an amount equal to the variance of the population minus the variance of the sample mean. In other words:\n\\[\\sigma^2 - \\frac{\\sigma^2}n\\]\nRewriting this 2, produces:\n\\[\\sigma^2\\cdot\\frac{n - 1}n\\] The variance of a sample will be biased by an amount equal to \\(\\frac{n - 1}n\\). To correct that bias we should multiply the sample variance by the inverse of this bias: \\(\\frac{n}{n-1}\\) 3. This is also called Bessel’s correction.\nSo, an unbiased measure of our sample variance is the biased sample variance times the correction factor:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n}\\cdot{\\frac n{n-1}}\\] Because the n in the denominator of the left term (the biased variance formula) cancels out the n in the numerator of the right term (the bias correction), the formula can be rewritten as:\n\\[\\frac{\\sum(x_i - \\overline{x})^2}{n-1}\\]"
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#summary",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#summary",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "Summary",
    "text": "Summary\nWe calculate the variance of a sample by summing the squared deviations of each data point from the sample mean and dividing it by \\(n - 1\\). The \\(n - 1\\) actually comes from a correction factor \\(\\frac n{n-1}\\) that is needed to correct for a bias caused by taking the deviations from the sample mean rather than the population mean. Taking the deviations from the sample mean only constitutes the variance around the sample mean, but ignores the variation of the sample mean around the population mean, producing an underestimation equal to the size of the variance of the sample mean: \\(\\frac{\\sigma^2}{n}\\). The correction factor corrects for this underestimation, producing an unbiased estimate of the population variance.\nThis post was last updated on 2022-05-30."
  },
  {
    "objectID": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#footnotes",
    "href": "content/posts/4-why-divide-by-n-1/why-divide-by-n-1.html#footnotes",
    "title": "Why divide by \\(n - 1\\) to calculate the variance of a sample?”",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere I cheat a little because in order to calculate the variance of the sample mean, I need to use the unbiased variance formula.↩︎\nHere are the steps to rewrite the formula: \\[\\sigma^2 - \\frac{\\sigma^2}n\\] Add an n to the numerator and denominator of the left term: \\[\\frac{\\sigma^2n}n - \\frac{\\sigma^2}n\\] Combine the terms: \\[\\frac{\\sigma^2n - \\sigma^2}n\\] Simplify the numerator: \\[\\frac{\\sigma^2(n - 1)}n\\] Move \\(\\sigma^2\\) out of the numerator: \\[\\sigma^2\\cdot\\frac{n - 1}n\\]↩︎\nThe inverse of \\(\\frac{n - 1}n\\) is \\(\\frac{1}{\\frac{n - 1}n}\\). Multiply both the numerator and the denominator by \\(n\\) and you get \\(\\frac{n}{n-1}\\).↩︎"
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "",
    "text": "This post is the first of a series of tutorial posts on Bayesian statistics. I’m not an expert on this topic, so this tutorial is partly, if not mostly, a way for me to figure it out myself.\nThe goal will be to go through each step of the data analysis process and make things as intuitive and clear as possible. I’ll use the brms package to run the models and I will rely heavily on the book Statistical Rethinking by Richard McElreath.\nThe basic idea behind Bayesian statistics is that we start off with prior beliefs about the parameters in the model and then update those beliefs using data. That means that for all models you need to figure out what your beliefs are before running any analyses. This is very different from frequentist statistics and probably the most off-putting part of running Bayesian analyses. However, my goal is to make this relatively easy by focusing on visualizing priors and how they change as a function of the Bayesian process. I’ll also try to come up with some methods to simplify the construction of priors, with the goal to have them be reasonable and non-controversial.\nIn some cases I might not even use a prior I personally believe in. Instead I’ll use a prior that represents a particular position or skepticism so that the results of the analysis can be used to change the mind of the skeptic, rather than me changing whatever I happen to believe.\nWith this in mind, let’s begin."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#setup",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#setup",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "Setup",
    "text": "Setup\nIn the code chunk below I show some setup code to get started, starting with the packages. After loading the packages I set the default ggplot2 theme and some colors. Finally, I set several brms specific options to speed up the functions and automatically re-run models if a model has changed. If a model has not changed, the results will be read from a file to speed things up.\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggdist)\n\ntheme_set(theme_minimal())\nblue_1 &lt;- \"#d1e1ec\"\nblue_2 &lt;- \"#b3cde0\"\nblue_3 &lt;- \"#6497b1\"\nblue_4 &lt;- \"#005b96\"\nblue_5 &lt;- \"#03396c\"\nblue_6 &lt;- \"#011f4b\"\n\noptions(\n  mc.cores = 4,\n  brms.threads = 4,\n  brms.backend = \"cmdstanr\",\n  brms.file_refit = \"on_change\"\n)"
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#data",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#data",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "Data",
    "text": "Data\nThe data I’ll play with is the same data Richard McElreath uses in Chapter 4 of his amazing book Statistical Rethinking. The data consists of partial census data of the !Kung San, compiled from interviews conducted by Nancy Howell in the late 1960s. Just like in the book, I will focus only on people 18 years or older.\n\n\nCode\ndata &lt;- read_csv(\"Howell1.csv\")\ndata &lt;- filter(data, age &gt;= 18)\n\nhead(data)\n\n\n\n\nPartial census data for the Dobe area !Kung San compiled by Nancy Howell in the late 1960s. \n\n\nheight\nweight\nage\nmale\n\n\n\n\n151.765\n47.82561\n63\n1\n\n\n139.700\n36.48581\n63\n0\n\n\n136.525\n31.86484\n65\n0\n\n\n156.845\n53.04191\n41\n1\n\n\n145.415\n41.27687\n51\n0\n\n\n163.830\n62.99259\n35\n1"
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#an-intercept-only-model",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#an-intercept-only-model",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "An intercept-only model",
    "text": "An intercept-only model\nLet’s focus the first question on the heights in the data. What are the heights of the Dobe area !Kung San?\nThe way to address this question is by constructing a model in which heights are regressed on only the intercept, i.e., an intercept-only model. You may be familiar with the R formula for this type of model: height ~ 1.\nWith this formula and the data we can use brms to figure out which priors we need to set by running the get_prior() function. This is probably the easiest way to figure which priors you need when you’re just starting out using brms.\n\n\nCode\nget_prior(height ~ 1, data = data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\nstudent_t(3, 154.3, 8.5)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 8.5)\nsigma\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\nThe output shows us that we need to set two priors, one for the Intercept and one for sigma. brms already determined a default prior for each parameter (they are required for a Bayesian analysis), so we could immediately run an analysis if we want to but it is recommended to construct your own priors because you can often do better than the default priors.\nAlso, using the get_prior() function is not the best way to think about which priors we need. Using the function will give us the answer, but it doesn’t really improve our understanding of why these two priors are needed. In this case I also omitted an important specification of the heights, which is that they are normally distributed (the default in get_prior()). So let’s instead write down the model in a different way, which explicitly specifies how we think the heights are distributed and which parameters we need to set priors on. If we think the heights are normally distributed, we define our model like this:\n\\[heights_i ∼ Normal(\\mu, \\sigma)\\]\nWe explicitly note that the individual heights come from a normal distribution, which is determined by the parameters \\(\\mu\\) and \\(\\sigma\\). This then also immediately tells us that we need to set two priors, one on \\(\\mu\\) and one on \\(\\sigma\\).\nIn our intercept-only model, the \\(\\mu\\) parameter refers to our intercept and the \\(\\sigma\\) parameter refers to, well, sigma. Sigma is not often discussed in the literature I’m familiar with, but we’ll figure it out below. In fact, let’s discuss each of these parameters in turn and figure out what kind of prior makes sense."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#the-intercept-mu-prior",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#the-intercept-mu-prior",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "The intercept (\\(\\mu\\)) prior",
    "text": "The intercept (\\(\\mu\\)) prior\nThe prior for the intercept indicates what I believe the average height of the !Kung San to be.\nbrms has set the default intercept prior as a Student t-distribution with 3 degrees of freedom, a mean of 154.3 and a standard deviation of 8.5. That means brms starts off with a ‘belief’ that the average of the heights is 154.3, but with quite some uncertainty reflected in the standard deviation of 8.5 and the fact that the distribution is a Student t-distribution. A Student t-distribution has thicker tails compared to a normal distribution, meaning that numbers in the tails of the distribution are more likely compared to a normal distribution, at least when the degrees of freedom are low. At higher degrees of freedom, the t-distribution becomes more and more like the normal distribution. So, the thicker tails of the t-distributions means smaller and taller average heights are relatively more plausible.\nBut this is the default prior. brms determines this prior by peeking at the data to create a weak prior that is easily updated by the data. We can do better a bit better than the default prior, though.\nWhat do I believe the average height to be? As a Dutch person, I might think that the average height is around 175 centimeters. This is probably too tall to use as an average for the !Kung San because we’re known for being quite tall. So I think the average should be lower than 175, perhaps 170. I am not very sure, though. After all, I am far from an expert on people’s heights; I am only using my layman knowledge here. An average of 165 seems possible to me too. So let’s describe my belief in the form of a distribution in which multiple averages are possible, to varying extents. We could use a Student t-distribution with small degrees of freedom if we want to allow for the possibility of being very wrong (remember, it has thicker tails, so it assigns more probability to a wider range of average heights). We’re not super uncertain about people’s heights, though, so let’s use a normal distribution.\nAs we saw in defining our height model, a normal distribution requires that we set two parameters: the \\(\\mu\\) and the \\(\\sigma\\). The \\(\\mu\\) we already covered (i.e., 170), so that leaves \\(\\sigma\\). Let’s set this to 10 and see what happens by visualizing this prior. Below I plot both the default brms prior and our own with \\(\\mu\\) = 170 and \\(\\sigma\\) = 10.\n\n\nCode\nheight_prior_intercept &lt;- tibble(\n  height_mean = seq(from = 100, to = 250, by = 0.1),\n  mine = dnorm(height_mean, mean = 170, sd = 10),\n  default = dstudent_t(height_mean, df = 30, mu = 154.3, sigma = 8.5),\n)\n\nheight_prior_intercept &lt;- pivot_longer(\n  height_prior_intercept,\n  cols = -height_mean,\n  names_to = \"prior\"\n)\n\nggplot(\n  height_prior_intercept,\n  aes(x = height_mean, y = value, linetype = fct_rev(prior))\n) +\n  geom_line() +\n  labs(x = \"Average height\", y = \"\", linetype = \"Prior\") +\n  scale_x_continuous(breaks = seq(100, 250, 20))\n\n\n\n\n\nTwo priors for \\(\\mu\\)\n\n\n\n\nMy prior indicates that I believe the average height to be higher than the default prior. In terms of the standard deviation, we both seem to be about equally uncertain about this average. Looking at this graph I think this prior of mine is not very plausible. Apparently I assign quite a chunk of plausibility to an average of 180 cm, or even 190 cm, which is very unlikely. An average of 160 cm is more plausible to me than an average of 180, so I should probably lower the mu, or use more of a skewed distribution. This is one of the benefits of visualizing the prior, it can make you think again about your prior so that you may improve on it. Based on the graph, I will change the mean of my prior to 160. I can probably also lower the standard deviation, but I’ll leave it at 10 to show how easily the data will update this prior."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#the-sigma-sigma-prior",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#the-sigma-sigma-prior",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "The sigma (\\(\\sigma\\)) prior",
    "text": "The sigma (\\(\\sigma\\)) prior\nWhat about the sigma prior? What even is sigma? Sigma is the estimated standard deviation of the errors or, in other words, the standard deviation of the residuals of the model. In the simple case of an intercept-only model, this is identical to the standard deviation of the outcome (heights, in this case).\nI think setting the standard deviation of the distribution of heights (not the mean of the heights) is quite difficult. There are parts that are easy, such as the fact that the standard deviation has to be 0 or larger (it can’t be negative), but exactly how large it should be, I don’t know.\nI do know it is unlikely to be close to 0, and unlikely to be very large. That’s because I know people’s heights do vary, so I know the sigma can’t be 0. I also know it’s not super large because we don’t see people who are taller than 2 meters very often. This means the peak of our prior should be somewhere above 0, with a tail to allow higher values but not too high. We can use a normal distribution for this with a mean above 0 and a particular standard deviation, and ignore everything that’s smaller than 0 (brms automatically ignores negative values for \\(\\sigma\\)).\nAs I mentioned before, there is a downside of using a normal distribution, though. Normal distributions have long tails, but there is actually very little density in those tails. If we are quite uncertain about our belief about sigma, we should use a t-distribution, or perhaps even a cauchy distribution (actually, the cauchy distribution is a special case of the t-distribution; they are equivalent if the degree of freedom is 1). The lower the degrees of freedom, the more probability we assign to higher and lower values.\nA t-distribution requires three parameters: \\(\\mu\\), \\(\\sigma\\), and the degrees of freedom. I set \\(\\mu\\) to 5, \\(\\sigma\\) to 5, and the degrees of freedom to 1. Below I plot this prior and brms’s default prior to get a better grasp of these priors.\n\n\nCode\nheight_prior_sigma &lt;- tibble(\n  height_sigma = seq(from = 0, to = 50, by = .1),\n  default = dstudent_t(height_sigma, df = 3, mu = 0, sigma = 8.5),\n  mine = dstudent_t(height_sigma, df = 1, mu = 5, sigma = 5)\n)\n\nheight_prior_sigma &lt;- pivot_longer(\n  height_prior_sigma,\n  cols = -height_sigma,\n  names_to = \"prior\"\n)\n\nggplot(\n  height_prior_sigma,\n  aes(x = height_sigma, y = value, linetype = fct_rev(prior))\n) +\n  geom_line() +\n  labs(x = \"Standard deviation of heights\", y = \"\", linetype = \"Prior\")\n\n\n\n\n\nTwo priors for \\(\\sigma\\)\n\n\n\n\nAs you can see, both distributions have longish tails, allowing for the possibility of high standard deviations. There are some notable differences between the two priors, though. Our prior puts more weight on a standard deviation larger than 0, while the default prior reflects a belief in which a standard deviation of 0 is most likely. However, both priors are quite weak."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#prior-predictive-check",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#prior-predictive-check",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "Prior predictive check",
    "text": "Prior predictive check\nSo far we have inspected each prior in isolation, but we can also use our priors to simulate heights and see if the distribution of heights makes sense. This is called a prior predictive check.\nWe can perform a prior predictive check using the brm() function from brms.The brm() function is the main work horse of the brms package. It allows us to run Bayesian analyses by using a common notation style familiar to those who use R. This is also one of the reasons why the brms package is so great; it’s very easy to get started with running Bayesian analyses.\nThe brm() function requires a model specification and the data. Optionally, but usefully, we should also specify the response distribution (a normal distribution by default) and the priors.\nHowever, we’re not ready to actually run the model just yet. Instead, we will kinda trick brms into running an analysis, but tell it to only sample from the prior using the sample_prior argument. This will give us ‘predicted’ responses based entirely on our priors and not the data.\nAdditionally, we also set a seed to make the results reproducible and a file to store the results into so that if we run the analysis again, we can simply read the results from the file rather than running the analysis again.\n\n\nCode\nmodel_height_prior &lt;- brm(\n  height ~ 1,\n  data = data,\n  family = gaussian,\n  prior = c(\n    prior(normal(160, 10), class = \"Intercept\", lb = 0, ub = 250),\n    prior(cauchy(5, 5), class = \"sigma\")\n  ),\n  sample_prior = \"only\",\n  seed = 4,\n  file = \"models/model_height_prior.rds\"\n)\n\n\nThe next part is a little tricky. The goal is to obtain a large sample of predicted heights so we can visualize its distribution. By default, brms will draw 4000 draws to approximate distributions. We could use the predict() function to get these draws (e.g., predict(model_height_prior), but I prefer to use the tidybayes package because it’s a really nice package that simplifies a lot of things about working with brms models.\nThe tidybayes function to use is add_predicted_draws(). The function takes a data frame and a model object to add predictions for each row in the data frame. This is the tricky part because we don’t really have any data to supply because it’s an intercept-only model. If you have predictors in the model you could give the function a data frame with values for each predictor that you want to obtain predicted values for. We don’t have that, so we need to give it an empty data frame. You can’t really give it an empty data frame so instead I’ll give it a data frame with a column (distribution) and a single row specifying that the predicted values reflect draws from the prior. This will produce a data frame with 4000 predicted values of height. This is another reason this step is kinda tricky. The function turns out data frame that consists of 1 row into a data frame that consists of 4000 rows. If we had a data frame consisting of more rows, we could get a new data frame that has rows equal to 4000 times the number of rows.\nAfter obtain the draws, I plot the draws using a simple histogram.\n\n\nCode\nheights_prior &lt;- tibble(distribution = \"prior\") %&gt;%\n  add_predicted_draws(model_height_prior, value = \"height\")\n\nggplot(heights_prior, aes(x = height)) +\n  geom_histogram(binwidth = 1, fill = blue_3) +\n  labs(x = \"Height\", y = \"\") +\n  coord_cartesian(xlim = c(50, 250))\n\n\n\n\n\nPrior predictive check\n\n\n\n\nOur priors result in a normal distribution of heights with the bulk of the observations ranging from about 120 cm to 205 cm. That seems fairly reasonable to me, as someone who doesn’t know too much about the heights of the !Kung San."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#running-the-model",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#running-the-model",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "Running the model",
    "text": "Running the model\nNow that the priors are in order we can run the model with the code below. Notice that this time I omit the sample_prior argument so we only obtain the posterior results.\n\n\nCode\nmodel_height &lt;- brm(\n  data = data,\n  family = gaussian,\n  height ~ 1,\n  prior = c(\n    prior(normal(160, 10), class = \"Intercept\"),\n    prior(cauchy(5, 5), class = \"sigma\")\n  ),\n  seed = 4,\n  file = \"models/model_height.rds\"\n)\n\n\nAfter running the model, we first check whether the chains look like caterpillars because that indicates we have samples from the entire distribution space of the posteriors.\n\n\nCode\nplot(model_height)\n\n\n\n\n\nThe chains look good.\nWe can call up the estimates and the 95% confidence intervals by printing the model object. Do note that the confidence intervals don’t have the same meaning as frequentist confidence intervals. The intervals here simply indicate what the most likely values are (given the model, priors, and data). By default, the function returns 95% intervals, meaning that 95% of the draws are between the lower and upper bounds.\n\n\nCode\nsummary(model_height)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: data (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.61      0.41   153.83   155.39 1.00     3857     2972\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.77      0.29     7.24     8.36 1.00     3076     2558\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we see the Intercept and sigma estimates. Apparently our posterior estimate for the Intercept is 154.61 and the estimate for \\(\\sigma\\) is 7.77. We also see the 95% CIs, but let’s visualize these results instead.\nInspecting the chains also showed us the posterior distributions of the two parameters, but let’s create our own graphs that compare both the prior and posterior distributions. We can use the spread_draws() function from tidybayes to get draws from each parameter in the model. In the code below I do that twice, once to get the draws from our previous model that sampled from the prior only and once from our new model. The result for each is a data frame and I’ll add a column to indicate whether the draw is from the prior or posterior. If you don’t know what the parameters are called, you can use the get_variables() function to get a list of the names you can extract draws from.\n\n\nCode\ndraws_prior &lt;- model_height_prior %&gt;%\n  spread_draws(b_Intercept, sigma) %&gt;%\n  mutate(distribution = \"prior\")\n\ndraws_posterior &lt;- model_height %&gt;%\n  spread_draws(b_Intercept, sigma) %&gt;%\n  mutate(distribution = \"posterior\")\n\ndraws &lt;- bind_rows(draws_prior, draws_posterior) %&gt;%\n  mutate(distribution = fct_relevel(distribution, \"prior\"))\n\nggplot(draws, aes(x = b_Intercept, fill = distribution)) +\n  geom_histogram(binwidth = 0.25, position = \"identity\") +\n  labs(\n    x = \"Intercept (i.e., average height)\",\n    y = \"\",\n    fill = \"Distribution\"\n  ) +\n  scale_fill_manual(values = c(blue_2, blue_4)) +\n  coord_cartesian(xlim = c(140, 190))\n\n\n\n\n\nHere we see that the posterior distribution of average heights is much more narrow and centered around 155 cm. So not only should we switch from thinking the average is lower than 160, we can also be much more confident about the mean.\nHow about sigma?\n\n\nCode\nggplot(draws, aes(x = sigma, fill = distribution)) +\n  geom_histogram(binwidth = 0.25, position = \"identity\") +\n  labs(\n    x = \"Sigma (i.e., height standard deviation)\",\n    y = \"\",\n    fill = \"Distribution\"\n  ) +\n  scale_fill_manual(values = c(blue_2, blue_4)) +\n  coord_cartesian(xlim = c(0, 25))\n\n\n\n\n\nSimilarly, we see that the posterior for sigma is also much more narrow and around 8.\nA final step is to conduct a posterior predictive check. Since we also conducted a prior predictive check we can plot both and compare how our overall beliefs about the distribution of heights should change as a function of the data. Below I create a new data frame with draws from the posterior, just like when I created the prior predictive check, and merge it with the prior data frame from before.\n\n\nCode\nheights_posterior &lt;- tibble(distribution = \"posterior\") %&gt;%\n  add_predicted_draws(model_height, value = \"height\")\n\nheights &lt;- bind_rows(heights_prior, heights_posterior) %&gt;%\n  ungroup() %&gt;%\n  mutate(distribution = fct_relevel(distribution, \"prior\"))\n\nggplot(heights, aes(x = height, fill = distribution)) +\n  geom_histogram(binwidth = 1, position = \"identity\") +\n  labs(x = \"Height\", y = \"\", fill = \"Distribution\") +\n  scale_fill_manual(values = c(blue_2, blue_4)) +\n  coord_cartesian(xlim = c(100, 250))\n\n\n\n\n\nPrior and posterior predictive check\n\n\n\n\nThis is one of my favorite plots. It shows how we started with a belief about heights and what our new belief should be, after seeing the data. That is the main goal of doing data analysis."
  },
  {
    "objectID": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#summary",
    "href": "content/posts/5-bayesian-tutorial-intercept-only/bayesian-tutorial-intercept-only.html#summary",
    "title": "Bayesian tutorial: Intercept-only model",
    "section": "Summary",
    "text": "Summary\nIn this post I showed how to run an intercept-only model in brms. It consisted of the following steps:\n\nDefine the model\nUse the model to figure out which priors to set\nVisualize the priors and create a prior predictive check to potentially tweak the priors (using brms)\nRun the model\nInspect the output, including the chains\nObtain draws of the estimates and visualize their distribution\nCompare the prior predictive check to the posterior results to see how much to update based on the data\n\nIn the next post I’ll show how to add a predictor to the model.\nThis post was last updated on 2023-08-09."
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html",
    "href": "content/projects/animal-welfare/animal-welfare.html",
    "title": "Animal welfare",
    "section": "",
    "text": "Now that I work at Rethink Priorities I get to devote a significant chunk of my time on projects related to animal welfare. I’ve only recently joined RP, though, so there is not much yet to show, but below I explain why I want to focus on this more and some steps I’ve taken so far."
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html#why-is-this-important",
    "href": "content/projects/animal-welfare/animal-welfare.html#why-is-this-important",
    "title": "Animal welfare",
    "section": "Why is this important?",
    "text": "Why is this important?\n\n“It may come one day to be recognized, that the number of legs, the villosity of the skin, or the termination of the os sacrum, are reasons equally insufficient for abandoning a sensitive being to the same fate. What else is it that should trace the insuperable line? Is it the faculty of reason, or perhaps, the faculty for discourse?…the question is not, Can they reason? nor, Can they talk? but, Can they suffer?\n\nThis (partial) quote by Jeremy Bentham gets to the heart of the matter. Whether something deserves moral concern is predominantly (if not only) a function of whether that something is capable of suffering. Cheating on a partner is bad not because cheating is inherently bad, but because it likely causes great suffering on the cheated-on partner. If your partner doesn’t care about being cheated on (i.e., being in an open relationship), cheating is no longer a bad thing. This shows morality is about the consequences of one’s actions and whether those consequences cause suffering.\nMany animals can suffer. It is unclear and impossible with our current knowledge about consciousness to assess which animals are capable of suffering, but we know enough to confidently say that some animals can suffer. Large mammals such as cows, sheep, goats, and horses can undoubtedly suffer. Smaller creatures such as chicken and turkeys are similarly unlucky and likely also capable. It is less clear when it comes to fish, but I would put my money them being able to suffer rather than being experience-less creatures.\nThe examples of animals I used above are the kinds of animals we farm. These are the kinds of animals we treat in ways that cause them to suffer, with great intensity and in great numbers. Chicken, for example, live in crowded spaces that cause in-fighting, the spread of diseases, and deaths due to, for instance, pile ups. They are artificially selected to grow at unhealthily fast rates, causing physical abnormalities. They are prevented from displaying their instinctive behaviors, such as establishing pecking orders, dust bathing, building nests, and spreading their wings. Sometimes farmers address these problems, although not always in the animal’s best interest. Injuries due to in-fighting is reduced by cutting or burning off the beaks, thus preventing them from harming each other. Other farm animals face similar situations.\nWhat makes it worse is the scale of factory farming. In the Netherlands alone, over 600 million land animals were killed in 2019. And that’s just in the Netherlands, a pretty tiny country. In the U.S., 9.76 billion land animals were killed in 2020. These numbers are so big they almost lose their meaning. The reality is, however, that factory farming causing suffering in billions and billions of individual animals, every year.\nPeople might retort that killing animals for food is simply the natural order of things. This argument is easy to refute: The natural order also sucks. We should not look at nature to determine what is good or bad (this is called the naturalistic fallacy). In nature, all kinds of suffering takes place. Animals (including humans) die due to various causes including disease, disasters, predation, starvation, and so on. These things are normal in nature. As humans, we have done our best to remove all these natural threats from our lives because that reduces our suffering. If we want to be natural, we should invite all these threats back into our lives. Of course, that’s not what we want to do, because we don’t want to suffer.\nI think we should extent that courtesy also to wild animals. We have succeeded in making our lives a lot better, while ignoring the same problems in other species. If we care about the lives of conscious creatures (such as our fellow humans, our pets, our zoo animals), we should also care about the lives of wild animals.\nIn short, farm animals suffer in horrible ways in great numbers, and the same happens in nature (although perhaps less efficiently than in factory farms). Given that suffering is the main reason to care about something, this logically means that we should figure out ways to alleviate their suffering. I hope to contribute to this."
  },
  {
    "objectID": "content/projects/animal-welfare/animal-welfare.html#what-am-i-working-on",
    "href": "content/projects/animal-welfare/animal-welfare.html#what-am-i-working-on",
    "title": "Animal welfare",
    "section": "What am I working on?",
    "text": "What am I working on?\nAt Rethink Priorities I’m working on the development of a scale to assess people’s attitudes toward wild animal suffering. We aim to publish the results of this project in an academic journal with the goal for many other academics to begin studying the topic of wild animal suffering.\nI have also joined the following groups:\n\nSociety for the Psychology of Human-Animal Intergroup Relations (PHAIR)\nResearch to End Consumption of Animal Products (RECAP)\n\nBy joining these groups I hope to learn more about current research directions and to join existing projects. Eventually I also hope to use these platforms to share my own work.\nI’ve joined a project by Mercy for Animals on a multi-country survey to develop insights on people’s knowledge, attitudes, behavioral intentions and behaviors regarding farmed animal issues and key advocacy activities.\nI’ve started my own project that is a meta-analysis on meat intervention studies. There have been several meta-analyses on this topic (see here for a recent one), but I think I can contribute in some unique ways by creating a ‘live meta-analysis’ that can continuously be updated with new studies."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html",
    "href": "content/projects/tidystats/tidystats.html",
    "title": "tidystats",
    "section": "",
    "text": "tidystats is a project centered around creating software to improve how statistics are reported and shared in the field of (social) psychology."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#why-is-this-important",
    "href": "content/projects/tidystats/tidystats.html#why-is-this-important",
    "title": "tidystats",
    "section": "Why is this important?",
    "text": "Why is this important?\nWith this project, I hope to address two problems in statistics reporting: Incorrect and incomplete statistics reporting.\nStatistics are often reported incorrectly (Nuijten et al., 2016). I think this is because researchers do not have the necessary software tools to reliably take the output of statistics from their data analysis software and enter it into their text editor. Instead, researchers are likely to copy statistics from the output by hand or by copy-pasting the output. Both techniques are error-prone, resulting in many papers containing statistical typos. This is a problem because statistical output is used in meta-analyses to aggregate the statistical evidence for theories, which in turn may affect policy. In some cases, the errors may even be so large that it affects the conclusion drawn from the statistical test.\nThere is also a more fundamental issue. Researchers usually only report the statistics in their manuscript and nowhere else. As a result, researchers face trade-offs between reporting all statistics, writing a legible text, and journal guidelines. Reporting all statistics makes results sections difficult (and boring) to read and it also takes up valuable space. Consequently, researchers are likely to only report the statistics that they deem to be relevant, rather than reporting all the statistics. While this is fine for someone who wants to simply read the paper and get the main takeaway, this is not desirable from a cumulative science perspective. All statistics should be easily available so they can be build on in future research."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#what-am-i-working-on",
    "href": "content/projects/tidystats/tidystats.html#what-am-i-working-on",
    "title": "tidystats",
    "section": "What am I working on?",
    "text": "What am I working on?\nI have designed an R package called tidystats that enables researchers to export the statistics from their analyses into a single file. It works by adding your analyses to a list (in R) and then exporting this list to a JSON file. This file will contain all the statistics from the analyses, in an organized format, ready to be used in other software.\nBy storing the output of statistical tests into a separate file, rather than only in one’s manuscript, the researcher no longer needs to worry about which analyses to report in the space-limited manuscript. They can simply share the file together with the manuscript, on OSF or as supplemental material.\nAn additional benefit is that because JSON files are easy to read for computers, it is (relatively) easy to write software that does cool things with these files.\nAn example of software that can read the JSON file is the tidystats Microsoft Word add-in. This add-in can be installed via the Add-in Store from inside Word. With this add-in, researchers can upload the JSON file to reveal a user-friendly list of their analyses. Clicking on an analysis reveals the associated statistics and clicking on a statistic inserts it into the document. This add-in also comes with several time-saving features, such as inserting multiple statistics at once (immediately in APA style) and automatic updating of statistics.\nRecently, I’ve also obtained a grant to work on tidystats. Thanks to this grant, the functionality of tidystats will be expanded, both in terms of adding support for more analyses and by expanding to other platforms, such as Python and Google Docs.\nBesides working on the software itself, I also spent some time on making it accessible. I have given talks introducing tidystats and I’ve created a website to help people become familiar with tidystats."
  },
  {
    "objectID": "content/projects/tidystats/tidystats.html#links",
    "href": "content/projects/tidystats/tidystats.html#links",
    "title": "tidystats",
    "section": "Links",
    "text": "Links\n\nThe tidystats website\nR package on CRAN\nR package GitHub repository\nThe tidystats Word add-in in AppSource (the Office add-in store)\nWord add-in GitHub repository\nA blog post describing an example of using tidystats"
  },
  {
    "objectID": "content/projects.html",
    "href": "content/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Animal welfare\n\n\nFarm animals and wild animals suffer in horrible ways in great numbers. At Rethink Priorities, I contribute to various projects aimed at addressing this important problem.\n\n\n\n\n\n\n\n\n\n\nCognitive dissonance\n\n\nIn this project I aim to assess the evidence for cognitive dissonance theory using a large-scaled replication study of a seminal cognitive dissonance study.\n\n\n\n\n\n\n\n\n\n\nstatcheck\n\n\nTogether with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer statistics-related typos.\n\n\n\n\n\n\n\n\n\n\ntidystats\n\n\ntidystats refers to a collection of software solutions to improve how statistics are reported and shared in the field of (social) psychology.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/cv/cv.html",
    "href": "content/cv/cv.html",
    "title": "dr. Willem Sleegers",
    "section": "",
    "text": "Senior Behavioral Scientist at Rethink Priorities\nResearch Affiliate at Tilburg University\n \n  \n   \n  \n     E-mail\n  \n  \n     X\n  \n  \n     Bluesky\n  \n  \n     Google Scholar\n  \n  \n     Github"
  },
  {
    "objectID": "content/cv/cv.html#employment",
    "href": "content/cv/cv.html#employment",
    "title": "dr. Willem Sleegers",
    "section": "Employment",
    "text": "Employment\n\n\n\n\n\n\n\n\n\n2021-current\nSenior Behavioral Scientist at Rethink Priorities\n\n\n2018-2021\nTenure track Assistant Professor in social psychology at Tilburg University\n\n\n2016-2018\nFixed term Assistant Professor in social psychology at Tilburg University\n\n\n2012-2016\nGraduate student on the topic of physiological arousal in meaning maintenance at Tilburg University\n\n\n2011-2012\nStudent assistant during my Research Master: Behavioral Science at Nijmegen University\n\n\n2012-2013\nProgrammer of experimental psychology tasks for FrieslandCampina\n\n\n2007-2010\nMedia analyst for Report International"
  },
  {
    "objectID": "content/cv/cv.html#education",
    "href": "content/cv/cv.html#education",
    "title": "dr. Willem Sleegers",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\n\n\n2012-2016\nGraduate student at Tilburg University supervised by prof. dr. Ilja van Beest and dr. Travis Proulx\n\n\n2012-2016\nStudent member of the Kurt Lewin Institute (KLI)\n\n\n2010-2012\nResearch Master Behavioural Science at Nijmegen University\n\n\n2010-2012\nExpert track in data-analysis\n\n\n2007-2010\nPsychology BSc. at Nijmegen University\n\n\n2007-2010\nHonours Program of Psychology at Nijmegen University"
  },
  {
    "objectID": "content/cv/cv.html#publications",
    "href": "content/cv/cv.html#publications",
    "title": "dr. Willem Sleegers",
    "section": "Publications",
    "text": "Publications\n\nIn preparation\n\n\n\n\n\n\n\n\nSleegers, W. W. A., Vaidis, D. (shared first author) et al. (in prep.). A multi-lab replication of the induced compliance paradigm of cognitive dissonance. Advances in Methods and Practices in Psychological Science. https://osf.io/52wpj\n\n\nJaeger, B., Sleegers, W. W. A., Stern, J., Penke, L., & Jones, A. (in prep.) Testing perceivers’ accuracy and accuracy awareness when forming personality impressions from faces\n\n\n\n\n\n\n\nPreprints\n\n\n\n\n\n\n\n\n\n2023\nSleegers, W., Moss, D., McAuliffe, W., Reinstein, D., & Waldhorn, D. R. (2023). Measuring attitudes towards wild animal welfare: The Wild Animal Welfare Scale. OSF Preprints. https://doi.org/10.31219/osf.io/qfz73\n\n\n\n\n\n\n\nStage-1 accepted manuscripts\n\n\n\n\n\n\n\n\n\n2021\nSleegers, W. W. A., Vaidis, D. (shared first author) et al. (2021). A multi-lab replication of the induced compliance paradigm of cognitive dissonance. Advances in Methods and Practices in Psychological Science. https://osf.io/52wpj\n\n\n\n\n\n\n\nPeer-reviewed journals\n\n\n\n\n\n\n\n\n\n2023\nRen, D., Wesselmann, E. D., Loh, W. W., van Beest, I., van Leeuwen, F., & Sleegers, W. W. A. (2023). Do cues of infectious disease shape people’s affective responses to social exclusion? Emotion, 23(4), 997–1010. https://doi.org/10.1037/emo0001157\n\n\n2023\nvan Leeuwen, F., Jaeger, B., Sleegers, W. W. A., & Petersen, M. B. (2023) Do experimental manipulations of pathogen avoidance motivations influence conformity? Personality & Social Psychology Bulletin. https://doi.org/10.1177/01461672231160655\n\n\n2023\nJaeger, B., & Sleegers, W. W. A. (2023). Racial disparities in the sharing economy: Evidence from more than 100,000 Airbnb hosts across 14 countries. Journal of the Association for Consumer Research, 8(1), 33–46. https://doi.org/10.1086/722700\n\n\n2022\nStavrova, O., Evans, A. M., Sleegers, W. W. A., & van Beest, I. (2022) Examining the accuracy of lay beliefs about the effects of personality on prosocial behavior. Journal of Behavioral Decision Making, 1-19. https://doi.org/10.1002/bdm.2282\n\n\n2022\nBreznau, N., Rinke, E. M., Wuttke, A., Nguyen, H. H. V., Adem, M., Adriaans, J., Alvarez-Benjumea, A., Andersen, H. K., Auer, D., Azevedo, F., Bahnsen, O., Balzer, D., Bauer, G., Bauer, P. C., Baumann, M., Baute, S., Benoit, V., Bernauer, J., Berning, C., Berthold, A., … Żółtak, T. (2022). Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty. Proceedings of the National Academy of Sciences of the United States of America, 119(44), e2203150119. https://doi.org/10.1073/pnas.2203150119\n\n\n2022\nHoogeveen, S., Sarafoglou, A., Aczel, B., Aditya, Y., Alayan, A. J., Allen, P. J., Altay, S., Alzahawi, S., Amir, Y., Anthony, F.-V., Kwame Appiah, O., Atkinson, Q. D., Baimel, A., Balkaya-Ince, M., Balsamo, M., Banker, S., Bartoš, F., Becerra, M., Beffara, B., … Wagenmakers, E.-J. (2022). A many-analysts approach to the relation between religiosity and well-being. Religion, Brain & Behavior, 1–47. https://doi.org/10.1080/2153599X.2022.2070255\n\n\n2021\nPronk, T. M., Bogaers, R. I., Verheijen, M. S., Sleegers, W. W. A. (2021). Pupil size predicts partner choices in online dating. Social Cognition.\n\n\n2021\nEvans, A. M., Kogler, C., & Sleegers, W. W. A. (2021). No effect of synchronicity in online social dilemma experiments: A registered report. Judgment and Decision Making, 16(4), pp. 823 - 843. https://doi.org/10.1017/S1930297500008007\n\n\n2021\nVan Osch, Y., & Sleegers, W. W. A. (2021). Replicating and reversing the group attractiveness effect: Relatively unattractive groups are perceived as less attractive than the average attractiveness of their members. Acta Psychologica, 217, 103331. https://.doi.org/10.1016/j.actpsy.2021.103331\n\n\n2021\nBrandt, M., Sleegers, W. W. A. (2021) Evaluating belief system networks as a theory of political belief system dynamics, 25(2), 159-185. https://doi.org/10.1177/1088868321993751\n\n\n2021\nJones, B. C., DeBruine, L. M., Flake, J. K., et al. (2021) To which world regions does the valence–dominance model of social perception apply? Nature Human Behavior, 5, 159–169. https://doi.org/10.1038/s41562-020-01007-2\n\n\n2020\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2020) Pupillometry and hindsight bias: Physiological arousal predicts compensatory behavior. Social Psychological and Personality Science. https://doi.org/10.1177/1948550620966153\n\n\n2020\nEvans, A., Sleegers, W. W. A., & Mlakar, Ž. (2020). Individual differences in receptivity to scientific bullshit. Judgment and Decision Making, 15(3), 401-412.\n\n\n2020\nJaeger, B., Sleegers, W. W. A., & Evans, A. M. (2020). Automated classification of demographics from face images: A tutorial and validation. Social and Personality Psychology Compass, 14(3). https://doi.org/10.1111/spc3.12520\n\n\n2019\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2019). Confirmation bias and misconceptions: Pupillometric evidence for a confirmation bias in misconceptions feedback. Biological Psychology, 145, 76–83. https://doi.org/10.1016/j.biopsycho.2019.03.018\n\n\n2019\nBender, M., van Osch, Y., Sleegers, W. W. A., & Ye, M. (2019). Social support benefits psychological adjustment of international students: Evidence from a meta-analysis. Journal of Cross-Cultural Psychology, 50(7), 827–847. https://doi.org/10.1177/0022022119861151\n\n\n2019\nVan ’t Veer, A. E., & Sleegers, W. W. A. (2019). Psychology data from an exploration of the effect of anticipatory stress on disgust vs. Non-disgust related moral judgments. Journal of Open Psychology Data, 7(1), 1. https://doi.org/10.5334/jopd.43\n\n\n2018\nJaeger, B., Sleegers, W. W. A., Evans, A. M., Stel, M., & van Beest, I. (2018). The effects of facial attractiveness and trustworthiness in online peer-to-peer markets. Journal of Economic Psychology. https://doi.org/https://doi.org/10.1016/j.joep.2018.11.004\n\n\n2017\nProulx, T., Sleegers, W. W. A., & Tritt, S. (2017). The expectancy bias: Expectancy-violating faces evoke earlier pupillary dilation than neutral or negative faces. Journal of Experimental Social Psychology, 70, 69-79. https://doi.org/10.1016/j.jesp.2016.12.003\n\n\n2017\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2017). The social pain of Cyberball: Decreased pupillary reactivity to exclusion cues. Journal of Experimental Social Psychology, 69, 187–200. https://doi.org/10.1016/j.jesp.2016.08.004\n\n\n2015\nSleegers, W. W. A., Proulx, T., & van Beest, I. (2015). Extremism reduces conflict arousal and increases values affirmation in response to meaning violations. Biological Psychology, 108, 126–131. https://doi.org/10.1016/j.biopsycho.2015.03.012\n\n\n2015\nSleegers, W. W. A., & Proulx, T. (2015). The comfort of approach: Self-soothing effects of behavioral approach in response to meaning violations. Frontiers in Psychology, 5, 1–10. https://doi.org/10.3389/fpsyg.2014.01568\n\n\n\n\n\n\n\nBook chapters\n\n\n\n\n\n\n\n\n\n2019\nvan Beest, I., & Sleegers, W.W.A (2019). Physiostracism: A case for non-invasive measures of arousal in ostracism research. In S. C. Rudert, R. Greifeneder, & K. D. Williams (Eds.), Current directions in ostracism, social exclusion and rejectionresearch. Routledge. https://doi.org/10.4324/9781351255912\n\n\n\n\n\n\n\nDissertation\n\n\n\n\n\n\n\n\n\n2017\nSleegers, W. W. A. (2017). Meaning and pupillometry: The role of physiological arousal in meaning maintenance (Doctoral dissertation). Retrieved from https://pure.uvt.nl/portal/en/publications/meaning-and-pupillometry(20680e63-e785-43d0-a3ae-e97b26de5f05).html\n\n\n\n\n\n\n\nSoftware\n\n\n\n\n\n\n\n\n\n2020\nSleegers, W. W. A. (2020). tidystats: Save output of statistical tests (Version 0.5) [Computer software]. https://doi.org/10.5281/zenodo.4041859\n\n\n2020\nSleegers, W. W. A. (2020). tidystats (Version 1) [Computer software]. https://doi.org/10.5281/zenodo.4434634\n\n\n\n\n\n\n\nWebsites\n\n\n\n\n\n\n\n\nMy personal website where I blog about (some of) my research.https://www.willemsleegers.com\n\n\nThe tidystats website, a support website for my tidystats software.https://www.tidystats.io"
  },
  {
    "objectID": "content/cv/cv.html#presentations",
    "href": "content/cv/cv.html#presentations",
    "title": "dr. Willem Sleegers",
    "section": "Presentations",
    "text": "Presentations\n\nInvited talks\n\n\n\n\n\n\n\n\n\n2021\nSleegers, W. W. A. (2021, March). tidystats. Talk for a research group at the Ministry of Defence.\n\n\n2021\nSleegers, W. W. A. (2021, February). tidystats. Talk for the BSI at Nijmegen University.\n\n\n2021\nSleegers, W. W. A. (2021, February). Cognitive dissonance RRR. Lab meeting at Cardiff University.\n\n\n2018\nSleegers, W. W. A. (2018, December). Pupillometry and psychology. Colloquium presentation for the Laboratoire de Psychologie Sociale department at Paris Descartes University.\n\n\n2018\nSleegers, W. W. A. (2018, October) tidystats. Colloquium presentation for the Methodology and Statistics department at Leiden University.\n\n\n2018\nSleegers, W. W. A. (2018, March) tidystats. Colloquium presentation for the MTO department at Tilburg University.\n\n\n2017\nSleegers, W. W. A. (2017, December). Meaning and pupillometry: The role of physiological arousal in meaning maintenance. Presentation at the ASPO conference as part of receiving the best ASPO dissertation award.\n\n\n2017\nSleegers, W. W. A. (2017, March). Pupillometry and psychological processes. Colloquium presentation at Cardiff University.\n\n\n2015\nSleegers, W. W. A., Proulx, T. & Van Beest, I. (2015, October). Capturing the physiological response to meaning violations: An eye tracker approach. Colloquium presentation at Tilburg University.\n\n\n\n\n\n\n\nConference presentations\n\n\n\n\n\n\n\n\n\n2023\nSleegers, W. W. A. (2023, June). The Wild Animal Welfare scale. Talk at the Animal Advocacy Conference, Canterbury, the UK.\n\n\n2019\nSleegers, W. W. A. (2019, July) tidystats. Lightning talk at the SIPS conference, Rotterdam, the Netherlands.\n\n\n2019\nSleegers, W. W. A. & Jaeger, B. (2019, December) The Social Cost of Correcting Others. Talk at the ASPO conference, Wageningen, the Netherlands.\n\n\n2018\nSleegers, W. W. A. (2018, June) tidystats. Lightning talk at the SIPS conference, Grand Rapids, MI.\n\n\n2017\nSleegers, W. W. A. (2017, August). oTree for social scientists. Presentation at the TIBER conference, Tilburg, the Netherlands.\n\n\n2016\nSleegers, W. W. A., Proulx, T. & Van Beest (2016, December). Evidence of aversive arousal motivating compensatory behavior. Presentation at ASPO conference, Leiden, the Netherlands.\n\n\n2014\nProulx, T. & Sleegers, W. W. A. (2014, May). Meaning Maintenance Model: Towards a unified account of threat-compensation behaviors. Presentation at KLI conference, Zeist, the Netherlands.\n\n\n2014\nSleegers, W. W. A., Proulx, T., & Van Beest (2014, December). Cyberball and eye tracking: Support for the numbing hypothesis of social exclusion. Presentation at ASPO 2014, Groningen, the Netherlands.\n\n\n2014\nSleegers, W. W. A., Proulx, T., & Van Beest (2014, July). Ostracism and eye tracking. Presentation at EASP preconference on threat regulation, Amsterdam, the Netherlands.\n\n\n\n\n\n\n\nSmall meetings\n\n\n\n\n\n\n\n\n\n2019\nSleegers, W. W. A. (2019, November) Addressing Incorrect and Incomplete Statistics Reporting (with tidystats). Talk at the meta-research day at Tilburg University, the Netherlands.\n\n\n\n\n\n\n\nPoster presentations\n\n\n\n\n\n\n\n\n\n2017\nSleegers, W. W. A. (2017, July). Pupillometry and psychology: Pupillometry as an experimental tool for psychologists. Poster session presented at the Ostracism, Social Exclusion, and Rejection conference, Vitznau, Switzerland.\n\n\n2016\nSleegers, W. W. A., Proulx, T., & Van Beest (2016, January). Ostracism and eye tracking: Decreased pupillary reactivity to exclusion cues. Poster session presented at the SPSP conference, San Diego.\n\n\n2015\nSleegers, W. W. A., Proulx, T., & Van Beest (2015, December). Meaning and misconceptions: The effect of error feedback and commitment towards misconceptions on pupil size. Poster session presented at ASPO conference, Amsterdam.\n\n\n2015\nSleegers, W. W. A., Proulx, T. & Van Beest (2015, March). Cyberball and eye tracking: Support for the numbing hypothesis of social exclusion. Poster session presented at ICPS conference, Amsterdam, the Netherlands.\n\n\n2014\nSleegers, W. W. A., Proulx, T, & Van Beest, I. (2014, May). Extremism and the response to meaning threats: Extremism reduces pupillary response to threat and increases affirmation of values. Poster session presented at the KLI conference, Zeist, the Netherlands.\n\n\n\n\n\n\n\nValorization presentations\n\n\n\n\n\n\n\n\n\n2017\nSleegers, W. W. A., & Wagemans, F. M. A. (2017, November). The psychology behind eye tracking. Presentation organized by the Academic Forum of Tilburg University.\n\n\n2017\nWagemans, F. M. A., & Sleegers, W. W. A. (2017, June). Waar walg jij van? Presentation at Mundial festival on attentional processes and disgust sensitivity using eye tracking."
  },
  {
    "objectID": "content/cv/cv.html#journals",
    "href": "content/cv/cv.html#journals",
    "title": "dr. Willem Sleegers",
    "section": "Journals",
    "text": "Journals\nI reviewed for Behavioural Processes, Biological Psychology, British Journal of Psychology, British Journal of Social Psychology, Collabra, European Journal of Social Psychology, Group Processes & Intergroup Relations, International Journal of Psychology, International Review of Social Psychology, Journal of Consumer Behaviour, Journal of Experimental Social Psychology, Journal of Social and Personal Relationships, Personality and Social Psychology Bulletin, PLOS ONE, Self and Identity, Social Cognition, Social Influence, Social Psychology, Current Psychology, Journal of Cognitive Psychology.\nI’m a consulting editor for the Psychology of Human-Animal Intergroup Relations (PHAIR) journal."
  },
  {
    "objectID": "content/cv/cv.html#teaching",
    "href": "content/cv/cv.html#teaching",
    "title": "dr. Willem Sleegers",
    "section": "Teaching",
    "text": "Teaching\n\nCourses\n\n\n\n\n\n\n\n\n\n2017-2021\nSocial Psychology\n\n\n2016-2021\nAttitudes and Advertising\n\n\n2019-2021\nUnderstanding Data with R\n\n\n2015/2017/2019\nResearch Master: Experimental Research and Meta-Analysis\n\n\n2016-2021\nCourse in R software\n\n\n\n\n\n\n\nSeminars\n\n\n\n\n\n\n\n\n\n2012-2017\nSocial Psychology\n\n\n2015-2016\nIntroduction and History of Psychology\n\n\n2014-2015\nCultural Psychology\n\n\n2013-2015\nAcademic Skills\n\n\n2012-2013\nGroup Skills\n\n\n\n\n\n\n\nIndividual lectures\n\n\n\n\n\n\n\n\n\n2016\nSocial Psychology\n\n\n2014\nIntroduction and History of Psychology on intrapersonal conflict\n\n\n2013\nIntroduction to Social Psychology for prospective students\n\n\n\n\n\n\n\nSupervision\n\n\n\n\n\n\n\n\n\n2021\nResearch Master in Psychology theses\n\n\n2016-2021\nMaster in Psychology theses\n\n\n2013-2021\nBachelor in Psychology theses\n\n\n2012-2018\nResearch Skills in Psychology\n\n\n\n\n\n\n\nCoordination\n\n\n\n\n\n\n\n\n\n2014-2021\nSocial Psychology\n\n\n2016-2021\nAttitudes and Advertising\n\n\n2019-2021\nUnderstanding Data with R\n\n\n\n\n\n\n\nOther\n\n\n\n\n\n\n\n\n\n2013-2021\nAn introduction to R; part of the Kurt Lewin Institute course program"
  },
  {
    "objectID": "content/cv/cv.html#technical-skills",
    "href": "content/cv/cv.html#technical-skills",
    "title": "dr. Willem Sleegers",
    "section": "Technical skills",
    "text": "Technical skills\n\nStatistics\n\n\n\n\n\nR: A free software environment for statistical computing and graphics\n\n\nSPSS: A proprietary data analysis program\n\n\n\n\n\n\n\nProgramming\n\n\n\n\n\nQuarto: An open-source scientific and technical publishing system\n\n\nHTML: Markup language for creating web pages and web applications\n\n\nCSS: Markup language for styling web pages and web applications\n\n\nJavaScript: A programming language for creating web applications\n\n\nReact: A JavaScript library for building user interfaces\n\n\nPython: A cross-platform procedural programming language\n\n\n\n\n\n\n\nExperimental design\n\n\n\n\n\n\n\n\nMillisecond’s Inquisit: Stimulus delivery and experimental design software\n\n\noTree: Framework based on Python and Django to create standard and interactive online psychological experiments\n\n\nTobii Studio and Tobii Studio Extensions for E-prime: software to run eye tracker experiments using Tobii eye trackers\n\n\nPsychology Software Tool’s E-Prime: Stimulus delivery and experimental design software\n\n\nAdobe’s Authorware: Stimulus delivery and experimental design software. This has been discontinued, please do not make me use it\n\n\nNeurobehavioural Systems’ Presentation®: A stimulus delivery and experimental control program for neuroscience"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dr. Willem Sleegers",
    "section": "",
    "text": "Senior Behavioral Scientist at Rethink Priorities\nResearch Affiliate at Tilburg University\n \n  \n   \n  \n    \n     E-mail\n  \n  \n    \n     Twitter\n  \n  \n    \n     Google Scholar\n  \n  \n    \n     GitHub"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "dr. Willem Sleegers",
    "section": "About",
    "text": "About\nI’m a Senior Behavioral Scientist at Rethink Priorities. I am part of the survey team, which means I conduct research on attitude assessments and attitude change, using surveys and experimental designs. Before joining Rethink Priorities, I was an assistant professor in the Department of Social Psychology at Tilburg University, where I continue to be affiliated. On this website you can find information about some of the projects I’m involved in. I also blog about various topics related to my work.\nLearn more"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "dr. Willem Sleegers",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\nAnimal welfare\n\n\nFarm animals and wild animals suffer in horrible ways in great numbers. At Rethink Priorities, I contribute to various projects aimed at addressing this important problem.\n\n\n\n\n\n\n\n\n\n\nCognitive dissonance\n\n\nIn this project I aim to assess the evidence for cognitive dissonance theory using a large-scaled replication study of a seminal cognitive dissonance study.\n\n\n\n\n\n\n\n\n\n\nstatcheck\n\n\nTogether with Michèle Nuijten I am working on improving statcheck. statcheck is a software tool to help researchers make fewer statistics-related typos.\n\n\n\n\n\n\n\n\n\n\ntidystats\n\n\ntidystats refers to a collection of software solutions to improve how statistics are reported and shared in the field of (social) psychology.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "dr. Willem Sleegers",
    "section": "Blog posts",
    "text": "Blog posts\n\n\n\n\n\n\nBayesian tutorial: Two groups\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\n\nThe fourth of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to model the difference between two groups.\n\n\n\n\n\n\nApr 24, 2023\n\n\n\n\n\n\n\n\nBayesian tutorial: Correlation\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\n\nThe third of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to model a correlation.\n\n\n\n\n\n\nFeb 12, 2023\n\n\n\n\n\n\n\n\nBayesian tutorial: Single predictor regression\n\n\n\n\n\n\n\nstatistics\n\n\ntutorial\n\n\nBayesian statistics\n\n\nregression\n\n\n\n\nThe second of a series of tutorial posts on Bayesian analyses. In this post I focus on using brms to run a regression with a single predictor.\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\nNo matching items\n\n\nMore posts"
  },
  {
    "objectID": "index.html#cv",
    "href": "index.html#cv",
    "title": "dr. Willem Sleegers",
    "section": "CV",
    "text": "CV\nI’m an Senior Behavioral Scientist at Rethink Priorities. I’m a former academic with 10 years of research experience. I have published, and continue to publish, scientific papers in peer-reviewed journals and I also develop research-related software. My skill set consists of various research skills (e.g., experimental design, data analysis, writing) and technical skills (e.g., programming). I’m currently focused on applying my skills to topics of high impact together with my colleagues at Rethink Priorities.\nLearn more"
  }
]